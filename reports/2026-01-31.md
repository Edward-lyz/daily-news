# AI 基础设施新闻简报 (2026-01-31)

## 摘要

本周重点关注 FlashInfer、SGLang 和 vLLM 项目的更新。FlashInfer 进行了重大重构，移除了对 Nemotron 的支持并简化了激活类型系统。SGLang 添加了新模型支持并优化了 CI 流程。vLLM 则进行了多项性能优化和错误修复。

## 具体内容分析

### FlashInfer

**主要变更**:
- 移除了对 Nemotron 专家的支持和相关代码
- 简化了激活类型系统，从 `ActivationType` 改为 `GatedActType`，仅支持 SwiGlu 和 GeGlu
- 限制了 DeepSeek 路由方法的 topK 从 22 降低到 8
- 更新了 Docker CI 标签并禁用了缓存

**影响评估**:
- **积极**: 代码库更简洁，维护负担减轻
- **风险**: 可能影响依赖 Nemotron 功能的用户
- **回归**: 移除了对多种激活类型的支持，可能影响灵活性

### SGLang

**主要变更**:
- 添加了对 Kimi-K2-Thinking 和 Mistral-Large3-675B-Instruct-2512 模型的支持
- 修复了 AMD CI 问题
- 添加了对 Ling Flash v2.0 和 Qwen3-Next Eagle3 的支持
- 优化了 SWA KV 缓存内存分配
- 修复了 GPT-OSS 在启用分段 CUDA 图时的准确性问题

**影响评估**:
- **积极**: 扩展了模型支持范围，提高了系统稳定性
- **风险**: 新模型支持可能引入未知问题
- **改进**: 性能优化和错误修复提升了整体用户体验

### vLLM

**主要变更**:
- 添加了对 SM120 (RTX Blackwell) 的支持
- 修复了 FlashInfer 相关测试
- 移除了弃用的池化相关代码
- 优化了多模态数据处理
- 修复了缓存重置的不一致处理

**影响评估**:
- **积极**: 新硬件支持扩展了兼容性范围
- **风险**: 重大变更可能引入新问题
- **改进**: 代码清理和优化提高了系统稳定性

## 总结

本周的更新主要集中在代码库的简化和优化上。FlashInfer 的重构虽然减少了功能支持，但提高了代码质量。SGLang 和 vLLM 则继续扩展功能支持并修复已知问题。整体而言，这些更新有助于提高系统的稳定性和性能，但也可能带来一些兼容性挑战。
