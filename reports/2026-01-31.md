# Daily AI Infra Report - 2026-01-31

Generated from raw data because summarization was unavailable.

## Papers
- No papers matched the date filter.

## Repos
### deepseek-ai/DeepGEMM
- No commits in the window.
### deepseek-ai/FlashMLA
- No commits in the window.
### NVIDIA/cutlass
- No commits in the window.
### flashinfer-ai/flashinfer
- [`87a45d1`](https://github.com/flashinfer-ai/flashinfer/commit/87a45d131dc6518493718e20086cc665ed46da4f) Revert "feat: Support Fused MoE non gated Relu2 NVFP4 & FP8 and support Nemotron" (#2451) (nv-yunzheq)
  - Stats: +304/-610 (total 914)
  - Files (truncated):
    - modified: benchmarks/bench_trtllm_gen_fused_moe_autotuner.py (+2/-24)
      ```diff
      @@ -4,7 +4,7 @@
       import numpy as np
       from flashinfer import (
           RoutingMethodType,
      -    ActivationType,
      +    GatedActType,
           fp4_quantize,
           mxfp8_quantize,
       )
      @@ -17,7 +17,6 @@
       from flashinfer.autotuner import autotune
       from flashinfer.testing.utils import bench_gpu_time
       from flashinfer.utils import device_support_pdl
      -from routines.flashinfer_benchmark_utils import enum_type
       
       FLOAT8_E4M3_MAX = torch.finfo(torch.float8_e4m3fn).max
       FLOAT4_E2M1_MAX = 6.0
      @@ -40,7 +39,6 @@ def bench_trtllm_gen_fused_moe_autotuner_fp8(
           top_k: int,
           warmups: int,
           iterations: int,
      -    activation_type: ActivationType,
       ):
           device = torch.device("cuda:0")
           enable_pdl = device_support_pdl(device)
      @@ -99,10 +97,6 @@ def bench_trtllm_gen_fused_moe_autotuner_fp8(
           )
       
           if is_block_scale:
      -        if activation_type != ActivationType.Swiglu:
      -            raise ValueError(
      -                "Only Swiglu activation is supported for FP8 block scale MoE."
      -            )
               fn = lambda: trtllm_fp8_block_scale_moe(
                   routing_logits,
                   routing_bias,
      @@ -150,7 +144,6 @@ def bench_trtllm_gen_fused_moe_autotuner_fp8(
                   RoutingMethodTyp...
      ```
    - modified: benchmarks/routines/flashinfer_benchmark_utils.py (+0/-16)
      ```diff
      @@ -1,4 +1,3 @@
      -import argparse
       import torch
       
       from flashinfer.testing.utils import set_seed
      @@ -454,18 +453,3 @@ def filter_backends_by_compute_capability(backends, routine, device):
                   f"[WARNING] {backend} for routine {routine} is not supported on compute capability {compute_capability}. Skipping."
               )
           return backends
      -
      -
      -def enum_type(enum_class):
      -    """Generic factory for argparse enum types."""
      -
      -    def converter(value):
      -        try:
      -            lower_name_to_member = {m.name.lower(): m for m in enum_class}
      -            return lower_name_to_member[value.lower()]
      -        except KeyError as e:
      -            raise argparse.ArgumentTypeError(
      -                f"Invalid value '{value}'. Must be one of: {', '.join([m.name for m in enum_class])}"
      -            ) from e
      -
      -    return converter
      ```
    - modified: benchmarks/routines/moe.py (+15/-14)
      ```diff
      @@ -5,7 +5,6 @@
       import torch
       
       import flashinfer
      -from flashinfer import ActivationType
       from flashinfer.autotuner import autotune
       from flashinfer.fused_moe import (
           trtllm_fp4_block_scale_moe,
      @@ -22,7 +21,6 @@
       
       from .flashinfer_benchmark_utils import (
           dtype_str_to_torch_dtype,
      -    enum_type,
           get_device,
           print_perf_metrics,
           filter_backends_by_compute_capability,
      @@ -172,12 +170,12 @@ def parse_moe_args(line, parser):
               help="Data type of the weights (before quantization).",
           )
           parser.add_argument(
      -        "--activation-type",
      -        type=enum_type(ActivationType),
      -        metavar=str([e.name for e in ActivationType]),
      +        "--gated_act",
      +        type=str,
               required=False,
      -        default=ActivationType.Swiglu,
      -        help=f"Type of activation function: {[e.name for e in ActivationType]}",
      +        default="swiglu",
      +        choices=["swiglu", "geglu"],
      +        help="Type of gated activation function: swiglu | geglu.",
           )
           parser.add_argument(
               "--autotune",
      @@ -244,6 +242,13 @@ def parse_moe_args(line, parser):
           }
           args.routing_method_type = routing_method_name_to_type[args.routing_meth...
      ```
    - modified: csrc/trtllm_batched_gemm_runner.cu (+2/-8)
      ```diff
      @@ -101,16 +101,14 @@ TrtllmGenBatchedGemmRunner::TrtllmGenBatchedGemmRunner(
               options.mTransposeMmaOutput == mOptions.transposeMmaOutput &&
               (!doesRouteImplUseNoRoute(options.mRouteImpl)) == mOptions.routeAct &&
               options.mFusedAct == mOptions.fusedAct && options.mIsStaticBatch == mOptions.staticBatch &&
      -        tileSize == mOptions.tileSize && options.mUseShuffledMatrix == mOptions.useShuffledMatrix &&
      +        tileSize == mOptions.tileSize &&
      +        options.mUseShuffledMatrix == mOptions.useShuffledMatrixA &&
               options.mLayoutA == mOptions.weightLayout) {
             if (options.mFusedAct) {
               if (options.mActType != static_cast<batchedGemm::gemmGatedAct::ActType>(mOptions.actType)) {
                 continue;
               }
             }
      -      if ((int64_t)options.mEltwiseActType != (int64_t)mOptions.eltwiseActType) {
      -        continue;
      -      }
       
             if (mOptions.transposeMmaOutput && options.mEpilogueTileM == mOptions.epilogueTileM) {
               mPassingConfigIndices.push_back(i);
      @@ -124,8 +122,6 @@ TrtllmGenBatchedGemmRunner::TrtllmGenBatchedGemmRunner(
                   << ", mDtypeB: " << tg::dtypeToString(mOptions.dtypeB)
                   << ", mDtypeC: ...
      ```
    - modified: csrc/trtllm_fused_moe_kernel_launcher.cu (+45/-44)
      ```diff
      @@ -36,7 +36,7 @@
       namespace flashinfer {
       
       namespace btg = batchedGemm::trtllm::gen;
      -using tensorrt_llm::kernels::trtllmgen_moe::MoE::ActivationType;
      +using tensorrt_llm::kernels::trtllmgen_moe::MoE::GatedActType;
       using tensorrt_llm::kernels::trtllmgen_moe::Routing::RoutingMethodType;
       using tvm::ffi::Array;
       using tvm::ffi::Optional;
      @@ -109,7 +109,7 @@ class FusedMoeLauncher {
         btg::Dtype mDtypeWeights{btg::Dtype::Bfloat16};
         btg::Dtype mRoutingBiasDtype{
             btg::Dtype::Bfloat16};  // Dtype for expert weights in routing, based on routing bias
      -  ActivationType activation_type{ActivationType::Swiglu};
      +  GatedActType gated_act_type{GatedActType::SwiGlu};
       
        public:
         // Constructor that initializes all TensorView members
      @@ -134,14 +134,14 @@ class FusedMoeLauncher {
               weight_layout{batchedGemm::gemm::MatrixLayout::MajorK},
               mDtypeAct{btg::Dtype::Bfloat16},
               mDtypeWeights{btg::Dtype::Bfloat16},
      -        activation_type{ActivationType::Swiglu} {}
      +        gated_act_type{GatedActType::SwiGlu} {}
       
        protected:
         // Initialize common data necessary for later.
         // May throw exception from TVM_FFI_ICHECK.
         void init_common(std::unique_ptr<te...
      ```
    - modified: csrc/trtllm_fused_moe_routing_deepseek.cu (+23/-44)
      ```diff
      @@ -14,7 +14,6 @@
        * limitations under the License.
        */
       
      -#include <algorithm>
       #include <cmath>
       
       #include "flashinfer/exception.h"
      @@ -26,14 +25,10 @@ namespace routingDeepSeek {
       
       ////////////////////////////////////////////////////////////////////////////////////////////////////
       
      -static constexpr int NumNemotronExperts = 512;
       static constexpr int NumKimiK2Experts = 384;
       static constexpr int NumDeepseekExperts = 256;
      -static constexpr int MaxSupportedExpertCount =
      -    std::max({NumNemotronExperts, NumKimiK2Experts, NumDeepseekExperts});
       static constexpr int NumTopGroupScores = 2;
      -static constexpr int DefaultMaxNumTopExperts = 8;
      -static constexpr int MaxSupportedTopExperts = 22;
      +static constexpr int MaxNumTopExperts = 8;
       static constexpr int MaxNumTopGroups = 4;
       static constexpr int MaxNumGroups = 8;
       
      @@ -122,8 +117,8 @@ __global__ void routingMainKernel(KernelParams params) {
           int32_t topGroupIdx[MaxNumTopGroups];
           float expertScoreGroup[MaxNumTopGroups];
           int32_t expertIdxGroup[MaxNumTopGroups];
      -    float topScores[KernelParams::MaxNumTopExperts];  // bound of params.mTopK
      -    int32_t topExperts[KernelParams::MaxNumTopExperts];
      +    float topSc...
      ```
    - modified: csrc/trtllm_fused_moe_runner.cu (+34/-95)
      ```diff
      @@ -60,7 +60,7 @@ void Runner::run(void* routingLogits, void* routingBias, int32_t numTokens, int3
                        bool useRoutingScalesOnInput, bool useDeepSeekFp8,
                        RoutingMethodType routingMethodType, cudaStream_t stream) {
         if (routingMethodType == RoutingMethodType::DeepSeekV3) {
      -    FLASHINFER_CHECK(topK <= 22, "For DeepSeek routing method, must have topK <= 22");
      +    FLASHINFER_CHECK(topK <= 8, "For DeepSeek routing method, must have topK <= 8");
           FLASHINFER_CHECK(topkGroup <= 4, "For DeepSeek routing method, must have topkGroup <= 4");
           moe::dev::routing::routingDeepSeek::Data routingData;
           routingData.mDtypeExpW =
      @@ -189,49 +189,13 @@ void Runner::run(void* routingLogits, void* routingBias, int32_t numTokens, int3
       
       namespace PermuteGemm1 {
       
      -using tensorrt_llm::kernels::trtllmgen_moe::MoE::ActivationType;
      -using tensorrt_llm::kernels::trtllmgen_moe::MoE::isGatedActivation;
      -using tensorrt_llm::kernels::trtllmgen_moe::MoE::serializeActivationType;
      -
      -static inline ActType activationTypeToGatedActType(ActivationType actType) {
      -  switch (actType) {
      -    case ActivationType::Swiglu:
      -      return ActType::SwiGlu;
      -    case ActivationT...
      ```
    - modified: flashinfer/__init__.py (+1/-1)
      ```diff
      @@ -75,8 +75,8 @@
       )
       from .fp8_quantization import mxfp8_dequantize_host, mxfp8_quantize
       from .fused_moe import (
      -    ActivationType,
           RoutingMethodType,
      +    GatedActType,
           cutlass_fused_moe,
           reorder_rows_for_gated_act_gemm,
           trtllm_fp4_block_scale_moe,
      ```
    - modified: flashinfer/fused_moe/__init__.py (+2/-2)
      ```diff
      @@ -15,8 +15,8 @@
       """
       
       from .core import (
      -    ActivationType,
           RoutingMethodType,
      +    GatedActType,
           WeightLayout,
           convert_to_block_layout,
           cutlass_fused_moe,
      @@ -40,8 +40,8 @@
       )
       
       __all__ = [
      -    "ActivationType",
           "RoutingMethodType",
      +    "GatedActType",
           "WeightLayout",
           "convert_to_block_layout",
           "cutlass_fused_moe",
      ```
    - modified: flashinfer/fused_moe/core.py (+31/-55)
      ```diff
      @@ -173,6 +173,15 @@ class WeightLayout(IntEnum):
           BlockMajorK = 2
       
       
      +# The type of gated activation function
      +# Please keep this in sync with the counterpart defined in include/flashinfer/trtllm/fused_moe/runner.h
      +class GatedActType(IntEnum):
      +    # SwiGlu
      +    SwiGlu = 0
      +    # GeGlu
      +    GeGlu = 1
      +
      +
       @functools.cache
       def is_trtllm_moe_supported(
           dtype_weights: DtypeTrtllmGen,
      @@ -212,16 +221,12 @@ def _maybe_get_cached_w3_w1_permute_indices(
           dst_w3_w1_weight: torch.Tensor,
           epilogue_tile_m: int,
           num_elts_per_sf: Union[None, int] = None,
      -    is_gated_act_gemm: bool = True,
       ) -> torch.Tensor:
           # Create a unique cache key (weight_type, weight_shape)
           cache_key = ("w3_w1", dst_w3_w1_weight.shape)
           if cache_key not in _cache_permute_indices:
               # Get permute indices and chain them together
      -        if is_gated_act_gemm:
      -            permute0 = get_reorder_rows_for_gated_act_gemm_row_indices(dst_w3_w1_weight)
      -        else:
      -            permute0 = torch.arange(dst_w3_w1_weight.shape[0], dtype=torch.long)
      +        permute0 = get_reorder_rows_for_gated_act_gemm_row_indices(dst_w3_w1_weight)
               if num_elts_per_sf is None:
              ...
      ```
    - modified: include/flashinfer/trtllm/batched_gemm/KernelRunner.h (+1/-17)
      ```diff
      @@ -47,35 +47,19 @@ enum class ActType {
         GeGlu,
       };
       
      -// Type of the element-wise activation to apply after the Gemm
      -enum class EltwiseActType {
      -  None = 0,
      -  // Gelu is defined as the following operation:
      -  // act = x0 * phi(x0)
      -  // where x0 is the output of the Gemm
      -  // phi is the CDF of standard normal distribution approximated by
      -  // phi(x) = 0.5 * (1 + tanh(0.7978845608028654 * (x + 0.044715 * x * x * x)))
      -  Gelu,
      -  // Relu2 (also known as squared Relu) is defined as the following operation:
      -  // act = relu(x0) ^ 2
      -  // where x0 is the output of the Gemm.
      -  Relu2,
      -};
      -
       struct TrtllmGenBatchedGemmRunnerOptions {
         batchedGemm::trtllm::gen::Dtype dtypeA;
         batchedGemm::trtllm::gen::Dtype dtypeB;
         batchedGemm::trtllm::gen::Dtype dtypeC;
         ActType actType{ActType::SwiGlu};
      -  EltwiseActType eltwiseActType{EltwiseActType::None};
         bool deepSeekFp8{false};
         bool fusedAct{false};
         bool routeAct{false};
         bool staticBatch{false};
         bool transposeMmaOutput{false};
         int32_t tileSize{8};
         int32_t epilogueTileM{128};
      -  bool useShuffledMatrix{false};
      +  bool useShuffledMatrixA{false};
         batchedGemm::gemm::MatrixLayout weightLayout{batchedGemm::gemm...
      ```
    - modified: include/flashinfer/trtllm/fused_moe/DevKernel.h (+48/-57)
      ```diff
      @@ -169,65 +169,56 @@ namespace moe::dev {
           FLASHINFER_WARN("Unsupported dtypeExpW");                                                      \
         }
       
      -#define LAUNCH_ROUTING_DEEPSEEK_WITH_EXTRA_FLAG(data, coopLaunch, kernel, numBlocks, numThreads,  \
      -                                                smemSize, stream, extraFlag, numExperts,          \
      -                                                numTopExperts)                                    \
      -  if (data.mDtypeScore == tg::Dtype::Fp32 && data.mDtypeBias == tg::Dtype::Fp32 &&                \
      -      data.mDtypeExpW == tg::Dtype::Fp32) {                                                       \
      -    LAUNCH_TILEN(data, coopLaunch,                                                                \
      -                 LAUNCH_ESC(float, float, float, numExperts, numTopExperts, extraFlag), kernel,   \
      -                 numBlocks, numThreads, smemSize, stream);                                        \
      -  } else if (data.mDtypeScore == tg::Dtype::Fp32 && data.mDtypeBias == tg::Dtype::Fp32 &&         \
      -             data.mDtypeExpW == tg::Dtype::Bfloat16) {                                            \
      -    LAUNCH_TILEN(data, coopLaunch,...
      ```
    - modified: include/flashinfer/trtllm/fused_moe/RoutingKernel.h (+2/-3)
      ```diff
      @@ -176,15 +176,14 @@ struct Data : public DataBase {
         bool mUseRoutingSoftmax;
       };
       
      -template <typename InputT_, typename BiasT_, typename OutputT_, int MaxNumExperts_,
      -          int MaxNumTopExperts_, bool UseGroups_, bool isPow2_, bool UsePdl_>
      +template <typename InputT_, typename BiasT_, typename OutputT_, int MaxNumExperts_, bool UseGroups_,
      +          bool isPow2_, bool UsePdl_>
       struct KernelParams : public KernelParamsBase<InputT_, OutputT_, MaxNumExperts_, isPow2_, UsePdl_> {
         using InputT = InputT_;
         using BiasT = BiasT_;
         using OutputT = OutputT_;
       
         static constexpr bool UseGroups = UseGroups_;
      -  static constexpr int MaxNumTopExperts = MaxNumTopExperts_;
       
         PackedScoreIdx<OutputT>* mPtrTopKPacked = nullptr;
      ```
    - modified: include/flashinfer/trtllm/fused_moe/runner.h (+17/-43)
      ```diff
      @@ -136,56 +136,33 @@ class Runner {
       }  // namespace Routing
       
       namespace MoE {
      -// The type of activation function
      +// The type of gated activation function
       // Please keep this in sync with the counterpart defined in flashinfer/flashinfer/fused_moe/core.py
      -enum class ActivationType : int64_t {
      -  Gelu = 0,
      -  Relu = 1,
      -  Silu = 2,
      -  Swiglu = 3,
      -  Geglu = 4,
      -  SwigluBias = 5,
      -  Relu2 = 6,
      -  Identity = 7,
      -  InvalidType = 8,  // Must be last
      +enum class GatedActType : int64_t {
      +  // SwiGlu
      +  SwiGlu = 0,
      +  // GeGlu
      +  GeGlu = 1,
       };
       
      -inline std::string serializeActivationType(ActivationType activationType) {
      -  switch (activationType) {
      -    case ActivationType::Gelu:
      -      return "Gelu";
      -    case ActivationType::Relu:
      -      return "Relu";
      -    case ActivationType::Silu:
      -      return "Silu";
      -    case ActivationType::Swiglu:
      -      return "Swiglu";
      -    case ActivationType::Geglu:
      -      return "Geglu";
      -    case ActivationType::SwigluBias:
      -      return "SwigluBias";
      -    case ActivationType::Relu2:
      -      return "Relu2";
      -    case ActivationType::Identity:
      -      return "Identity";
      +inline std::string serializeGatedActType(GatedActType gatedActType) {
      +  switch...
      ```
    - modified: tests/moe/test_dpsk_fused_moe_fp8.py (+2/-2)
      ```diff
      @@ -8,7 +8,7 @@
           trtllm_fp8_block_scale_moe,
       )
       from .utils import skip_checks, QuantMode
      -from flashinfer import ActivationType
      +from flashinfer import GatedActType
       
       
       def dequant_fp8_block_scaled(
      @@ -616,7 +616,7 @@ def __init__(self):
               moe_impl=moe_impl,
               routing_config=routing_config,
               weight_processing=weight_processing,
      -        activation_type=ActivationType.Swiglu,
      +        gated_act_type=GatedActType.SwiGlu,
               num_tokens=seq_len,
               hidden_size=7168,  # DeepSeek-V3 hidden size
               intermediate_size=intermediate_size,
      ```
    - modified: tests/moe/test_trtllm_gen_fused_moe.py (+68/-152)
      ```diff
      @@ -22,8 +22,8 @@
       from torch.nn import functional as F
       
       from flashinfer import (
      -    ActivationType,
           RoutingMethodType,
      +    GatedActType,
           e2m1_and_ufp8sf_scale_to_float,
           fp4_quantize,
           mxfp8_dequantize_host,
      @@ -46,7 +46,7 @@
           get_w2_permute_indices_with_cache,
           _maybe_get_cached_w3_w1_permute_indices,
       )
      -from .utils import is_gated_activation, skip_checks, QuantMode
      +from .utils import skip_checks, QuantMode
       
       
       # Max num tokens to tune for trtllm-gen fused moe
      @@ -209,7 +209,7 @@ def _run_moe_computation(self, runtime_args):
                   local_num_experts=self.config["num_experts"],
                   routed_scaling_factor=self.config["routed_scaling"],
                   routing_method_type=self.config["routing_method_type"],
      -            activation_type=self.config["activation_type"],
      +            gated_act_type=self.config["gated_act_type"],
                   do_finalize=True,
                   tune_max_num_tokens=TUNE_MAX_NUM_TOKENS,
               )
      @@ -227,12 +227,6 @@ class Moe(ABC):
           def __init__(self):
               self.name = self.__class__.__name__
       
      -    @property
      -    @abstractmethod
      -    def quant_mode(self) -> QuantMode:
      -        """Get the quantization mo...
      ```
- [`f61fa8d`](https://github.com/flashinfer-ai/flashinfer/commit/f61fa8d7fe19683f4592957e7b1003307754dcb2) Update Docker CI tags to 20260131-a52eff1 (#2457) (FlashInfer Bot)
  - Stats: +4/-4 (total 8)
  - Files:
    - modified: ci/docker-tags.yml (+4/-4)
      ```diff
      @@ -1,4 +1,4 @@
      -flashinfer/flashinfer-ci-cu126: 20260114-cc1a362
      -flashinfer/flashinfer-ci-cu128: 20260114-cc1a362
      -flashinfer/flashinfer-ci-cu129: 20260114-cc1a362
      -flashinfer/flashinfer-ci-cu130: 20260114-cc1a362
      +flashinfer/flashinfer-ci-cu126: 20260131-a52eff1
      +flashinfer/flashinfer-ci-cu128: 20260131-a52eff1
      +flashinfer/flashinfer-ci-cu129: 20260131-a52eff1
      +flashinfer/flashinfer-ci-cu130: 20260131-a52eff1
      ```
- [`a52eff1`](https://github.com/flashinfer-ai/flashinfer/commit/a52eff1e837bba6724ebbd7d98445635b8b4835d) ci: CI build workflow should always pull fresh and do not cache (#2454) (Brian K. Ryu)
  - Stats: +2/-2 (total 4)
  - Files:
    - modified: .github/workflows/release-ci-docker.yml (+2/-2)
      ```diff
      @@ -61,10 +61,10 @@ jobs:
                 file: docker/Dockerfile.${{ matrix.cuda }}
                 platforms: linux/${{ matrix.arch }}
                 push: ${{ github.event_name != 'pull_request' }}
      +          pull: true  # Always pull the latest base image
      +          no-cache: true  # Disable all caching
                 tags: |
                   flashinfer/flashinfer-ci-${{ matrix.cuda }}:${{ matrix.arch }}-${{ needs.generate-tag.outputs.date_sha }}
      -          cache-from: type=registry,ref=flashinfer/flashinfer-ci-${{ matrix.cuda }}:buildcache-${{ matrix.arch }}
      -          cache-to: ${{ github.event_name != 'pull_request' && format('type=registry,ref=flashinfer/flashinfer-ci-{0}:buildcache-{1},mode=max', matrix.cuda, matrix.arch) || '' }}
                 provenance: false
                 sbom: false
      ```
### Dao-AILab/flash-attention
- No commits in the window.
### sgl-project/sglang
- [`56907cb`](https://github.com/sgl-project/sglang/commit/56907cbcb190affa4dfa3473cbe7534622f2f8b1) Move deleted 8-GPU tests to test/manual/ (#18060) (Alison Shao)
  - Stats: +157/-0 (total 157)
  - Files:
    - added: test/manual/models/test_kimi_k2_models.py (+70/-0)
      ```diff
      @@ -0,0 +1,70 @@
      +import unittest
      +from types import SimpleNamespace
      +
      +import requests
      +
      +from sglang.srt.utils import kill_process_tree
      +from sglang.test.few_shot_gsm8k import run_eval as run_eval_few_shot_gsm8k
      +from sglang.test.test_utils import (
      +    DEFAULT_TIMEOUT_FOR_SERVER_LAUNCH,
      +    DEFAULT_URL_FOR_TEST,
      +    CustomTestCase,
      +    is_in_ci,
      +    popen_launch_server,
      +    write_github_step_summary,
      +)
      +
      +
      +class TestKimiK2Thinking(CustomTestCase):
      +    @classmethod
      +    def setUpClass(cls):
      +        cls.model = "moonshotai/Kimi-K2-Thinking"
      +        cls.base_url = DEFAULT_URL_FOR_TEST
      +        other_args = [
      +            "--tp",
      +            "8",
      +            "--trust-remote-code",
      +            "--tool-call-parser",
      +            "kimi_k2",
      +            "--reasoning-parser",
      +            "kimi_k2",
      +            "--model-loader-extra-config",
      +            '{"enable_multithread_load": true, "num_threads": 64}',
      +        ]
      +        cls.process = popen_launch_server(
      +            cls.model,
      +            cls.base_url,
      +            timeout=DEFAULT_TIMEOUT_FOR_SERVER_LAUNCH,
      +            other_args=other_args,
      +        )
      +
      +    @classmethod
      +    def tearDownClass(cls):
      +     ...
      ```
    - added: test/manual/models/test_mistral_large3_basic.py (+87/-0)
      ```diff
      @@ -0,0 +1,87 @@
      +import os
      +import unittest
      +from types import SimpleNamespace
      +
      +from sglang.srt.utils import kill_process_tree
      +from sglang.test.few_shot_gsm8k import run_eval as run_eval_few_shot_gsm8k
      +from sglang.test.send_one import BenchArgs, send_one_prompt
      +from sglang.test.test_utils import (
      +    DEFAULT_TIMEOUT_FOR_SERVER_LAUNCH,
      +    DEFAULT_URL_FOR_TEST,
      +    CustomTestCase,
      +    is_in_ci,
      +    popen_launch_server,
      +    write_github_step_summary,
      +)
      +
      +MISTRAL_LARGE3_MODEL_PATH = "mistralai/Mistral-Large-3-675B-Instruct-2512"
      +
      +
      +class TestMistralLarge3Basic(CustomTestCase):
      +    @classmethod
      +    def setUpClass(cls):
      +        # Set environment variable to disable JIT DeepGemm
      +        os.environ["SGLANG_ENABLE_JIT_DEEPGEMM"] = "0"
      +
      +        cls.model = MISTRAL_LARGE3_MODEL_PATH
      +        cls.base_url = DEFAULT_URL_FOR_TEST
      +        other_args = [
      +            "--tp",
      +            "8",
      +            "--attention-backend",
      +            "trtllm_mla",
      +            "--model-loader-extra-config",
      +            '{"enable_multithread_load": true}',
      +            "--chat-template",
      +            "mistral",
      +        ]
      +        cls.process = popen_launch_server(
      +            c...
      ```
- [`47592a2`](https://github.com/sgl-project/sglang/commit/47592a23c74b1cc1974a9f0166396e811d0a9971) [CI] Fix AMD CI by inlining dummy_grok config (#18044) (sunxxuns)
  - Stats: +27/-1 (total 28)
  - Files:
    - modified: python/sglang/multimodal_gen/__init__.py (+2/-0)
      ```diff
      @@ -4,3 +4,5 @@
       from sglang.multimodal_gen.runtime.entrypoints.diffusion_generator import DiffGenerator
       
       __all__ = ["DiffGenerator", "PipelineConfig", "SamplingParams"]
      +
      +# Trigger multimodal CI tests
      ```
    - modified: scripts/ci/amd/amd_ci_install_dependency.sh (+25/-1)
      ```diff
      @@ -124,7 +124,31 @@ docker cp human-eval ci_sglang:/
       install_with_retry docker exec -w /human-eval ci_sglang pip install --cache-dir=/sgl-data/pip-cache -e .
       
       docker exec -w / ci_sglang mkdir ...
      ```
      - Diff truncated to fit prompt budget.
- [`855dd05`](https://github.com/sgl-project/sglang/commit/855dd0546ce7ba460f23bed08c469e63955228e2) feat: Add Ling Flash v2.0 support for Eagle3 (#15119) (yefei12)
  - Stats: +30/-1 (total 31)
  - Files:
    - modified: python/sglang/srt/models/bailing_moe.py (+30/-1)
      - Diff truncated to fit prompt budget.
- [`3ca29df`](https://github.com/sgl-project/sglang/commit/3ca29dffc72ac8a1000d3dad988fd96ab3339b9d) support qwen3-next eagle3 (#14607) (lukec)
  - Stats: +73/-6 (total 79)
  - Files:
    - modified: python/sglang/srt/models/qwen3_next.py (+73/-6)
      - Diff truncated to fit prompt budget.
- [`9bb1260`](https://github.com/sgl-project/sglang/commit/9bb1260558315c03a6b772e82a6bb146efd7ca6f) [Feature] Support file:// URL format for multimodal inputs (#14490) (Praneth Paruchuri)
  - Stats: +15/-2 (total 17)
  - Files:
    - modified: python/sglang/srt/utils/common.py (+15/-2)
      - Diff truncated to fit prompt budget.
- [`71babde`](https://github.com/sgl-project/sglang/commit/71babdef510ef3d95ea829ec61e5f7c3f9ae7232) Fix CUDA 12 dependency when importing Mooncake in official CUDA 13.x image (#17540) (ZhenshengWu)
  - Stats: +22/-1 (total 23)
  - Files:
    - modified: docker/Dockerfile (+22/-1)
      - Diff truncated to fit prompt budget.
- [`486c7de`](https://github.com/sgl-project/sglang/commit/486c7de39f5c6ec8d651c5c0c662e940fccb63e0) Optimizing all_reduce in RMSNormTP in minimax_m2 (#16483) (Roger Young)
  - Stats: +8/-2 (total 10)
  - Files:
    - modified: python/sglang/srt/models/minimax_m2.py (+8/-2)
      - Diff truncated to fit prompt budget.
- [`9c168fc`](https://github.com/sgl-project/sglang/commit/9c168fcac7f6200d9584ea9a60ee45ce3ac0cc14) Fix Diffusion Request Validation to allow missing input artifacts if the input only contains text (#16610) (Kangyan-Zhou)
  - Stats: +33/-19 (total 52)
  - Files:
    - modified: python/sglang/multimodal_gen/runtime/entrypoints/openai/protocol.py (+1/-0)
      - Diff truncated to fit prompt budget.
    - modified: python/sglang/multimodal_gen/runtime/entrypoints/openai/video_api.py (+32/-19)
      - Diff truncated to fit prompt budget.
- [`4d28cda`](https://github.com/sgl-project/sglang/commit/4d28cda007b0ff34e9f936c4ad1b8ec08ed0574b) [model] Support MiniCPM-V 4.5 (#9610) (tc-mb)
  - Stats: +389/-5 (total 394)
  - Files:
    - modified: python/sglang/srt/models/minicpmv.py (+389/-5)
      - Diff truncated to fit prompt budget.
- [`2c036f1`](https://github.com/sgl-project/sglang/commit/2c036f1eb120b3e175380ef15feef5a49aaea19b) [Bugfix] fix the display error (inconsistent context) (#17699) (linhaifeng)
  - Stats: +2/-2 (total 4)
  - Files:
    - modified: python/sglang/srt/multimodal/processors/step3_vl.py (+1/-1)
      - Diff truncated to fit prompt budget.
    - modified: python/sglang/srt/multiplex/pdmux_context.py (+1/-1)
      - Diff truncated to fit prompt budget.
- [`e5ac622`](https://github.com/sgl-project/sglang/commit/e5ac6229e18662dff4bef85582d8c71dcd885fff) Fix installation script for H200 runners (#18050) (Kangyan-Zhou)
  - Stats: +60/-9 (total 69)
  - Files:
    - modified: scripts/ci/cuda/ci_install_deepep.sh (+36/-6)
      - Diff truncated to fit prompt budget.
    - modified: scripts/ci/cuda/ci_install_dependency.sh (+24/-3)
      - Diff truncated to fit prompt budget.
- [`a0bae4c`](https://github.com/sgl-project/sglang/commit/a0bae4c34349d09b3ad71db71d4fc3f4f32f221e) Migrate 4-GPU/8-GPU workflow jobs to stage-c and add CI registry decorators (#17299) (Alison Shao)
  - Stats: +189/-284 (total 473)
  - Files (truncated):
    - modified: .github/workflows/pr-test.yml (+35/-63)
      - Diff truncated to fit prompt budget.
    - modified: scripts/ci/utils/slash_command_handler.py (+7/-0)
      - Diff truncated to fit prompt budget.
    - renamed: test/registered/4-gpu-models/test_deepseek_v3_cutedsl_4gpu.py (+3/-0)
      - Diff truncated to fit prompt budget.
    - renamed: test/registered/4-gpu-models/test_gpt_oss_4gpu.py (+4/-0)
      - Diff truncated to fit prompt budget.
    - renamed: test/registered/4-gpu-models/test_qwen3_next_models.py (+3/-0)
      - Diff truncated to fit prompt budget.
    - renamed: test/registered/4-gpu-models/test_qwen3_next_models_mtp.py (+3/-0)
      - Diff truncated to fit prompt budget.
    - renamed: test/registered/8-gpu-models/test_deepseek_v32_basic.py (+3/-0)
      - Diff truncated to fit prompt budget.
    - renamed: test/registered/8-gpu-models/test_deepseek_v32_mtp.py (+3/-0)
      - Diff truncated to fit prompt budget.
    - renamed: test/registered/8-gpu-models/test_deepseek_v3_basic.py (+3/-0)
      - Diff truncated to fit prompt budget.
    - renamed: test/registered/8-gpu-models/test_deepseek_v3_mtp.py (+3/-0)
      - Diff truncated to fit prompt budget.
    - renamed: test/registered/8-gpu-models/test_mimo_models.py (+3/-0)
      - Diff truncated to fit prompt budget.
    - renamed: test/registered/amd/test_moriep_small.py (+3/-0)
      - Diff truncated to fit prompt budget.
    - renamed: test/registered/distributed/test_disaggregation_aarch64.py (+3/-0)
      - Diff truncated to fit prompt budget.
    - renamed: test/registered/distributed/test_disaggregation_different_tp.py (+3/-0)
      - Diff truncated to fit prompt budget.
    - renamed: test/registered/distributed/test_disaggregation_dp_attention.py (+3/-0)
      - Diff truncated to fit prompt budget.
    - renamed: test/registered/distributed/test_disaggregation_hybrid_attention.py (+5/-0)
      - Diff truncated to fit prompt budget.
- [`9518048`](https://github.com/sgl-project/sglang/commit/95180484e9a63ff4ceffc3454126dfb16f618b89) Disable test_mla_int8_deepseek_v3.py temporarily (#18057) (Alison Shao)
  - Stats: +5/-1 (total 6)
  - Files:
    - modified: test/registered/mla/test_mla_int8_deepseek_v3.py (+5/-1)
      - Diff truncated to fit prompt budget.
- [`d396650`](https://github.com/sgl-project/sglang/commit/d396650bd29bbf4376a926844194e91ebfcbe679) Fix swa kv cache memory allocation (#18039) (Ke Bao)
  - Stats: +87/-34 (total 121)
  - Files:
    - modified: python/sglang/srt/configs/model_config.py (+16/-6)
      - Diff truncated to fit prompt budget.
    - modified: python/sglang/srt/model_executor/model_runner_kv_cache_mixin.py (+71/-28)
      - Diff truncated to fit prompt budget.
- [`38d275a`](https://github.com/sgl-project/sglang/commit/38d275a9fd7c42353839b88ce83359444fb3cb6c) [BugFix] fix gpt-oss accuracy issue when enabling piecewise cuda graph (#18013) (Minglei Zhu)
  - Stats: +9/-1 (total 10)
  - Files:
    - modified: python/sglang/srt/managers/schedule_batch.py (+9/-1)
      - Diff truncated to fit prompt budget.
- [`1189259`](https://github.com/sgl-project/sglang/commit/11892599f164d1c2da9c489e5f0b922c2ec6a3c4) [metric] Optional extra metric labels (#18049) (Yinghai Lu)
  - Stats: +27/-4 (total 31)
  - Files:
    - modified: python/sglang/srt/managers/scheduler_metrics_mixin.py (+2/-0)
      - Diff truncated to fit prompt budget.
    - modified: python/sglang/srt/managers/tokenizer_manager.py (+2/-0)
      - Diff truncated to fit prompt budget.
    - modified: python/sglang/srt/mem_cache/base_prefix_cache.py (+7/-3)
      - Diff truncated to fit prompt budget.
    - modified: python/sglang/srt/mem_cache/hiradix_cache.py (+8/-1)
      - Diff truncated to fit prompt budget.
    - modified: python/sglang/srt/server_args.py (+8/-0)
      - Diff truncated to fit prompt budget.
### vllm-project/vllm
- [`6720238`](https://github.com/vllm-project/vllm/commit/672023877bfd17a8720ae05ed198456d9b454363) Change defaults for vllm bench startup (#33489) (Luka Govedič)
  - Stats: +3/-3 (total 6)
  - Files:
    - modified: vllm/benchmarks/startup.py (+3/-3)
      - Diff truncated to fit prompt budget.
- [`754a8ca`](https://github.com/vllm-project/vllm/commit/754a8ca9426080a652377b556d466cadb5c3468f) fix: only include Authorization header when OPENAI_API_KEY is set (#33488) (Zack Yu)
  - Stats: +17/-26 (total 43)
  - Files:
    - modified: vllm/benchmarks/lib/endpoint_request_func.py (+17/-26)
      - Diff truncated to fit prompt budget.
- [`302ecf6`](https://github.com/vllm-project/vllm/commit/302ecf64ff22e6e2f59d108e95f0c66d96c9ffc0) [Models]: lfm2_siglip2 return intermediate encoder layers (#33370) (Eduardo Salinas)
  - Stats: +76/-7 (total 83)
  - Files:
    - modified: vllm/model_executor/models/lfm2_siglip2.py (+76/-7)
      - Diff truncated to fit prompt budget.
- [`b6bb284`](https://github.com/vllm-project/vllm/commit/b6bb2842cf30251951ccf6d22f808a9a2884d96e) [Critical] Revert #33110 (#33500) (Cyrus Leung)
  - Stats: +0/-31 (total 31)
  - Files:
    - modified: vllm/v1/engine/input_processor.py (+0/-31)
      - Diff truncated to fit prompt budget.
- [`79b6ec6`](https://github.com/vllm-project/vllm/commit/79b6ec6aab4b3d93e421a15ab41ba6d09faadd8f) [Bugfix] Fix inconsistent handling of cache reset (#33481) (Cyrus Leung)
  - Stats: +35/-11 (total 46)
  - Files:
    - modified: docs/benchmarking/sweeps.md (+1/-1)
      - Diff truncated to fit prompt budget.
    - modified: vllm/benchmarks/sweep/server.py (+9/-5)
      - Diff truncated to fit prompt budget.
    - modified: vllm/entrypoints/openai/engine/serving.py (+0/-4)
      - Diff truncated to fit prompt budget.
    - modified: vllm/v1/engine/async_llm.py (+1/-0)
      - Diff truncated to fit prompt budget.
    - modified: vllm/v1/worker/gpu/mm/encoder_runner.py (+16/-0)
      - Diff truncated to fit prompt budget.
    - modified: vllm/v1/worker/gpu/model_runner.py (+4/-1)
      - Diff truncated to fit prompt budget.
    - modified: vllm/v1/worker/gpu_model_runner.py (+4/-0)
      - Diff truncated to fit prompt budget.
- [`d6416fd`](https://github.com/vllm-project/vllm/commit/d6416fdde93476afc51a51e391a9b44d2c25df2f) pin LMCache to v0.3.9 or greater with vLLM v0.15.0 (#33440) (Greg Pereira)
  - Stats: +1/-1 (total 2)
  - Files:
    - modified: requirements/kv_connectors.txt (+1/-1)
      - Diff truncated to fit prompt budget.
- [`0fb3157`](https://github.com/vllm-project/vllm/commit/0fb3157267127a69667f5c9b92d030c0de10614a) [ROCm][CI] Update huggingface-hub pin (#33492) (Andreas Karatzas)
  - Stats: +2/-0 (total 2)
  - Files:
    - modified: requirements/rocm-test.txt (+2/-0)
      - Diff truncated to fit prompt budget.
- [`a358e4d`](https://github.com/vllm-project/vllm/commit/a358e4dffe10ebff2c44434958bcc190ad2e5542) [Refactor] Make Renderer an abstract class (#33479) (Cyrus Leung)
  - Stats: +49/-50 (total 99)
  - Files:
    - modified: vllm/engine/protocol.py (+2/-2)
      - Diff truncated to fit prompt budget.
    - modified: vllm/renderers/__init__.py (+2/-2)
      - Diff truncated to fit prompt budget.
    - modified: vllm/renderers/deepseek_v32.py (+4/-6)
      - Diff truncated to fit prompt budget.
    - modified: vllm/renderers/grok2.py (+4/-6)
      - Diff truncated to fit prompt budget.
    - modified: vllm/renderers/hf.py (+4/-5)
      - Diff truncated to fit prompt budget.
    - modified: vllm/renderers/mistral.py (+4/-6)
      - Diff truncated to fit prompt budget.
    - modified: vllm/renderers/protocol.py (+16/-8)
      - Diff truncated to fit prompt budget.
    - modified: vllm/renderers/registry.py (+3/-3)
      - Diff truncated to fit prompt budget.
    - modified: vllm/renderers/terratorch.py (+4/-6)
      - Diff truncated to fit prompt budget.
    - modified: vllm/v1/engine/async_llm.py (+2/-2)
      - Diff truncated to fit prompt budget.
    - modified: vllm/v1/engine/input_processor.py (+2/-2)
      - Diff truncated to fit prompt budget.
    - modified: vllm/v1/engine/llm_engine.py (+2/-2)
      - Diff truncated to fit prompt budget.
- [`0797811`](https://github.com/vllm-project/vllm/commit/079781177ae4c9fba429bf093cae73cf4cfae7a8) fix: Add SM120 (RTX Blackwell) support for FlashInfer CUTLASS NVFP4 MoE kernels (#33417) (René Honig)
  - Stats: +22/-110 (total 132)
  - Files:
    - modified: vllm/model_executor/layers/fused_moe/cutlass_moe.py (+6/-1)
      - Diff truncated to fit prompt budget.
    - modified: vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe.py (+2/-1)
      - Diff truncated to fit prompt budget.
    - modified: vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py (+14/-13)
      - Diff truncated to fit prompt budget.
    - modified: vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py (+0/-1)
      - Diff truncated to fit prompt budget.
    - modified: vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py (+0/-27)
      - Diff truncated to fit prompt budget.
    - removed: vllm/model_executor/layers/quantization/utils/nvfp4_moe_support.py (+0/-67)
      - Diff truncated to fit prompt budget.
- [`63c0889`](https://github.com/vllm-project/vllm/commit/63c0889416f0d4c3979c4046c6bf41c43143080c) [Misc] Fix flashinfer related tests (#33462) (Roy Wang)
  - Stats: +9/-8 (total 17)
  - Files:
    - modified: tests/kernels/moe/test_moe.py (+1/-1)
      - Diff truncated to fit prompt budget.
    - modified: tests/kernels/quantization/test_flashinfer_nvfp4_scaled_mm.py (+1/-1)
      - Diff truncated to fit prompt budget.
    - modified: tests/kernels/quantization/test_fp8_quant.py (+2/-2)
      - Diff truncated to fit prompt budget.
    - modified: vllm/model_executor/layers/quantization/utils/nvfp4_utils.py (+4/-3)
      - Diff truncated to fit prompt budget.
    - modified: vllm/utils/flashinfer.py (+1/-1)
      - Diff truncated to fit prompt budget.
- [`1e86c80`](https://github.com/vllm-project/vllm/commit/1e86c802d473b4ca4e889a1b2585a05aad7220ba) Fix grammar (#33121) (smashyalts)
  - Stats: +1/-1 (total 2)
  - Files:
    - modified: vllm/v1/worker/cpu_worker.py (+1/-1)
      - Diff truncated to fit prompt budget.
- [`fedf643`](https://github.com/vllm-project/vllm/commit/fedf64332e9940c8119f584a74b9c08bab6852b7) [Bugfix]: Fix display errors in TORCH_CHECK messages (#32942) (linhaifeng)
  - Stats: +8/-8 (total 16)
  - Files:
    - modified: csrc/cpu/sgl-kernels/gemm.cpp (+1/-1)
      - Diff truncated to fit prompt budget.
    - modified: csrc/cpu/sgl-kernels/gemm_fp8.cpp (+1/-1)
      - Diff truncated to fit prompt budget.
    - modified: csrc/cpu/sgl-kernels/gemm_int8.cpp (+1/-1)
      - Diff truncated to fit prompt budget.
    - modified: csrc/cpu/sgl-kernels/moe.cpp (+2/-2)
      - Diff truncated to fit prompt budget.
    - modified: csrc/cpu/sgl-kernels/moe_int8.cpp (+2/-2)
      - Diff truncated to fit prompt budget.
    - modified: csrc/moe/marlin_moe_wna16/ops.cu (+1/-1)
      - Diff truncated to fit prompt budget.
- [`2238a12`](https://github.com/vllm-project/vllm/commit/2238a12c13b9a1e27f132473a0fff1b4b233b1dc) [Misc] support collect_env for endpoint /server_info (#33246) (Xiao Yang)
  - Stats: +9/-0 (total 9)
  - Files:
    - modified: vllm/entrypoints/serve/instrumentator/server_info.py (+9/-0)
      - Diff truncated to fit prompt budget.
- [`ce0afe2`](https://github.com/vllm-project/vllm/commit/ce0afe2451b0f8a04547514f9efcac98e39beab0) Update `huggingface-hub` pin for the last time before Transformers v5 (#33473) (Harry Mellor)
  - Stats: +1/-1 (total 2)
  - Files:
    - modified: requirements/test.txt (+1/-1)
      - Diff truncated to fit prompt budget.
- [`88c3e11`](https://github.com/vllm-project/vllm/commit/88c3e114d8f564d1e88ff2ef5be56a979792e90b) [Refactor] Move MM data parsing outside processor (#33408) (Cyrus Leung)
  - Stats: +228/-139 (total 367)
  - Files (truncated):
    - modified: tests/models/multimodal/processing/test_common.py (+6/-5)
      - Diff truncated to fit prompt budget.
    - modified: tests/models/multimodal/processing/test_gemma3.py (+5/-1)
      - Diff truncated to fit prompt budget.
    - modified: tests/models/multimodal/processing/test_glm4_1v.py (+15/-3)
      - Diff truncated to fit prompt budget.
    - modified: tests/models/multimodal/processing/test_h2ovl.py (+5/-1)
      - Diff truncated to fit prompt budget.
    - modified: tests/models/multimodal/processing/test_idefics3.py (+5/-1)
      - Diff truncated to fit prompt budget.
    - modified: tests/models/multimodal/processing/test_internvl.py (+5/-1)
      - Diff truncated to fit prompt budget.
    - modified: tests/models/multimodal/processing/test_llama4.py (+5/-1)
      - Diff truncated to fit prompt budget.
    - modified: tests/models/multimodal/processing/test_llava_next.py (+5/-1)
      - Diff truncated to fit prompt budget.
    - modified: tests/models/multimodal/processing/test_llava_onevision.py (+5/-1)
      - Diff truncated to fit prompt budget.
    - modified: tests/models/multimodal/processing/test_minimax_vl_01.py (+10/-2)
      - Diff truncated to fit prompt budget.
    - modified: tests/models/multimodal/processing/test_nemotron_vl.py (+5/-1)
      - Diff truncated to fit prompt budget.
    - modified: tests/models/multimodal/processing/test_phi3v.py (+5/-1)
      - Diff truncated to fit prompt budget.
    - modified: tests/models/multimodal/processing/test_phi4mm.py (+5/-1)
      - Diff truncated to fit prompt budget.
    - modified: tests/models/multimodal/processing/test_qwen2_vl.py (+10/-2)
      - Diff truncated to fit prompt budget.
    - modified: tests/models/multimodal/processing/test_qwen3_omni.py (+10/-2)
      - Diff truncated to fit prompt budget.
    - modified: tests/models/multimodal/processing/test_smolvlm.py (+5/-1)
      - Diff truncated to fit prompt budget.
- [`92924b2`](https://github.com/vllm-project/vllm/commit/92924b2dddebc6dc80b3fff6c84ff095530b8e58) [Deprecation] Remove deprecated items related to pooling (#33477) (Cyrus Leung)
  - Stats: +52/-105 (total 157)
  - Files:
    - modified: docs/models/pooling_models.md (+0/-9)
      - Diff truncated to fit prompt budget.
    - modified: vllm/config/model.py (+1/-17)
      - Diff truncated to fit prompt budget.
    - modified: vllm/config/pooler.py (+6/-45)
      - Diff truncated to fit prompt budget.
    - modified: vllm/entrypoints/pooling/base/protocol.py (+20/-14)
      - Diff truncated to fit prompt budget.
    - modified: vllm/entrypoints/pooling/pooling/protocol.py (+19/-3)
      - Diff truncated to fit prompt budget.
    - modified: vllm/entrypoints/pooling/score/protocol.py (+1/-2)
      - Diff truncated to fit prompt budget.
    - modified: vllm/model_executor/models/config.py (+2/-2)
      - Diff truncated to fit prompt budget.
    - modified: vllm/pooling_params.py (+3/-13)
      - Diff truncated to fit prompt budget.

## Notes
- Diff content truncated to fit prompt budget.
- OpenRouter summarize failed: OpenRouter 429 Too Many Requests: {"error":{"message":"Provider returned error","code":429,"metadata":{"raw":"moonshotai/kimi-k2:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations","provider_name":"OpenInference","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
