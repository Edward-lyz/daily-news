# AI 基础设施动态简报 - 2026年1月31日

## 摘要

本期简报重点关注三个主要AI基础设施项目的最新动态：FlashInfer、SGLang和VLLM。FlashInfer进行了一次重要功能回退，SGLang持续扩展模型支持并优化性能，VLLM则专注于模型支持和代码重构。

## 具体内容分析

### FlashInfer
FlashInfer进行了一次重要回退，撤销了对Fused MoE非门控Relu2 NVFP4 & FP8和Nemotron的支持。这表明该实现可能存在严重问题。具体变更包括：
- 简化激活类型，从多种选项（Gelu、Relu、Silu、Swiglu、Geglu、SwigluBias、Relu2、Identity）缩减为仅两种（SwiGlu、GeGlu）
- 移除了对多种激活类型的支持代码
- 更新CI Docker标签并禁用构建缓存

**PR链接**: [https://github.com/flashinfer-ai/flashinfer/pull/2451](https://github.com/flashinfer-ai/flashinfer/pull/2451)

### SGLang
SGLang在本期有多个重要更新：
- 新增模型支持：Kimi-K2-Thinking、Mistral-Large3-675B-Instruct-2512、Ling Flash v2.0、Qwen3-next、MiniCPM-V 4.5
- 优化内存分配：修复SWA KV缓存内存分配问题
- 改进多模态处理：支持file:// URL格式，修复Diffusion请求验证
- 测试策略调整：将8-GPU测试移至手动测试目录
- 性能优化：优化minimax_m2中的RMSNormTP的all_reduce操作

**PR链接**: [https://github.com/sgl-project/sglang/pull/18060](https://github.com/sgl-project/sglang/pull/18060)

### VLLM
VLLM的更新主要集中在：
- 模型支持：新增LFM2 SigLip2模型
- 硬件支持：为RTX Blackwell (SM120)添加FlashInfer CUTLASS NVFP4 MoE内核支持
- 代码重构：将Renderer重构为抽象类，移除已弃用的池化相关项
- 关键修复：回退了一个可能导致问题的输入处理更改，修复缓存处理不一致问题
- 多模态改进：将多模态数据解析移出处理器

**PR链接**: [https://github.com/vllm-project/vllm/pull/33500](https://github.com/vllm-project/vllm/pull/33500)

## 总结

- **FlashInfer**: 高影响变更，回退了重要功能，可能影响依赖该功能的用户
- **SGLang**: 稳定发展，持续扩展模型支持并优化性能，风险较低
- **VLLM**: 专注稳定性和代码质量，通过重构和修复提高系统可靠性

各项目均显示出活跃的开发状态，但FlashInfer的回退表明复杂功能实现需要更加谨慎。
