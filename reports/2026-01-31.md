# Daily AI Infra Report - 2026-01-31

Generated from raw data because summarization was unavailable.

## Papers
- No papers matched the date filter.

## Repos
### deepseek-ai/DeepGEMM
- No commits in the window.
### deepseek-ai/FlashMLA
- No commits in the window.
### NVIDIA/cutlass
- No commits in the window.
### flashinfer-ai/flashinfer
- [`87a45d1`](https://github.com/flashinfer-ai/flashinfer/commit/87a45d131dc6518493718e20086cc665ed46da4f) Revert "feat: Support Fused MoE non gated Relu2 NVFP4 & FP8 and support Nemotron" (#2451) (nv-yunzheq)
  - Stats: +304/-610 (total 914)
  - Files (truncated):
    - modified: benchmarks/bench_trtllm_gen_fused_moe_autotuner.py (+2/-24)
      ```diff
      @@ -4,7 +4,7 @@
       import numpy as np
       from flashinfer import (
           RoutingMethodType,
      -    ActivationType,
      +    GatedActType,
           fp4_quantize,
           mxfp8_quantize,
       )
      @@ -17,7 +17,6 @@
       from flashinfer.autotuner import autotune
       from flashinfer.testing.utils import bench_gpu_time
       from flashinfer.utils import device_support_pdl
      -from routines.flashinfer_benchmark_utils import enum_type
       
       FLOAT8_E4M3_MAX = torch.finfo(torch.float8_e4m3fn).max
       FLOAT4_E2M1_MAX = 6.0
      @@ -40,7 +39,6 @@ def bench_trtllm_gen_fused_moe_autotuner_fp8(
           top_k: int,
           warmups: int,
           iterations: int,
      -    activation_type: ActivationType,
       ):
           device = torch.device("cuda:0")
           enable_pdl = device_support_pdl(device)
      @@ -99,10 +97,6 @@ def bench_trtllm_gen_fused_moe_autotuner_fp8(
           )
       
           if is_block_scale:
      -        if activation_type != ActivationType.Swiglu:
      -            raise ValueError(
      -                "Only Swiglu activation is supported for FP8 block scale MoE."
      -            )
               fn = lambda: trtllm_fp8_block_scale_moe(
                   routing_logits,
                   routing_bias,
      @@ -150,7 +144,6 @@ def bench_trtllm_gen_fused_moe_autotuner_fp8(
                   RoutingMethodTyp...
      ```
    - modified: benchmarks/routines/flashinfer_benchmark_utils.py (+0/-16)
      ```diff
      @@ -1,4 +1,3 @@
      -import argparse
       import torch
       
       from flashinfer.testing.utils import set_seed
      @@ -454,18 +453,3 @@ def filter_backends_by_compute_capability(backends, routine, device):
                   f"[WARNING] {backend} for routine {routine} is not supported on compute capability {compute_capability}. Skipping."
               )
           return backends
      -
      -
      -def enum_type(enum_class):
      -    """Generic factory for argparse enum types."""
      -
      -    def converter(value):
      -        try:
      -            lower_name_to_member = {m.name.lower(): m for m in enum_class}
      -            return lower_name_to_member[value.lower()]
      -        except KeyError as e:
      -            raise argparse.ArgumentTypeError(
      -                f"Invalid value '{value}'. Must be one of: {', '.join([m.name for m in enum_class])}"
      -            ) from e
      -
      -    return converter
      ```
    - modified: benchmarks/routines/moe.py (+15/-14)
      ```diff
      @@ -5,7 +5,6 @@
       import torch
       
       import flashinfer
      -from flashinfer import ActivationType
       from flashinfer.autotuner import autotune
       from flashinfer.fused_moe import (
           trtllm_fp4_block_scale_moe,
      @@ -22,7 +21,6 @@
       
       from .flashinfer_benchmark_utils import (
           dtype_str_to_torch_dtype,
      -    enum_type,
           get_device,
           print_perf_metrics,
           filter_backends_by_compute_capability,
      @@ -172,12 +170,12 @@ def parse_moe_args(line, parser):
               help="Data type of the weights (before quantization).",
           )
           parser.add_argument(
      -        "--activation-type",
      -        type=enum_type(ActivationType),
      -        metavar=str([e.name for e in ActivationType]),
      +        "--gated_act",
      +        type=str,
               required=False,
      -        default=ActivationType.Swiglu,
      -        help=f"Type of activation function: {[e.name for e in ActivationType]}",
      +        default="swiglu",
      +        choices=["swiglu", "geglu"],
      +        help="Type of gated activation function: swiglu | geglu.",
           )
           parser.add_argument(
               "--autotune",
      @@ -244,6 +242,13 @@ def parse_moe_args(line, parser):
           }
           args.routing_method_type = routing_method_name_to_type[args.routing_meth...
      ```
    - modified: csrc/trtllm_batched_gemm_runner.cu (+2/-8)
      ```diff
      @@ -101,16 +101,14 @@ TrtllmGenBatchedGemmRunner::TrtllmGenBatchedGemmRunner(
               options.mTransposeMmaOutput == mOptions.transposeMmaOutput &&
               (!doesRouteImplUseNoRoute(options.mRouteImpl)) == mOptions.routeAct &&
               options.mFusedAct == mOptions.fusedAct && options.mIsStaticBatch == mOptions.staticBatch &&
      -        tileSize == mOptions.tileSize && options.mUseShuffledMatrix == mOptions.useShuffledMatrix &&
      +        tileSize == mOptions.tileSize &&
      +        options.mUseShuffledMatrix == mOptions.useShuffledMatrixA &&
               options.mLayoutA == mOptions.weightLayout) {
             if (options.mFusedAct) {
               if (options.mActType != static_cast<batchedGemm::gemmGatedAct::ActType>(mOptions.actType)) {
                 continue;
               }
             }
      -      if ((int64_t)options.mEltwiseActType != (int64_t)mOptions.eltwiseActType) {
      -        continue;
      -      }
       
             if (mOptions.transposeMmaOutput && options.mEpilogueTileM == mOptions.epilogueTileM) {
               mPassingConfigIndices.push_back(i);
      @@ -124,8 +122,6 @@ TrtllmGenBatchedGemmRunner::TrtllmGenBatchedGemmRunner(
                   << ", mDtypeB: " << tg::dtypeToString(mOptions.dtypeB)
                   << ", mDtypeC: ...
      ```
    - modified: csrc/trtllm_fused_moe_kernel_launcher.cu (+45/-44)
      ```diff
      @@ -36,7 +36,7 @@
       namespace flashinfer {
       
       namespace btg = batchedGemm::trtllm::gen;
      -using tensorrt_llm::kernels::trtllmgen_moe::MoE::ActivationType;
      +using tensorrt_llm::kernels::trtllmgen_moe::MoE::GatedActType;
       using tensorrt_llm::kernels::trtllmgen_moe::Routing::RoutingMethodType;
       using tvm::ffi::Array;
       using tvm::ffi::Optional;
      @@ -109,7 +109,7 @@ class FusedMoeLauncher {
         btg::Dtype mDtypeWeights{btg::Dtype::Bfloat16};
         btg::Dtype mRoutingBiasDtype{
             btg::Dtype::Bfloat16};  // Dtype for expert weights in routing, based on routing bias
      -  ActivationType activation_type{ActivationType::Swiglu};
      +  GatedActType gated_act_type{GatedActType::SwiGlu};
       
        public:
         // Constructor that initializes all TensorView members
      @@ -134,14 +134,14 @@ class FusedMoeLauncher {
               weight_layout{batchedGemm::gemm::MatrixLayout::MajorK},
               mDtypeAct{btg::Dtype::Bfloat16},
               mDtypeWeights{btg::Dtype::Bfloat16},
      -        activation_type{ActivationType::Swiglu} {}
      +        gated_act_type{GatedActType::SwiGlu} {}
       
        protected:
         // Initialize common data necessary for later.
         // May throw exception from TVM_FFI_ICHECK.
         void init_common(std::unique_ptr<te...
      ```
    - modified: csrc/trtllm_fused_moe_routing_deepseek.cu (+23/-44)
      ```diff
      @@ -14,7 +14,6 @@
        * limitations under the License.
        */
       
      -#include <algorithm>
       #include <cmath>
       
       #include "flashinfer/exception.h"
      @@ -26,14 +25,10 @@ namespace routingDeepSeek {
       
       ////////////////////////////////////////////////////////////////////////////////////////////////////
       
      -static constexpr int NumNemotronExperts = 512;
       static constexpr int NumKimiK2Experts = 384;
       static constexpr int NumDeepseekExperts = 256;
      -static constexpr int MaxSupportedExpertCount =
      -    std::max({NumNemotronExperts, NumKimiK2Experts, NumDeepseekExperts});
       static constexpr int NumTopGroupScores = 2;
      -static constexpr int DefaultMaxNumTopExperts = 8;
      -static constexpr int MaxSupportedTopExperts = 22;
      +static constexpr int MaxNumTopExperts = 8;
       static constexpr int MaxNumTopGroups = 4;
       static constexpr int MaxNumGroups = 8;
       
      @@ -122,8 +117,8 @@ __global__ void routingMainKernel(KernelParams params) {
           int32_t topGroupIdx[MaxNumTopGroups];
           float expertScoreGroup[MaxNumTopGroups];
           int32_t expertIdxGroup[MaxNumTopGroups];
      -    float topScores[KernelParams::MaxNumTopExperts];  // bound of params.mTopK
      -    int32_t topExperts[KernelParams::MaxNumTopExperts];
      +    float topSc...
      ```
    - modified: csrc/trtllm_fused_moe_runner.cu (+34/-95)
      ```diff
      @@ -60,7 +60,7 @@ void Runner::run(void* routingLogits, void* routingBias, int32_t numTokens, int3
                        bool useRoutingScalesOnInput, bool useDeepSeekFp8,
                        RoutingMethodType routingMethodType, cudaStream_t stream) {
         if (routingMethodType == RoutingMethodType::DeepSeekV3) {
      -    FLASHINFER_CHECK(topK <= 22, "For DeepSeek routing method, must have topK <= 22");
      +    FLASHINFER_CHECK(topK <= 8, "For DeepSeek routing method, must have topK <= 8");
           FLASHINFER_CHECK(topkGroup <= 4, "For DeepSeek routing method, must have topkGroup <= 4");
           moe::dev::routing::routingDeepSeek::Data routingData;
           routingData.mDtypeExpW =
      @@ -189,49 +189,13 @@ void Runner::run(void* routingLogits, void* routingBias, int32_t numTokens, int3
       
       namespace PermuteGemm1 {
       
      -using tensorrt_llm::kernels::trtllmgen_moe::MoE::ActivationType;
      -using tensorrt_llm::kernels::trtllmgen_moe::MoE::isGatedActivation;
      -using tensorrt_llm::kernels::trtllmgen_moe::MoE::serializeActivationType;
      -
      -static inline ActType activationTypeToGatedActType(ActivationType actType) {
      -  switch (actType) {
      -    case ActivationType::Swiglu:
      -      return ActType::SwiGlu;
      -    case ActivationT...
      ```
    - modified: flashinfer/__init__.py (+1/-1)
      ```diff
      @@ -75,8 +75,8 @@
       )
       from .fp8_quantization import mxfp8_dequantize_host, mxfp8_quantize
       from .fused_moe import (
      -    ActivationType,
           RoutingMethodType,
      +    GatedActType,
           cutlass_fused_moe,
           reorder_rows_for_gated_act_gemm,
           trtllm_fp4_block_scale_moe,
      ```
    - modified: flashinfer/fused_moe/__init__.py (+2/-2)
      ```diff
      @@ -15,8 +15,8 @@
       """
       
       from .core import (
      -    ActivationType,
           RoutingMethodType,
      +    GatedActType,
           WeightLayout,
           convert_to_block_layout,
           cutlass_fused_moe,
      @@ -40,8 +40,8 @@
       )
       
       __all__ = [
      -    "ActivationType",
           "RoutingMethodType",
      +    "GatedActType",
           "WeightLayout",
           "convert_to_block_layout",
           "cutlass_fused_moe",
      ```
    - modified: flashinfer/fused_moe/core.py (+31/-55)
      ```diff
      @@ -173,6 +173,15 @@ class WeightLayout(IntEnum):
           BlockMajorK = 2
       
       
      +# The type of gated activation function
      +# Please keep this in sync with the counterpart defined in include/flashinfer/trtllm/fused_moe/runner.h
      +class GatedActType(IntEnum):
      +    # SwiGlu
      +    SwiGlu = 0
      +    # GeGlu
      +    GeGlu = 1
      +
      +
       @functools.cache
       def is_trtllm_moe_supported(
           dtype_weights: DtypeTrtllmGen,
      @@ -212,16 +221,12 @@ def _maybe_get_cached_w3_w1_permute_indices(
           dst_w3_w1_weight: torch.Tensor,
           epilogue_tile_m: int,
           num_elts_per_sf: Union[None, int] = None,
      -    is_gated_act_gemm: bool = True,
       ) -> torch.Tensor:
           # Create a unique cache key (weight_type, weight_shape)
           cache_key = ("w3_w1", dst_w3_w1_weight.shape)
           if cache_key not in _cache_permute_indices:
               # Get permute indices and chain them together
      -        if is_gated_act_gemm:
      -            permute0 = get_reorder_rows_for_gated_act_gemm_row_indices(dst_w3_w1_weight)
      -        else:
      -            permute0 = torch.arange(dst_w3_w1_weight.shape[0], dtype=torch.long)
      +        permute0 = get_reorder_rows_for_gated_act_gemm_row_indices(dst_w3_w1_weight)
               if num_elts_per_sf is None:
              ...
      ```
    - modified: include/flashinfer/trtllm/batched_gemm/KernelRunner.h (+1/-17)
      ```diff
      @@ -47,35 +47,19 @@ enum class ActType {
         GeGlu,
       };
       
      -// Type of the element-wise activation to apply after the Gemm
      -enum class EltwiseActType {
      -  None = 0,
      -  // Gelu is defined as the following operation:
      -  // act = x0 * phi(x0)
      -  // where x0 is the output of the Gemm
      -  // phi is the CDF of standard normal distribution approximated by
      -  // phi(x) = 0.5 * (1 + tanh(0.7978845608028654 * (x + 0.044715 * x * x * x)))
      -  Gelu,
      -  // Relu2 (also known as squared Relu) is defined as the following operation:
      -  // act = relu(x0) ^ 2
      -  // where x0 is the output of the Gemm.
      -  Relu2,
      -};
      -
       struct TrtllmGenBatchedGemmRunnerOptions {
         batchedGemm::trtllm::gen::Dtype dtypeA;
         batchedGemm::trtllm::gen::Dtype dtypeB;
         batchedGemm::trtllm::gen::Dtype dtypeC;
         ActType actType{ActType::SwiGlu};
      -  EltwiseActType eltwiseActType{EltwiseActType::None};
         bool deepSeekFp8{false};
         bool fusedAct{false};
         bool routeAct{false};
         bool staticBatch{false};
         bool transposeMmaOutput{false};
         int32_t tileSize{8};
         int32_t epilogueTileM{128};
      -  bool useShuffledMatrix{false};
      +  bool useShuffledMatrixA{false};
         batchedGemm::gemm::MatrixLayout weightLayout{batchedGemm::gemm...
      ```
    - modified: include/flashinfer/trtllm/fused_moe/DevKernel.h (+48/-57)
      ```diff
      @@ -169,65 +169,56 @@ namespace moe::dev {
           FLASHINFER_WARN("Unsupported dtypeExpW");                                                      \
         }
       
      -#define LAUNCH_ROUTING_DEEPSEEK_WITH_EXTRA_FLAG(data, coopLaunch, kernel, numBlocks, numThreads,  \
      -                                                smemSize, stream, extraFlag, numExperts,          \
      -                                                numTopExperts)                                    \
      -  if (data.mDtypeScore == tg::Dtype::Fp32 && data.mDtypeBias == tg::Dtype::Fp32 &&                \
      -      data.mDtypeExpW == tg::Dtype::Fp32) {                                                       \
      -    LAUNCH_TILEN(data, coopLaunch,                                                                \
      -                 LAUNCH_ESC(float, float, float, numExperts, numTopExperts, extraFlag), kernel,   \
      -                 numBlocks, numThreads, smemSize, stream);                                        \
      -  } else if (data.mDtypeScore == tg::Dtype::Fp32 && data.mDtypeBias == tg::Dtype::Fp32 &&         \
      -             data.mDtypeExpW == tg::Dtype::Bfloat16) {                                            \
      -    LAUNCH_TILEN(data, coopLaunch,...
      ```
    - modified: include/flashinfer/trtllm/fused_moe/RoutingKernel.h (+2/-3)
      ```diff
      @@ -176,15 +176,14 @@ struct Data : public DataBase {
         bool mUseRoutingSoftmax;
       };
       
      -template <typename InputT_, typename BiasT_, typename OutputT_, int MaxNumExperts_,
      -          int MaxNumTopExperts_, bool UseGroups_, bool isPow2_, bool UsePdl_>
      +template <typename InputT_, typename BiasT_, typename OutputT_, int MaxNumExperts_, bool UseGroups_,
      +          bool isPow2_, bool UsePdl_>
       struct KernelParams : public KernelParamsBase<InputT_, OutputT_, MaxNumExperts_, isPow2_, UsePdl_> {
         using InputT = InputT_;
         using BiasT = BiasT_;
         using OutputT = OutputT_;
       
         static constexpr bool UseGroups = UseGroups_;
      -  static constexpr int MaxNumTopExperts = MaxNumTopExperts_;
       
         PackedScoreIdx<OutputT>* mPtrTopKPacked = nullptr;
      ```
    - modified: include/flashinfer/trtllm/fused_moe/runner.h (+17/-43)
      ```diff
      @@ -136,56 +136,33 @@ class Runner {
       }  // namespace Routing
       
       namespace MoE {
      -// The type of activation function
      +// The type of gated activation function
       // Please keep this in sync with the counterpart defined in flashinfer/flashinfer/fused_moe/core.py
      -enum class ActivationType : int64_t {
      -  Gelu = 0,
      -  Relu = 1,
      -  Silu = 2,
      -  Swiglu = 3,
      -  Geglu = 4,
      -  SwigluBias = 5,
      -  Relu2 = 6,
      -  Identity = 7,
      -  InvalidType = 8,  // Must be last
      +enum class GatedActType : int64_t {
      +  // SwiGlu
      +  SwiGlu = 0,
      +  // GeGlu
      +  GeGlu = 1,
       };
       
      -inline std::string serializeActivationType(ActivationType activationType) {
      -  switch (activationType) {
      -    case ActivationType::Gelu:
      -      return "Gelu";
      -    case ActivationType::Relu:
      -      return "Relu";
      -    case ActivationType::Silu:
      -      return "Silu";
      -    case ActivationType::Swiglu:
      -      return "Swiglu";
      -    case ActivationType::Geglu:
      -      return "Geglu";
      -    case ActivationType::SwigluBias:
      -      return "SwigluBias";
      -    case ActivationType::Relu2:
      -      return "Relu2";
      -    case ActivationType::Identity:
      -      return "Identity";
      +inline std::string serializeGatedActType(GatedActType gatedActType) {
      +  switch...
      ```
    - modified: tests/moe/test_dpsk_fused_moe_fp8.py (+2/-2)
      ```diff
      @@ -8,7 +8,7 @@
           trtllm_fp8_block_scale_moe,
       )
       from .utils import skip_checks, QuantMode
      -from flashinfer import ActivationType
      +from flashinfer import GatedActType
       
       
       def dequant_fp8_block_scaled(
      @@ -616,7 +616,7 @@ def __init__(self):
               moe_impl=moe_impl,
               routing_config=routing_config,
               weight_processing=weight_processing,
      -        activation_type=ActivationType.Swiglu,
      +        gated_act_type=GatedActType.SwiGlu,
               num_tokens=seq_len,
               hidden_size=7168,  # DeepSeek-V3 hidden size
               intermediate_size=intermediate_size,
      ```
    - modified: tests/moe/test_trtllm_gen_fused_moe.py (+68/-152)
      ```diff
      @@ -22,8 +22,8 @@
       from torch.nn import functional as F
       
       from flashinfer import (
      -    ActivationType,
           RoutingMethodType,
      +    GatedActType,
           e2m1_and_ufp8sf_scale_to_float,
           fp4_quantize,
           mxfp8_dequantize_host,
      @@ -46,7 +46,7 @@
           get_w2_permute_indices_with_cache,
           _maybe_get_cached_w3_w1_permute_indices,
       )
      -from .utils import is_gated_activation, skip_checks, QuantMode
      +from .utils import skip_checks, QuantMode
       
       
       # Max num tokens to tune for trtllm-gen fused moe
      @@ -209,7 +209,7 @@ def _run_moe_computation(self, runtime_args):
                   local_num_experts=self.config["num_experts"],
                   routed_scaling_factor=self.config["routed_scaling"],
                   routing_method_type=self.config["routing_method_type"],
      -            activation_type=self.config["activation_type"],
      +            gated_act_type=self.config["gated_act_type"],
                   do_finalize=True,
                   tune_max_num_tokens=TUNE_MAX_NUM_TOKENS,
               )
      @@ -227,12 +227,6 @@ class Moe(ABC):
           def __init__(self):
               self.name = self.__class__.__name__
       
      -    @property
      -    @abstractmethod
      -    def quant_mode(self) -> QuantMode:
      -        """Get the quantization mo...
      ```
- [`f61fa8d`](https://github.com/flashinfer-ai/flashinfer/commit/f61fa8d7fe19683f4592957e7b1003307754dcb2) Update Docker CI tags to 20260131-a52eff1 (#2457) (FlashInfer Bot)
  - Stats: +4/-4 (total 8)
  - Files:
    - modified: ci/docker-tags.yml (+4/-4)
      ```diff
      @@ -1,4 +1,4 @@
      -flashinfer/flashinfer-ci-cu126: 20260114-cc1a362
      -flashinfer/flashinfer-ci-cu128: 20260114-cc1a362
      -flashinfer/flashinfer-ci-cu129: 20260114-cc1a362
      -flashinfer/flashinfer-ci-cu130: 20260114-cc1a362
      +flashinfer/flashinfer-ci-cu126: 20260131-a52eff1
      +flashinfer/flashinfer-ci-cu128: 20260131-a52eff1
      +flashinfer/flashinfer-ci-cu129: 20260131-a52eff1
      +flashinfer/flashinfer-ci-cu130: 20260131-a52eff1
      ```
- [`a52eff1`](https://github.com/flashinfer-ai/flashinfer/commit/a52eff1e837bba6724ebbd7d98445635b8b4835d) ci: CI build workflow should always pull fresh and do not cache (#2454) (Brian K. Ryu)
  - Stats: +2/-2 (total 4)
  - Files:
    - modified: .github/workflows/release-ci-docker.yml (+2/-2)
      ```diff
      @@ -61,10 +61,10 @@ jobs:
                 file: docker/Dockerfile.${{ matrix.cuda }}
                 platforms: linux/${{ matrix.arch }}
                 push: ${{ github.event_name != 'pull_request' }}
      +          pull: true  # Always pull the latest base image
      +          no-cache: true  # Disable all caching
                 tags: |
                   flashinfer/flashinfer-ci-${{ matrix.cuda }}:${{ matrix.arch }}-${{ needs.generate-tag.outputs.date_sha }}
      -          cache-from: type=registry,ref=flashinfer/flashinfer-ci-${{ matrix.cuda }}:buildcache-${{ matrix.arch }}
      -          cache-to: ${{ github.event_name != 'pull_request' && format('type=registry,ref=flashinfer/flashinfer-ci-{0}:buildcache-{1},mode=max', matrix.cuda, matrix.arch) || '' }}
                 provenance: false
                 sbom: false
      ```
### Dao-AILab/flash-attention
- No commits in the window.
### sgl-project/sglang
- [`56907cb`](https://github.com/sgl-project/sglang/commit/56907cbcb190affa4dfa3473cbe7534622f2f8b1) Move deleted 8-GPU tests to test/manual/ (#18060) (Alison Shao)
  - Stats: +157/-0 (total 157)
  - Files:
    - added: test/manual/models/test_kimi_k2_models.py (+70/-0)
      ```diff
      @@ -0,0 +1,70 @@
      +import unittest
      +from types import SimpleNamespace
      +
      +import requests
      +
      +from sglang.srt.utils import kill_process_tree
      +from sglang.test.few_shot_gsm8k import run_eval as run_eval_few_shot_gsm8k
      +from sglang.test.test_utils import (
      +    DEFAULT_TIMEOUT_FOR_SERVER_LAUNCH,
      +    DEFAULT_URL_FOR_TEST,
      +    CustomTestCase,
      +    is_in_ci,
      +    popen_launch_server,
      +    write_github_step_summary,
      +)
      +
      +
      +class TestKimiK2Thinking(CustomTestCase):
      +    @classmethod
      +    def setUpClass(cls):
      +        cls.model = "moonshotai/Kimi-K2-Thinking"
      +        cls.base_url = DEFAULT_URL_FOR_TEST
      +        other_args = [
      +            "--tp",
      +            "8",
      +            "--trust-remote-code",
      +            "--tool-call-parser",
      +            "kimi_k2",
      +            "--reasoning-parser",
      +            "kimi_k2",
      +            "--model-loader-extra-config",
      +            '{"enable_multithread_load": true, "num_threads": 64}',
      +        ]
      +        cls.process = popen_launch_server(
      +            cls.model,
      +            cls.base_url,
      +            timeout=DEFAULT_TIMEOUT_FOR_SERVER_LAUNCH,
      +            other_args=other_args,
      +        )
      +
      +    @classmethod
      +    def tearDownClass(cls):
      +     ...
      ```
    - added: test/manual/models/test_mistral_large3_basic.py (+87/-0)
      ```diff
      @@ -0,0 +1,87 @@
      +import os
      +import unittest
      +from types import SimpleNamespace
      +
      +from sglang.srt.utils import kill_process_tree
      +from sglang.test.few_shot_gsm8k import run_eval as run_eval_few_shot_gsm8k
      +from sglang.test.send_one import BenchArgs, send_one_prompt
      +from sglang.test.test_utils import (
      +    DEFAULT_TIMEOUT_FOR_SERVER_LAUNCH,
      +    DEFAULT_URL_FOR_TEST,
      +    CustomTestCase,
      +    is_in_ci,
      +    popen_launch_server,
      +    write_github_step_summary,
      +)
      +
      +MISTRAL_LARGE3_MODEL_PATH = "mistralai/Mistral-Large-3-675B-Instruct-2512"
      +
      +
      +class TestMistralLarge3Basic(CustomTestCase):
      +    @classmethod
      +    def setUpClass(cls):
      +        # Set environment variable to disable JIT DeepGemm
      +        os.environ["SGLANG_ENABLE_JIT_DEEPGEMM"] = "0"
      +
      +        cls.model = MISTRAL_LARGE3_MODEL_PATH
      +        cls.base_url = DEFAULT_URL_FOR_TEST
      +        other_args = [
      +            "--tp",
      +            "8",
      +            "--attention-backend",
      +            "trtllm_mla",
      +            "--model-loader-extra-config",
      +            '{"enable_multithread_load": true}',
      +            "--chat-template",
      +            "mistral",
      +        ]
      +        cls.process = popen_launch_server(
      +            c...
      ```
- [`47592a2`](https://github.com/sgl-project/sglang/commit/47592a23c74b1cc1974a9f0166396e811d0a9971) [CI] Fix AMD CI by inlining dummy_grok config (#18044) (sunxxuns)
  - Stats: +27/-1 (total 28)
  - Files:
    - modified: python/sglang/multimodal_gen/__init__.py (+2/-0)
      ```diff
      @@ -4,3 +4,5 @@
       from sglang.multimodal_gen.runtime.entrypoints.diffusion_generator import DiffGenerator
       
       __all__ = ["DiffGenerator", "PipelineConfig", "SamplingParams"]
      +
      +# Trigger multimodal CI tests
      ```
    - modified: scripts/ci/amd/amd_ci_install_dependency.sh (+25/-1)
      ```diff
      @@ -124,7 +124,31 @@ docker cp human-eval ci_sglang:/
       install_with_retry docker exec -w /human-eval ci_sglang pip install --cache-dir=/sgl-data/pip-cache -e .
       
       docker exec -w / ci_sglang mkdir -p /dummy-grok
      -mkdir -p dummy-grok && wget https://sharkpublic.blob.core.windows.net/sharkpublic/sglang/dummy_grok.json -O dummy-grok/config.json
      +# Create dummy grok config inline (bypasses Azure blob storage which may have auth issues)
      +mkdir -p dummy-grok
      +cat > dummy-grok/config.json << 'EOF'
      +{
      +  "architectures": [
      +    "Grok1ModelForCausalLM"
      +  ],
      +  "embedding_multiplier_scale": 78.38367176906169,
      +  "output_multiplier_scale": 0.5773502691896257,
      +  "vocab_size": 131072,
      +  "hidden_size": 6144,
      +  "intermediate_size": 32768,
      +  "max_position_embeddings": 8192,
      +  "num_experts_per_tok": 2,
      +  "num_local_experts": 8,
      +  "num_attention_heads": 48,
      +  "num_hidden_layers": 64,
      +  "num_key_value_heads": 8,
      +  "head_dim": 128,
      +  "rms_norm_eps": 1e-05,
      +  "rope_theta": 10000.0,
      +  "model_type": "mixtral",
      +  "torch_dtype": "bfloat16"
      +}
      +EOF
       docker cp ./dummy-grok ci_sglang:/
       
       docker exec ci_sglang pip install --cache-dir=/sgl-data/pip-cache huggingface_hub[hf_xet]
      ```
- [`855dd05`](https://github.com/sgl-project/sglang/commit/855dd0546ce7ba460f23bed08c469e63955228e2) feat: Add Ling Flash v2.0 support for Eagle3 (#15119) (yefei12)
  - Stats: +30/-1 (total 31)
  - Files:
    - modified: python/sglang/srt/models/bailing_moe.py (+30/-1)
      ```diff
      @@ -742,6 +742,8 @@ def __init__(
               else:
                   self.norm = PPMissingLayer(return_tuple=True)
       
      +        self.layers_to_capture = []
      +
           def forward(
               self,
               input_ids: torch.Tensor,
      @@ -764,6 +766,10 @@ def forward(
               aux_hidden_states = []
               for i in range(self.start_layer, self.end_layer):
                   with get_global_expert_distribution_recorder().with_current_layer(i):
      +                if i in self.layers_to_capture:
      +                    aux_hidden_states.append(
      +                        hidden_states if residual is None else hidden_states + residual
      +                    )
                       layer = self.layers[i]
                       hidden_states, residual = layer(
                           positions,
      @@ -789,7 +795,10 @@ def forward(
                           hidden_states = self.norm(hidden_states)
                       else:
                           hidden_states, _ = self.norm(hidden_states, residual)
      +
      +        if len(aux_hidden_states) == 0:
                   return hidden_states
      +        return hidden_states, aux_hidden_states
       
       
       class BailingMoEForCausalLM(nn.Module):
      @@ -826,6 +835,8 @@ def __init__(
                   )
               self.logits_processo...
      ```
- [`3ca29df`](https://github.com/sgl-project/sglang/commit/3ca29dffc72ac8a1000d3dad988fd96ab3339b9d) support qwen3-next eagle3 (#14607) (lukec)
  - Stats: +73/-6 (total 79)
  - Files:
    - modified: python/sglang/srt/models/qwen3_next.py (+73/-6)
      ```diff
      @@ -547,12 +547,18 @@ def forward(
               self,
               hidden_states: torch.Tensor,
               residual: Optional[torch.Tensor],
      +        captured_last_layer_outputs: Optional[list[torch.Tensor]] = None,
               **kwargs,
           ):
               forward_batch = kwargs.get("forward_batch", None)
       
      -        hidden_states, residual = self.layer_communicator.prepare_attn(
      -            hidden_states, residual, forward_batch
      +        hidden_states, residual = (
      +            self.layer_communicator.prepare_attn_and_capture_last_layer_outputs(
      +                hidden_states,
      +                residual,
      +                forward_batch,
      +                captured_last_layer_outputs=captured_last_layer_outputs,
      +            )
               )
       
               if not forward_batch.forward_mode.is_idle():
      @@ -769,10 +775,16 @@ def forward(
               hidden_states: torch.Tensor,
               residual: Optional[torch.Tensor],
               forward_batch: ForwardBatch,
      +        captured_last_layer_outputs: Optional[list[torch.Tensor]] = None,
               **kwargs: Any,
           ):
      -        hidden_states, residual = self.layer_communicator.prepare_attn(
      -            hidden_states, residual, forward_batch
      +        hidden_states, re...
      ```
- [`9bb1260`](https://github.com/sgl-project/sglang/commit/9bb1260558315c03a6b772e82a6bb146efd7ca6f) [Feature] Support file:// URL format for multimodal inputs (#14490) (Praneth Paruchuri)
  - Stats: +15/-2 (total 17)
  - Files:
    - modified: python/sglang/srt/utils/common.py (+15/-2)
      ```diff
      @@ -70,7 +70,7 @@
           Union,
       )
       from unittest import SkipTest
      -from urllib.parse import urlparse
      +from urllib.parse import unquote, urlparse
       
       import numpy as np
       import orjson
      @@ -860,6 +860,9 @@ def load_audio(
               audio_file = BytesIO(response.content)
               response.close()
               audio, original_sr = sf.read(audio_file)
      +    elif audio_file.startswith("file://"):
      +        audio_file = unquote(urlparse(audio_file).path)
      +        audio, original_sr = sf.read(audio_file)
           elif isinstance(audio_file, str):
               audio, original_sr = sf.read(audio_file)
           else:
      @@ -905,6 +908,9 @@ def load_image(
                   image.load()  # Force loading to avoid issues after closing the stream
               finally:
                   response.close()
      +    elif image_file.startswith("file://"):
      +        image_file = unquote(urlparse(image_file).path)
      +        image = Image.open(image_file)
           elif image_file.lower().endswith(("png", "jpg", "jpeg", "webp", "gif")):
               image = Image.open(image_file)
           elif image_file.startswith("data:"):
      @@ -925,6 +931,10 @@ def get_image_bytes(image_file: Union[str, bytes]):
               timeout = int(os.getenv("REQUEST_TIMEOUT", "3"))
            ...
      ```
- [`71babde`](https://github.com/sgl-project/sglang/commit/71babdef510ef3d95ea829ec61e5f7c3f9ae7232) Fix CUDA 12 dependency when importing Mooncake in official CUDA 13.x image (#17540) (ZhenshengWu)
  - Stats: +22/-1 (total 23)
  - Files:
    - modified: docker/Dockerfile (+22/-1)
      ```diff
      @@ -19,6 +19,11 @@ ARG PIP_DEFAULT_INDEX
       ARG UBUNTU_MIRROR
       ARG GITHUB_ARTIFACTORY=github.com
       ARG INSTALL_FLASHINFER_JIT_CACHE=0
      +ARG FLASHINFER_VERSION=0.6.1
      +ARG MOONCAKE_VERSION=0.3.8.post1
      +#if need other arg please add in MOONCAKE_COMPILE_ARG
      +ARG MOONCAKE_COMPILE_ARG="-DUSE_HTTP=ON -DUSE_MNNVL=ON -DUSE_CUDA=ON -DWITH_EP=ON"
      +
       ARG FLASHINFER_VERSION=0.6.2
       
       ENV DEBIAN_FRONTEND=noninteractive \
      @@ -269,11 +274,27 @@ RUN --mount=type=cache,target=/root/.cache/pip \
           fi && \
           TORCH_CUDA_ARCH_LIST="${CHOSEN_TORCH_CUDA_ARCH_LIST}" MAX_JOBS=${BUILD_AND_DOWNLOAD_PARALLEL} pip install --no-build-isolation .
       
      +# Install Mooncake
      +RUN --mount=type=cache,target=/root/.cache/pip \
      +    CUDA_MAJOR="${CUDA_VERSION%%.*}" && \
      +    if [ "$CUDA_MAJOR" -ge 13 ]; then \
      +        echo "CUDA >= 13, installing mooncake-transfer-engine from source code"; \
      +        git clone --branch v${MOONCAKE_VERSION} --depth 1 https://github.com/kvcache-ai/Mooncake.git && \
      +        cd Mooncake && \
      +        bash dependencies.sh && \
      +        mkdir -p build && \
      +        cd build && \
      +        cmake .. ${MOONCAKE_COMPILE_ARG} && \
      +        make -j$(nproc) && \
      +        make install; \
      +    else \...
      ```
- [`486c7de`](https://github.com/sgl-project/sglang/commit/486c7de39f5c6ec8d651c5c0c662e940fccb63e0) Optimizing all_reduce in RMSNormTP in minimax_m2 (#16483) (Roger Young)
  - Stats: +8/-2 (total 10)
  - Files:
    - modified: python/sglang/srt/models/minimax_m2.py (+8/-2)
      ```diff
      @@ -166,7 +166,14 @@ def rms_sumsq_serial(x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:
           stride_x1 = x1.stride(0)
           stride_x2 = x2.stride(0)
       
      -    sum_sq = torch.empty(B + B2, device=x1.device, dtype=torch.float32)
      +    # We found that custom all-reduce `sglang::cross_device_reduce_1stage`
      +    # is much faster than the nccl all-reduce in torch.
      +    # However, `should_custom_ar` checks if the reduced buffer is 16-byte aligned.
      +    # RMSNormTP reduces a [B, 2] fp32 tensor, so we pad the total element count to
      +    # satisfy the alignment requirement.
      +    B_padded = (B + B2 + 3) // 4 * 4
      +
      +    sum_sq = torch.empty(B_padded, device=x1.device, dtype=torch.float32)
       
           BLOCK_SIZE1 = triton.next_power_of_2(D1)
           BLOCK_SIZE2 = triton.next_power_of_2(D2)
      @@ -285,7 +292,6 @@ def forward(
               return x
       
           @staticmethod
      -    @torch.compile(dynamic=True, backend=get_compiler_backend())
           def forward_qk(
               q_norm: "MiniMaxM2RMSNormTP",
               k_norm: "MiniMaxM2RMSNormTP",
      ```
- [`9c168fc`](https://github.com/sgl-project/sglang/commit/9c168fcac7f6200d9584ea9a60ee45ce3ac0cc14) Fix Diffusion Request Validation to allow missing input artifacts if the input only contains text (#16610) (Kangyan-Zhou)
  - Stats: +33/-19 (total 52)
  - Files:
    - modified: python/sglang/multimodal_gen/runtime/entrypoints/openai/protocol.py (+1/-0)
      ```diff
      @@ -71,6 +71,7 @@ class VideoResponse(BaseModel):
       class VideoGenerationsRequest(BaseModel):
           prompt: str
           input_reference: Optional[str] = None
      +    reference_url: Optional[str] = None
           model: Optional[str] = None
           seconds: Optional[int] = 4
           size: Optional[str] = ""
      ```
    - modified: python/sglang/multimodal_gen/runtime/entrypoints/openai/video_api.py (+32/-19)
      ```diff
      @@ -178,27 +178,33 @@ async def create_video(
           content_type = request.headers.get("content-type", "").lower()
           request_id = generate_request_id()
       
      +    server_args = get_global_server_args()
      +    task_type = server_args.pipeline_config.task_type
      +
           if "multipart/form-data" in content_type:
               if not prompt:
                   raise HTTPException(status_code=400, detail="prompt is required")
      -        if input_reference is None and reference_url is None:
      -            raise HTTPException(
      -                status_code=400,
      -                detail="input_reference file or reference_url is required",
      -            )
      +        # Validate image input based on model task type
               image_list = merge_image_input_list(input_reference, reference_url)
      -        # Save first input image
      -        image = image_list[0]
      -        uploads_dir = os.path.join("outputs", "uploads")
      -        os.makedirs(uploads_dir, exist_ok=True)
      -        filename = image.filename if hasattr(image, "filename") else f"url_image"
      -        input_path = os.path.join(uploads_dir, f"{request_id}_{filename}")
      -        try:
      -            input_path = await save_image_to_path(image, input_path)
      -        except Ex...
      ```
- [`4d28cda`](https://github.com/sgl-project/sglang/commit/4d28cda007b0ff34e9f936c4ad1b8ec08ed0574b) [model] Support MiniCPM-V 4.5 (#9610) (tc-mb)
  - Stats: +389/-5 (total 394)
  - Files:
    - modified: python/sglang/srt/models/minicpmv.py (+389/-5)
      ```diff
      @@ -21,7 +21,9 @@
       # limitations under the License.
       """Inference-only MiniCPM-V model compatible with HuggingFace weights."""
       
      +import types
       from functools import partial
      +from itertools import chain
       from typing import (
           Any,
           Callable,
      @@ -56,6 +58,7 @@
       from sglang.srt.models.idefics2 import Idefics2VisionTransformer
       from sglang.srt.models.llama import LlamaConfig, LlamaForCausalLM
       from sglang.srt.models.qwen2 import Qwen2Config, Qwen2ForCausalLM
      +from sglang.srt.models.qwen3 import Qwen3Config, Qwen3ForCausalLM
       from sglang.srt.utils import add_prefix, flatten_nested_list
       
       RawImageType = Union[Image.Image, torch.Tensor]
      @@ -356,6 +359,218 @@ def forward(self, x: torch.Tensor, tgt_sizes: torch.Tensor) -> torch.Tensor:
               return x
       
       
      +class Resampler4_5(BaseResampler):
      +
      +    def __init__(
      +        self,
      +        num_queries: int,
      +        embed_dim: int,
      +        num_heads: int,
      +        kv_dim: Optional[int] = None,
      +        norm_layer: Callable[[int], nn.LayerNorm] = DEFAULT_LN,
      +        max_size: tuple[int, int] = (70, 70),
      +        max_temporal_size=36000,
      +        quant_config: Optional[QuantizationConfig] = None,
      +        prefix: str = "",
      +    ) ...
      ```
- [`2c036f1`](https://github.com/sgl-project/sglang/commit/2c036f1eb120b3e175380ef15feef5a49aaea19b) [Bugfix] fix the display error (inconsistent context) (#17699) (linhaifeng)
  - Stats: +2/-2 (total 4)
  - Files:
    - modified: python/sglang/srt/multimodal/processors/step3_vl.py (+1/-1)
      ```diff
      @@ -102,7 +102,7 @@ def slide_window(
               steps: list[tuple[int, int]],
               img_rate_thr: float = 0.6,
           ) -> tuple[list[tuple[int, int, int, int]], tuple[int, int]]:
      -        assert 1 >= img_rate_thr >= 0, "The `in_rate_thr` should lie in 0~1"
      +        assert 1 >= img_rate_thr >= 0, "The `img_rate_thr` should lie in 0~1"
               windows = []
               # Sliding windows.
               for size, step in zip(sizes, steps):
      ```
    - modified: python/sglang/srt/multiplex/pdmux_context.py (+1/-1)
      ```diff
      @@ -33,7 +33,7 @@ def load_pdmux_config(config_path: str) -> PDMuxConfig:
               raise ValueError("Missing required field: sm_group_num")
       
           if raw["sm_group_num"] < 3:
      -        raise ValueError("sm_group_num must greater than 3")
      +        raise ValueError("sm_group_num must be >= 3")
       
           manual_divisions = raw.get("manual_divisions", [])
      ```
- [`e5ac622`](https://github.com/sgl-project/sglang/commit/e5ac6229e18662dff4bef85582d8c71dcd885fff) Fix installation script for H200 runners (#18050) (Kangyan-Zhou)
  - Stats: +60/-9 (total 69)
  - Files:
    - modified: scripts/ci/cuda/ci_install_deepep.sh (+36/-6)
      ```diff
      @@ -21,17 +21,40 @@ if python3 -c "import deep_ep" >/dev/null 2>&1; then
       fi
       
       # Install system dependencies
      -apt install -y curl wget git sudo rdma-core infiniband-diags openssh-server perftest libibumad3 libibverbs-dev libibverbs1 ibverbs-providers ibverbs-utils libnl-3-200 libnl-route-3-200 librdmacm1 build-essential cmake
      +# Use fallback logic in case apt fails due to unrelated broken packages on the runner
      +DEEPEP_SYSTEM_DEPS="curl wget git sudo rdma-core infiniband-diags openssh-server perftest libibumad3 libibverbs-dev libibverbs1 ibverbs-providers ibverbs-utils libnl-3-200 libnl-route-3-200 librdmacm1 build-essential cmake"
      +apt-get install -y --no-install-recommends $DEEPEP_SYSTEM_DEPS || {
      +    echo "Warning: apt-get install failed, checking if required packages are available..."
      +    for pkg in $DEEPEP_SYSTEM_DEPS; do
      +        if ! dpkg -l "$pkg" 2>/dev/null | grep -q "^ii"; then
      +            echo "ERROR: Required package $pkg is not installed and apt-get failed"
      +            exit 1
      +        fi
      +    done
      +    echo "All required packages are already installed, continuing..."
      +}
       
       # Install GDRCopy
       rm -rf /opt/gdrcopy && mkdir -p /opt/gdrcopy
       cd /opt/gdrcopy
       git cl...
      ```
    - modified: scripts/ci/cuda/ci_install_dependency.sh (+24/-3)
      ```diff
      @@ -27,7 +27,19 @@ echo "CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-}"
       python3 -c 'import os, shutil, tempfile, getpass; cache_dir = os.environ.get("TORCHINDUCTOR_CACHE_DIR") or os.path.join(tempfile.gettempdir(), "torchinductor_" + getpass.getuser()); shutil.rmtree(cache_dir, ignore_errors=True)'
       
       # Install apt packages
      -apt install -y git libnuma-dev libssl-dev pkg-config libibverbs-dev libibverbs1 ibverbs-providers ibverbs-utils
      +# Use --no-install-recommends and ignore errors from unrelated broken packages on the runner
      +# The NVIDIA driver packages may have broken dependencies that are unrelated to these packages
      +apt-get install -y --no-install-recommends git libnuma-dev libssl-dev pkg-config libibverbs-dev libibverbs1 ibverbs-providers ibverbs-utils || {
      +    echo "Warning: apt-get install failed, checking if required packages are available..."
      +    # Verify the packages we need are actually installed
      +    for pkg in git libnuma-dev libssl-dev pkg-config libibverbs-dev libibverbs1 ibverbs-providers ibverbs-utils; do
      +        if ! dpkg -l "$pkg" 2>/dev/null | grep -q "^ii"; then
      +            echo "ERROR: Required package $pkg is not installed and apt-get failed"
      +    ...
      ```
- [`a0bae4c`](https://github.com/sgl-project/sglang/commit/a0bae4c34349d09b3ad71db71d4fc3f4f32f221e) Migrate 4-GPU/8-GPU workflow jobs to stage-c and add CI registry decorators (#17299) (Alison Shao)
  - Stats: +189/-284 (total 473)
  - Files (truncated):
    - modified: .github/workflows/pr-test.yml (+35/-63)
      ```diff
      @@ -1298,12 +1298,12 @@ jobs:
                   --total-partitions 2 \
                   $CONTINUE_ON_ERROR_FLAG
       
      -  unit-test-backend-4-gpu:
      +  stage-c-test-4-gpu-h100:
           needs: [check-changes, call-gate, wait-for-stage-b]
           if: |
             always() &&
             (
      -        (inputs.target_stage == 'unit-test-backend-4-gpu') ||
      +        (inputs.target_stage == 'stage-c-test-4-gpu-h100') ||
               (
                 !inputs.target_stage &&
                 ((github.event_name == 'schedule' || inputs.test_parallel_dispatch == true) || (!failure() && !cancelled())) &&
      @@ -1340,23 +1340,19 @@ jobs:
             - name: Run test
               timeout-minutes: 20
               run: |
      -          cd test/srt
      -          RETRY_FLAG=""
      -          if [[ "${{ needs.check-changes.outputs.enable_retry }}" == "true" ]]; then
      -            RETRY_FLAG="--enable-retry"
      -          fi
      +          cd test
                 CONTINUE_ON_ERROR_FLAG=""
                 if [[ "${{ needs.check-changes.outputs.continue_on_error }}" == "true" ]]; then
                   CONTINUE_ON_ERROR_FLAG="--continue-on-error"
                 fi
      -          python3 run_suite.py --suite per-commit-4-gpu --auto-partition-id ${{ matrix.part }} --auto-partition-size 3 $RETRY_FLAG $CONTI...
      ```
    - modified: scripts/ci/utils/slash_command_handler.py (+7/-0)
      ```diff
      @@ -258,6 +258,13 @@ def handle_rerun_stage(
               "stage-b-test-large-2-gpu",
               "stage-c-test-large-4-gpu",
               "stage-c-test-large-4-gpu-b200",
      +        "stage-c-test-4-gpu-h100",
      +        "stage-c-test-8-gpu-h200",
      +        "stage-c-test-8-gpu-h20",
      +        "stage-c-test-4-gpu-b200",
      +        "stage-c-test-4-gpu-gb200",
      +        "stage-c-test-deepep-4-gpu",
      +        "stage-c-test-deepep-8-gpu-h200",
               "multimodal-gen-test-1-gpu",
               "multimodal-gen-test-2-gpu",
               "quantization-test",
      ```
    - renamed: test/registered/4-gpu-models/test_deepseek_v3_cutedsl_4gpu.py (+3/-0)
      ```diff
      @@ -3,6 +3,7 @@
       from types import SimpleNamespace
       
       from sglang.srt.utils import kill_process_tree
      +from sglang.test.ci.ci_register import register_cuda_ci
       from sglang.test.few_shot_gsm8k import run_eval as run_eval_few_shot_gsm8k
       from sglang.test.test_utils import (
           DEFAULT_DEEPSEEK_NVFP4_MODEL_FOR_TEST,
      @@ -13,6 +14,8 @@
           try_cached_model,
       )
       
      +register_cuda_ci(est_time=1800, suite="stage-c-test-4-gpu-gb200")
      +
       
       class TestDeepseekR1Nvfp4CuteDSLDeepEP(CustomTestCase):
           @classmethod
      ```
    - renamed: test/registered/4-gpu-models/test_gpt_oss_4gpu.py (+4/-0)
      ```diff
      @@ -1,7 +1,11 @@
       import unittest
       
      +from sglang.test.ci.ci_register import register_cuda_ci
       from sglang.test.gpt_oss_common import BaseTestGptOss
       
      +register_cuda_ci(est_time=300, suite="stage-c-test-4-gpu-h100")
      +register_cuda_ci(est_time=300, suite="stage-c-test-4-gpu-b200")
      +
       
       class TestGptOss4Gpu(BaseTestGptOss):
           def test_bf16_120b(self):
      ```
    - renamed: test/registered/4-gpu-models/test_qwen3_next_models.py (+3/-0)
      ```diff
      @@ -4,6 +4,7 @@
       import requests
       
       from sglang.srt.utils import kill_process_tree
      +from sglang.test.ci.ci_register import register_cuda_ci
       from sglang.test.few_shot_gsm8k import run_eval
       from sglang.test.kl_test_utils import (
           test_input_output_logprobs_match_decode_cache_hit_helper,
      @@ -16,6 +17,8 @@
           popen_launch_server,
       )
       
      +register_cuda_ci(est_time=350, suite="stage-c-test-4-gpu-h100")
      +
       QWEN3_NEXT_MODEL = "Qwen/Qwen3-Next-80B-A3B-Instruct"
       
       ACC_THRESHOLDS = {
      ```
    - renamed: test/registered/4-gpu-models/test_qwen3_next_models_mtp.py (+3/-0)
      ```diff
      @@ -4,6 +4,7 @@
       import requests
       
       from sglang.srt.utils import kill_process_tree
      +from sglang.test.ci.ci_register import register_cuda_ci
       from sglang.test.few_shot_gsm8k import run_eval
       from sglang.test.kl_test_utils import (
           test_input_output_logprobs_match_decode_cache_hit_helper,
      @@ -16,6 +17,8 @@
           popen_launch_server,
       )
       
      +register_cuda_ci(est_time=500, suite="stage-c-test-4-gpu-h100")
      +
       QWEN3_NEXT_MODEL = "Qwen/Qwen3-Next-80B-A3B-Instruct"
       
       ACC_THRESHOLDS = {
      ```
    - renamed: test/registered/8-gpu-models/test_deepseek_v32_basic.py (+3/-0)
      ```diff
      @@ -2,6 +2,7 @@
       from types import SimpleNamespace
       
       from sglang.srt.utils import kill_process_tree
      +from sglang.test.ci.ci_register import register_cuda_ci
       from sglang.test.few_shot_gsm8k import run_eval as run_eval_few_shot_gsm8k
       from sglang.test.send_one import BenchArgs, send_one_prompt
       from sglang.test.test_utils import (
      @@ -13,6 +14,8 @@
           write_github_step_summary,
       )
       
      +register_cuda_ci(est_time=360, suite="stage-c-test-8-gpu-h200")
      +
       DEEPSEEK_V32_MODEL_PATH = "deepseek-ai/DeepSeek-V3.2-Exp"
      ```
    - renamed: test/registered/8-gpu-models/test_deepseek_v32_mtp.py (+3/-0)
      ```diff
      @@ -4,6 +4,7 @@
       import requests
       
       from sglang.srt.utils import kill_process_tree
      +from sglang.test.ci.ci_register import register_cuda_ci
       from sglang.test.few_shot_gsm8k import run_eval as run_eval_few_shot_gsm8k
       from sglang.test.send_one import BenchArgs, send_one_prompt
       from sglang.test.test_utils import (
      @@ -15,6 +16,8 @@
           write_github_step_summary,
       )
       
      +register_cuda_ci(est_time=360, suite="stage-c-test-8-gpu-h200")
      +
       FULL_DEEPSEEK_V32_MODEL_PATH = "deepseek-ai/DeepSeek-V3.2-Exp"
      ```
    - renamed: test/registered/8-gpu-models/test_deepseek_v3_basic.py (+3/-0)
      ```diff
      @@ -2,6 +2,7 @@
       from types import SimpleNamespace
       
       from sglang.srt.utils import kill_process_tree
      +from sglang.test.ci.ci_register import register_cuda_ci
       from sglang.test.few_shot_gsm8k import run_eval as run_eval_few_shot_gsm8k
       from sglang.test.send_one import BenchArgs, send_one_prompt
       from sglang.test.test_utils import (
      @@ -14,6 +15,8 @@
           write_github_step_summary,
       )
       
      +register_cuda_ci(est_time=275, suite="stage-c-test-8-gpu-h200")
      +
       FULL_DEEPSEEK_V3_MODEL_PATH = "deepseek-ai/DeepSeek-V3-0324"
      ```
    - renamed: test/registered/8-gpu-models/test_deepseek_v3_mtp.py (+3/-0)
      ```diff
      @@ -4,6 +4,7 @@
       import requests
       
       from sglang.srt.utils import kill_process_tree
      +from sglang.test.ci.ci_register import register_cuda_ci
       from sglang.test.few_shot_gsm8k import run_eval as run_eval_few_shot_gsm8k
       from sglang.test.send_one import BenchArgs, send_one_prompt
       from sglang.test.test_utils import (
      @@ -16,6 +17,8 @@
           write_github_step_summary,
       )
       
      +register_cuda_ci(est_time=275, suite="stage-c-test-8-gpu-h200")
      +
       FULL_DEEPSEEK_V3_MODEL_PATH = "deepseek-ai/DeepSeek-V3-0324"
      ```
    - renamed: test/registered/8-gpu-models/test_mimo_models.py (+3/-0)
      ```diff
      @@ -1,9 +1,12 @@
       import unittest
       
      +from sglang.test.ci.ci_register import register_cuda_ci
       from sglang.test.kits.gsm8k_accuracy_kit import GSM8KMixin
       from sglang.test.kits.spec_decoding_kit import SpecDecodingMixin
       from sglang.test.server_fixtures.default_fixture import DefaultServerBase
       
      +register_cuda_ci(est_time=200, suite="stage-c-test-8-gpu-h200")
      +
       
       class TestMiMoV2Flash(GSM8KMixin, SpecDecodingMixin, DefaultServerBase):
           gsm8k_accuracy_thres = 0.75
      ```
    - renamed: test/registered/amd/test_moriep_small.py (+3/-0)
      ```diff
      @@ -3,6 +3,7 @@
       from types import SimpleNamespace
       
       from sglang.srt.utils import kill_process_tree
      +from sglang.test.ci.ci_register import register_amd_ci
       from sglang.test.few_shot_gsm8k import run_eval as run_eval_few_shot_gsm8k
       from sglang.test.test_utils import (
           DEFAULT_DEEPEP_MODEL_NAME_FOR_TEST,
      @@ -13,6 +14,8 @@
           popen_launch_server,
       )
       
      +register_amd_ci(est_time=1200, suite="stage-c-test-large-8-gpu-amd")
      +
       
       class TestPureDP(CustomTestCase):
      ```
    - renamed: test/registered/distributed/test_disaggregation_aarch64.py (+3/-0)
      ```diff
      @@ -2,6 +2,7 @@
       import unittest
       from types import SimpleNamespace
       
      +from sglang.test.ci.ci_register import register_cuda_ci
       from sglang.test.few_shot_gsm8k import run_eval as run_eval_few_shot_gsm8k
       from sglang.test.server_fixtures.disaggregation_fixture import (
           PDDisaggregationServerBase,
      @@ -12,6 +13,8 @@
           popen_launch_pd_server,
       )
       
      +register_cuda_ci(est_time=300, suite="stage-c-test-4-gpu-gb200")
      +
       
       class TestDisaggregationMooncakeAARCH64Accuracy(PDDisaggregationServerBase):
           @classmethod
      ```
    - renamed: test/registered/distributed/test_disaggregation_different_tp.py (+3/-0)
      ```diff
      @@ -2,6 +2,7 @@
       from types import SimpleNamespace
       
       from sglang.srt.environ import envs
      +from sglang.test.ci.ci_register import register_cuda_ci
       from sglang.test.few_shot_gsm8k import run_eval as run_eval_few_shot_gsm8k
       from sglang.test.server_fixtures.disaggregation_fixture import (
           PDDisaggregationServerBase,
      @@ -14,6 +15,8 @@
           try_cached_model,
       )
       
      +register_cuda_ci(est_time=600, suite="stage-c-test-8-gpu-h20")
      +
       
       class TestDisaggregationMooncakePrefillLargerTP(PDDisaggregationServerBase):
           @classmethod
      ```
    - renamed: test/registered/distributed/test_disaggregation_dp_attention.py (+3/-0)
      ```diff
      @@ -2,6 +2,7 @@
       from types import SimpleNamespace
       
       from sglang.srt.environ import envs
      +from sglang.test.ci.ci_register import register_cuda_ci
       from sglang.test.few_shot_gsm8k import run_eval as run_eval_few_shot_gsm8k
       from sglang.test.server_fixtures.disaggregation_fixture import (
           PDDisaggregationServerBase,
      @@ -13,6 +14,8 @@
           try_cached_model,
       )
       
      +register_cuda_ci(est_time=155, suite="stage-c-test-8-gpu-h20")
      +
       
       class TestDisaggregationDPAttention(PDDisaggregationServerBase):
           PREFILL_DP_SIZE = 4
      ```
    - renamed: test/registered/distributed/test_disaggregation_hybrid_attention.py (+5/-0)
      ```diff
      @@ -1,6 +1,7 @@
       import unittest
       from types import SimpleNamespace
       
      +from sglang.test.ci.ci_register import register_cuda_ci
       from sglang.test.few_shot_gsm8k import run_eval as run_eval_few_shot_gsm8k
       from sglang.test.server_fixtures.disaggregation_fixture import (
           PDDisaggregationServerBase,
      @@ -11,6 +12,10 @@
           popen_launch_pd_server,
       )
       
      +register_cuda_ci(
      +    est_time=400, suite="stage-c-test-8-gpu-h200", disabled="TCP fallback flaky"
      +)
      +
       
       @unittest.skipIf(is_in_ci(), "Temporarily disable the flaky test.")
       class TestDisaggregationHybridAttentionMamba(PDDisaggregationServerBase):
      ```
- [`9518048`](https://github.com/sgl-project/sglang/commit/95180484e9a63ff4ceffc3454126dfb16f618b89) Disable test_mla_int8_deepseek_v3.py temporarily (#18057) (Alison Shao)
  - Stats: +5/-1 (total 6)
  - Files:
    - modified: test/registered/mla/test_mla_int8_deepseek_v3.py (+5/-1)
      ```diff
      @@ -16,7 +16,11 @@
       )
       
       # DeepSeek-V3 INT8 quantization tests (channel and block INT8)
      -register_cuda_ci(est_time=341, suite="stage-b-test-large-1-gpu")
      +register_cuda_ci(
      +    est_time=341,
      +    suite="stage-b-test-large-1-gpu",
      +    disabled="HF token doesn't have private repository access",
      +)
       
       
       class TestMLADeepseekV3ChannelInt8(CustomTestCase):
      ```
- [`d396650`](https://github.com/sgl-project/sglang/commit/d396650bd29bbf4376a926844194e91ebfcbe679) Fix swa kv cache memory allocation (#18039) (Ke Bao)
  - Stats: +87/-34 (total 121)
  - Files:
    - modified: python/sglang/srt/configs/model_config.py (+16/-6)
      ```diff
      @@ -392,6 +392,17 @@ def _derive_model_shapes(self):
                   self.head_dim,
               )
       
      +        self.swa_head_dim = getattr(
      +            self.hf_text_config,
      +            "swa_head_dim",
      +            self.head_dim,
      +        )
      +        self.swa_v_head_dim = getattr(
      +            self.hf_text_config,
      +            "swa_v_head_dim",
      +            self.v_head_dim,
      +        )
      +
               # FIXME: temporary special judge for MLA architecture
               if (
                   "DeepseekV2ForCausalLM" in self.hf_config.architectures
      @@ -592,12 +603,11 @@ def get_num_kv_heads(self, tensor_parallel_size) -> int:
       
           def get_swa_num_kv_heads(self, tensor_parallel_size) -> int:
               """Similar to get_num_kv_heads(), but for SWA."""
      -        if not self.is_hybrid_swa_compress:
      -            return 0
      -
      -        # For MiMoV2FlashForCausalLM models
      -        total_num_kv_heads = self.hf_text_config.swa_num_key_value_heads
      -        return max(1, total_num_kv_heads // tensor_parallel_size)
      +        if hasattr(self.hf_text_config, "swa_num_key_value_heads"):
      +            total_num_kv_heads = self.hf_text_config.swa_num_key_value_heads
      +            return max(1, total_num_kv_heads // tensor_paral...
      ```
    - modified: python/sglang/srt/model_executor/model_runner_kv_cache_mixin.py (+71/-28)
      ```diff
      @@ -79,12 +79,28 @@ def get_cell_size_per_token(self: ModelRunner, num_layers: int) -> int:
                       )
                       cell_size += indexer_size_per_token * num_layers * element_size
               else:
      -            cell_size = (
      -                self.model_config.get_num_kv_heads(get_attention_tp_size())
      -                * (self.model_config.head_dim + self.model_config.v_head_dim)
      -                * num_layers
      -                * kv_size
      -            )
      +            if self.model_config.is_hybrid_swa:
      +                full_layers_num = len(self.model_config.full_attention_layer_ids)
      +                swa_layers_num = len(self.model_config.swa_attention_layer_ids)
      +
      +                full_per_token = self.model_config.get_num_kv_heads(
      +                    get_attention_tp_size()
      +                ) * (self.model_config.head_dim + self.model_config.v_head_dim)
      +
      +                swa_per_token = self.model_config.get_swa_num_kv_heads(
      +                    get_attention_tp_size()
      +                ) * (self.model_config.swa_head_dim + self.model_config.swa_v_head_dim)
      +
      +                cell_size = (
      +                    full_per_token * full_layers_num + swa_per_token * swa_layers...
      ```
- [`38d275a`](https://github.com/sgl-project/sglang/commit/38d275a9fd7c42353839b88ce83359444fb3cb6c) [BugFix] fix gpt-oss accuracy issue when enabling piecewise cuda graph (#18013) (Minglei Zhu)
  - Stats: +9/-1 (total 10)
  - Files:
    - modified: python/sglang/srt/managers/schedule_batch.py (+9/-1)
      ```diff
      @@ -2283,6 +2283,15 @@ def copy(self):
           def maybe_evict_swa(self):
               if self.tree_cache.supports_swa():
                   sliding_window_size = self.tree_cache.sliding_window_size
      +            server_args = get_global_server_args()
      +
      +            if (
      +                self.forward_mode.is_decode()
      +                and server_args.enable_piecewise_cuda_graph
      +                and not self.tree_cache.is_chunk_cache()
      +            ):
      +                return
      +
                   for idx, req in enumerate(self.reqs):
                       if self.forward_mode.is_decode():
                           # We set evict_swa condition here with two reasons:
      @@ -2297,7 +2306,6 @@ def maybe_evict_swa(self):
                               if req.extend_batch_idx < 2:
                                   continue
                               else:
      -                            server_args = get_global_server_args()
                                   pre_len = (
                                       pre_len - server_args.chunked_prefill_size
                                       if server_args.chunked_prefill_size > 0
      ```
- [`1189259`](https://github.com/sgl-project/sglang/commit/11892599f164d1c2da9c489e5f0b922c2ec6a3c4) [metric] Optional extra metric labels (#18049) (Yinghai Lu)
  - Stats: +27/-4 (total 31)
  - Files:
    - modified: python/sglang/srt/managers/scheduler_metrics_mixin.py (+2/-0)
      ```diff
      @@ -109,6 +109,8 @@ def init_metrics(
                   }
                   if dp_rank is not None:
                       labels["dp_rank"] = dp_rank
      +            if self.server_args.extra_metric_labels:
      +                labels.update(self.server_args.extra_metric_labels)
                   self.metrics_collector = SchedulerMetricsCollector(
                       labels=labels,
                       enable_lora=self.enable_lora,
      ```
    - modified: python/sglang/srt/managers/tokenizer_manager.py (+2/-0)
      ```diff
      @@ -437,6 +437,8 @@ def init_metric_collector_watchdog(self):
                   if self.server_args.tokenizer_metrics_allowed_custom_labels:
                       for label in self.server_args.tokenizer_metrics_allowed_custom_labels:
                           labels[label] = ""
      +            if self.server_args.extra_metric_labels:
      +                labels.update(self.server_args.extra_metric_labels)
                   self.metrics_collector = TokenizerMetricsCollector(
                       server_args=self.server_args,
                       labels=labels,
      ```
    - modified: python/sglang/srt/mem_cache/base_prefix_cache.py (+7/-3)
      ```diff
      @@ -119,9 +119,13 @@ class BasePrefixCache(ABC, PrefixCacheTrait):
           )
       
           def init_metrics_collector(self):
      -        self.metrics_collector = RadixCacheMetricsCollector(
      -            labels={"cache_type": self.__class__.__name__}
      -        )
      +        from sglang.srt.server_args import get_global_server_args
      +
      +        server_args = get_global_server_args()
      +        labels = {"cache_type": self.__class__.__name__}
      +        if server_args.extra_metric_labels:
      +            labels.update(server_args.extra_metric_labels)
      +        self.metrics_collector = RadixCacheMetricsCollector(labels=labels)
       
           def update_eviction_metrics(self, num_evicted: int, start_time: float):
               if self.metrics_collector is not None and num_evicted > 0:
      ```
    - modified: python/sglang/srt/mem_cache/hiradix_cache.py (+8/-1)
      ```diff
      @@ -8,7 +8,7 @@
       import threading
       import time
       from queue import Empty
      -from typing import TYPE_CHECKING, List, Optional
      +from typing import TYPE_CHECKING, Dict, List, Optional
       
       import torch
       
      @@ -88,6 +88,7 @@ def __init__(self, params: CacheInitParams, server_args: ServerArgs):
               self.pp_size = params.pp_size
               self.enable_storage = server_args.hicache_storage_backend is not None
               self.enable_storage_metrics = self.enable_storage and params.enable_metrics
      +        self.extra_metric_labels = server_args.extra_metric_labels
       
               (
                   extra_config,
      @@ -126,6 +127,7 @@ def __init__(self, params: CacheInitParams, server_args: ServerArgs):
                   hicache_storage_pass_prefix_keys=hicache_storage_pass_prefix_keys,
                   enable_storage=self.enable_storage,
                   enable_storage_metrics=self.enable_storage_metrics,
      +            extra_metric_labels=self.extra_metric_labels,
               )
       
               # record the nodes with ongoing write through
      @@ -168,6 +170,7 @@ def _apply_storage_runtime_config(
               hicache_storage_pass_prefix_keys: bool,
               enable_storage: bool,
               enable_storage_metrics: bool,
      +        extra_me...
      ```
    - modified: python/sglang/srt/server_args.py (+8/-0)
      ```diff
      @@ -380,6 +380,7 @@ class ServerArgs:
           enable_metrics_for_all_schedulers: bool = False
           tokenizer_metrics_custom_labels_header: str = "x-custom-labels"
           tokenizer_metrics_allowed_custom_labels: Optional[List[str]] = None
      +    extra_metric_labels: Optional[Dict[str, str]] = None
           bucket_time_to_first_token: Optional[List[float]] = None
           bucket_inter_token_latency: Optional[List[float]] = None
           bucket_e2e_request_latency: Optional[List[float]] = None
      @@ -3369,6 +3370,13 @@ def add_cli_args(parser: argparse.ArgumentParser):
                   "'--tokenizer-metrics-custom-labels-header' field in HTTP requests, e.g., {'label1': 'value1', 'label2': "
                   "'value2'} is allowed if '--tokenizer-metrics-allowed-custom-labels label1 label2' is set.",
               )
      +        parser.add_argument(
      +            "--extra-metric-labels",
      +            type=json.loads,
      +            default=ServerArgs.extra_metric_labels,
      +            help="The custom labels for metrics. "
      +            'e.g. \'{"label1": "value1", "label2": "value2"}\'',
      +        )
               parser.add_argument(
                   "--bucket-time-to-first-token",
                   type=float,
      ```
### vllm-project/vllm
- [`6720238`](https://github.com/vllm-project/vllm/commit/672023877bfd17a8720ae05ed198456d9b454363) Change defaults for vllm bench startup (#33489) (Luka Govedi)
  - Stats: +3/-3 (total 6)
  - Files:
    - modified: vllm/benchmarks/startup.py (+3/-3)
      ```diff
      @@ -159,19 +159,19 @@ def add_cli_args(parser: argparse.ArgumentParser):
           parser.add_argument(
               "--num-iters-cold",
               type=int,
      -        default=5,
      +        default=3,
               help="Number of cold startup iterations.",
           )
           parser.add_argument(
               "--num-iters-warmup",
               type=int,
      -        default=3,
      +        default=1,
               help="Number of warmup iterations before benchmarking warm startups.",
           )
           parser.add_argument(
               "--num-iters-warm",
               type=int,
      -        default=5,
      +        default=3,
               help="Number of warm startup iterations.",
           )
           parser.add_argument(
      ```
- [`754a8ca`](https://github.com/vllm-project/vllm/commit/754a8ca9426080a652377b556d466cadb5c3468f) fix: only include Authorization header when OPENAI_API_KEY is set (#33488) (Zack Yu)
  - Stats: +17/-26 (total 43)
  - Files:
    - modified: vllm/benchmarks/lib/endpoint_request_func.py (+17/-26)
      ```diff
      @@ -138,6 +138,16 @@ def _update_headers_common(
               headers["x-request-id"] = request_func_input.request_id
       
       
      +def _get_headers(content_type: str | None = None) -> dict[str, str]:
      +    headers = {}
      +    if content_type:
      +        headers["Content-Type"] = content_type
      +    api_key = os.environ.get("OPENAI_API_KEY")
      +    if api_key:
      +        headers["Authorization"] = f"Bearer {api_key}"
      +    return headers
      +
      +
       async def async_request_openai_completions(
           request_func_input: RequestFuncInput,
           session: aiohttp.ClientSession,
      @@ -170,9 +180,7 @@ async def async_request_openai_completions(
           }
           _update_payload_common(payload, request_func_input)
       
      -    headers = {
      -        "Authorization": f"Bearer {os.environ.get('OPENAI_API_KEY')}",
      -    }
      +    headers = _get_headers()
           _update_headers_common(headers, request_func_input)
       
           output = RequestFuncOutput()
      @@ -301,10 +309,7 @@ async def async_request_openai_chat_completions(
           }
           _update_payload_common(payload, request_func_input)
       
      -    headers = {
      -        "Content-Type": "application/json",
      -        "Authorization": f"Bearer {os.environ.get('OPENAI_API_KEY')}",
      -    }
      +    headers = _get_heade...
      ```
- [`302ecf6`](https://github.com/vllm-project/vllm/commit/302ecf64ff22e6e2f59d108e95f0c66d96c9ffc0) [Models]: lfm2_siglip2 return intermediate encoder layers (#33370) (Eduardo Salinas)
  - Stats: +76/-7 (total 83)
  - Files:
    - modified: vllm/model_executor/models/lfm2_siglip2.py (+76/-7)
      ```diff
      @@ -22,7 +22,11 @@
       from vllm.model_executor.layers.quantization import QuantizationConfig
       from vllm.model_executor.model_loader.weight_utils import default_weight_loader
       
      -from .vision import is_vit_use_data_parallel, should_torch_compile_mm_vit
      +from .vision import (
      +    is_vit_use_data_parallel,
      +    resolve_visual_encoder_outputs,
      +    should_torch_compile_mm_vit,
      +)
       
       
       class Siglip2VisionEmbeddings(nn.Module):
      @@ -331,18 +335,25 @@ def __init__(
               self,
               config: Siglip2VisionConfig,
               quant_config: QuantizationConfig | None = None,
      +        num_hidden_layers_override: int | None = None,
               prefix: str = "",
           ):
               super().__init__()
               self.config = config
      +
      +        if num_hidden_layers_override is None:
      +            num_hidden_layers = config.num_hidden_layers
      +        else:
      +            num_hidden_layers = num_hidden_layers_override
      +
               self.layers = nn.ModuleList(
                   [
                       Siglip2EncoderLayer(
                           config=config,
                           quant_config=quant_config,
                           prefix=f"{prefix}.layers.{idx}",
                       )
      -                for idx in range(config.num_...
      ```
- [`b6bb284`](https://github.com/vllm-project/vllm/commit/b6bb2842cf30251951ccf6d22f808a9a2884d96e) [Critical] Revert #33110 (#33500) (Cyrus Leung)
  - Stats: +0/-31 (total 31)
  - Files:
    - modified: vllm/v1/engine/input_processor.py (+0/-31)
      ```diff
      @@ -35,7 +35,6 @@
       from vllm.tokenizers.mistral import MistralTokenizer
       from vllm.utils import length_from_prompt_token_ids_or_embeds, random_uuid
       from vllm.utils.torch_utils import set_default_torch_num_threads
      -from vllm.v1.core.encoder_cache_manager import compute_mm_encoder_budget
       from vllm.v1.engine import EngineCoreRequest
       from vllm.v1.metrics.stats import MultiModalCacheStats
       from vllm.v1.structured_output.backend_guidance import (
      @@ -69,17 +68,6 @@ def __init__(
       
               self.mm_registry = mm_registry
               self.mm_processor_cache = mm_registry.processor_cache_from_config(vllm_config)
      -        self.mm_encoder_cache_size = None
      -        if (
      -            self.mm_registry.supports_multimodal_inputs(self.model_config)
      -            and not self.model_config.skip_tokenizer_init
      -        ):
      -            max_tokens_by_modality = mm_registry.get_max_tokens_per_item_by_modality(
      -                self.model_config
      -            )
      -            _, self.mm_encoder_cache_size = compute_mm_encoder_budget(
      -                self.vllm_config.scheduler_config, max_tokens_by_modality
      -            )
       
               self.input_preprocessor = InputPreprocessor(
                   self.model_con...
      ```
- [`79b6ec6`](https://github.com/vllm-project/vllm/commit/79b6ec6aab4b3d93e421a15ab41ba6d09faadd8f) [Bugfix] Fix inconsistent handling of cache reset (#33481) (Cyrus Leung)
  - Stats: +35/-11 (total 46)
  - Files:
    - modified: docs/benchmarking/sweeps.md (+1/-1)
      ```diff
      @@ -82,7 +82,7 @@ vllm bench sweep serve \
           You can use `--dry-run` to preview the commands to be run.
       
           We only start the server once for each `--serve-params`, and keep it running for multiple `--bench-params`.
      -    Between each benchmark run, we call the `/reset_prefix_cache` and `/reset_mm_cache` endpoints to get a clean slate for the next run.
      +    Between each benchmark run, we call all `/reset_*_cache` endpoints to get a clean slate for the next run.
           In case you are using a custom `--serve-cmd`, you can override the commands used for resetting the state by setting `--after-bench-cmd`.
       
       !!! note
      ```
    - modified: vllm/benchmarks/sweep/server.py (+9/-5)
      ```diff
      @@ -12,6 +12,12 @@
       
       
       class ServerProcess:
      +    VLLM_RESET_CACHE_ENDPOINTS = [
      +        "/reset_prefix_cache",
      +        "/reset_mm_cache",
      +        "/reset_encoder_cache",
      +    ]
      +
           def __init__(
               self,
               server_cmd: list[str],
      @@ -120,11 +126,9 @@ def reset_caches(self) -> None:
                   server_address = self._get_vllm_server_address()
                   print(f"Resetting caches at {server_address}")
       
      -            res = requests.post(f"{server_address}/reset_prefix_cache")
      -            res.raise_for_status()
      -
      -            res = requests.post(f"{server_address}/reset_mm_cache")
      -            res.raise_for_status()
      +            for endpoint in self.VLLM_RESET_CACHE_ENDPOINTS:
      +                res = requests.post(server_address + endpoint)
      +                res.raise_for_status()
               elif server_cmd[0].endswith("infinity_emb"):
                   if "--vector-disk-cache" in server_cmd:
                       raise NotImplementedError(
      ```
    - modified: vllm/entrypoints/openai/engine/serving.py (+0/-4)
      ```diff
      @@ -286,10 +286,6 @@ def _get_reasoning_parser(
                   raise TypeError(f"{reasoning_parser_name=} has not been registered") from e
               return parser
       
      -    async def reset_mm_cache(self) -> None:
      -        self.input_processor.clear_mm_cache()
      -        await self.engine_client.reset_mm_cache()
      -
           async def beam_search(
               self,
               prompt: PromptType,
      ```
    - modified: vllm/v1/engine/async_llm.py (+1/-0)
      ```diff
      @@ -741,6 +741,7 @@ async def pause_generation(
               if clear_cache:
                   await self.reset_prefix_cache()
                   await self.reset_mm_cache()
      +            await self.reset_encoder_cache()
       
           async def resume_generation(self) -> None:
               """Resume generation after :meth:`pause_generation`."""
      ```
    - modified: vllm/v1/worker/gpu/mm/encoder_runner.py (+16/-0)
      ```diff
      @@ -31,6 +31,22 @@ def __init__(
               self.req_id_to_mm_features: dict[str, list[MultiModalFeatureSpec]] = {}
               self.encoder_cache: dict[str, torch.Tensor] = {}
       
      +    def reset_mm_cache(self) -> None:
      +        """
      +        Clear the multi-modal cache that was used during profiling,
      +        but no longer needed during inference.
      +        """
      +        # TODO: Implement MM budget for encoder dummy run
      +        pass
      +
      +    def reset_encoder_cache(self) -> None:
      +        """Clear the GPU-side encoder cache storing vision embeddings.
      +
      +        This should be called when model weights are updated to ensure
      +        stale embeddings computed with old weights are not reused.
      +        """
      +        self.encoder_cache.clear()
      +
           def add_request(self, req_id: str, mm_features: list[MultiModalFeatureSpec]):
               self.req_id_to_mm_features[req_id] = mm_features
      ```
    - modified: vllm/v1/worker/gpu/model_runner.py (+4/-1)
      ```diff
      @@ -339,7 +339,10 @@ def profile_run(self) -> None:
               gc.collect()
       
           def reset_mm_cache(self) -> None:
      -        pass
      +        self.encoder_runner.reset_mm_cache()
      +
      +    def reset_encoder_cache(self) -> None:
      +        self.encoder_runner.reset_encoder_cache()
       
           def _get_num_input_tokens(self, num_scheduled_tokens: int) -> int:
               # SP is not supported yet.
      ```
    - modified: vllm/v1/worker/gpu_model_runner.py (+4/-0)
      ```diff
      @@ -717,6 +717,10 @@ def update_max_model_len(self, max_model_len: int) -> None:
                       self.effective_drafter_max_model_len = self.max_model_len
       
           def reset_mm_cache(self) -> None:
      +        """
      +        Clear the multi-modal cache that was used during profiling,
      +        but no longer needed during inference.
      +        """
               if self.mm_budget:
                   self.mm_budget.reset_cache()
      ```
- [`d6416fd`](https://github.com/vllm-project/vllm/commit/d6416fdde93476afc51a51e391a9b44d2c25df2f) pin LMCache to v0.3.9 or greater with vLLM v0.15.0 (#33440) (Greg Pereira)
  - Stats: +1/-1 (total 2)
  - Files:
    - modified: requirements/kv_connectors.txt (+1/-1)
      ```diff
      @@ -1,2 +1,2 @@
      -lmcache
      +lmcache >= 0.3.9
       nixl >= 0.7.1 # Required for disaggregated prefill
      ```
- [`0fb3157`](https://github.com/vllm-project/vllm/commit/0fb3157267127a69667f5c9b92d030c0de10614a) [ROCm][CI] Update huggingface-hub pin (#33492) (Andreas Karatzas)
  - Stats: +2/-0 (total 2)
  - Files:
    - modified: requirements/rocm-test.txt (+2/-0)
      ```diff
      @@ -94,3 +94,5 @@ timm==1.0.17
       albumentations==1.4.6
       # Pin transformers version
       transformers==4.57.3
      +# Pin HF Hub version
      +huggingface-hub==0.36.0
      ```
- [`a358e4d`](https://github.com/vllm-project/vllm/commit/a358e4dffe10ebff2c44434958bcc190ad2e5542) [Refactor] Make Renderer an abstract class (#33479) (Cyrus Leung)
  - Stats: +49/-50 (total 99)
  - Files:
    - modified: vllm/engine/protocol.py (+2/-2)
      ```diff
      @@ -11,7 +11,7 @@
       from vllm.outputs import PoolingRequestOutput, RequestOutput
       from vllm.plugins.io_processors import IOProcessor
       from vllm.pooling_params import PoolingParams
      -from vllm.renderers import RendererLike
      +from vllm.renderers import BaseRenderer
       from vllm.sampling_params import SamplingParams
       from vllm.tasks import SupportedTask
       from vllm.v1.engine import EngineCoreRequest
      @@ -28,7 +28,7 @@ class EngineClient(ABC):
       
           @property
           @abstractmethod
      -    def renderer(self) -> RendererLike: ...
      +    def renderer(self) -> BaseRenderer: ...
       
           @property
           @abstractmethod
      ```
    - modified: vllm/renderers/__init__.py (+2/-2)
      ```diff
      @@ -2,11 +2,11 @@
       # SPDX-FileCopyrightText: Copyright contributors to the vLLM project
       
       from .params import ChatParams, TokenizeParams, merge_kwargs
      -from .protocol import RendererLike
      +from .protocol import BaseRenderer
       from .registry import RendererRegistry, renderer_from_config
       
       __all__ = [
      -    "RendererLike",
      +    "BaseRenderer",
           "RendererRegistry",
           "renderer_from_config",
           "ChatParams",
      ```
    - modified: vllm/renderers/deepseek_v32.py (+4/-6)
      ```diff
      @@ -15,28 +15,26 @@
       from vllm.tokenizers.deepseek_v32 import DeepseekV32Tokenizer
       
       from .params import ChatParams
      -from .protocol import RendererLike
      +from .protocol import BaseRenderer
       
       logger = init_logger(__name__)
       
       
      -class DeepseekV32Renderer(RendererLike):
      +class DeepseekV32Renderer(BaseRenderer):
           @classmethod
           def from_config(
               cls,
               config: ModelConfig,
               tokenizer_kwargs: dict[str, Any],
      -    ) -> "RendererLike":
      +    ) -> "BaseRenderer":
               return cls(config, tokenizer_kwargs)
       
           def __init__(
               self,
               config: ModelConfig,
               tokenizer_kwargs: dict[str, Any],
           ) -> None:
      -        super().__init__()
      -
      -        self.config = config
      +        super().__init__(config)
       
               if config.skip_tokenizer_init:
                   tokenizer = None
      ```
    - modified: vllm/renderers/grok2.py (+4/-6)
      ```diff
      @@ -15,28 +15,26 @@
       from vllm.tokenizers.grok2 import Grok2Tokenizer
       
       from .params import ChatParams
      -from .protocol import RendererLike
      +from .protocol import BaseRenderer
       
       logger = init_logger(__name__)
       
       
      -class Grok2Renderer(RendererLike):
      +class Grok2Renderer(BaseRenderer):
           @classmethod
           def from_config(
               cls,
               config: ModelConfig,
               tokenizer_kwargs: dict[str, Any],
      -    ) -> "RendererLike":
      +    ) -> "BaseRenderer":
               return cls(config, tokenizer_kwargs)
       
           def __init__(
               self,
               config: ModelConfig,
               tokenizer_kwargs: dict[str, Any],
           ) -> None:
      -        super().__init__()
      -
      -        self.config = config
      +        super().__init__(config)
       
               if config.skip_tokenizer_init:
                   tokenizer = None
      ```
    - modified: vllm/renderers/hf.py (+4/-5)
      ```diff
      @@ -34,7 +34,7 @@
       from vllm.utils.func_utils import supports_kw
       
       from .params import ChatParams
      -from .protocol import RendererLike
      +from .protocol import BaseRenderer
       
       if TYPE_CHECKING:
           from vllm.multimodal.inputs import MultiModalDataDict, MultiModalUUIDDict
      @@ -584,23 +584,22 @@ def replace_vision_chunk_video_placeholder(
           return prompt_raw
       
       
      -class HfRenderer(RendererLike):
      +class HfRenderer(BaseRenderer):
           @classmethod
           def from_config(
               cls,
               config: ModelConfig,
               tokenizer_kwargs: dict[str, Any],
      -    ) -> "RendererLike":
      +    ) -> "BaseRenderer":
               return cls(config, tokenizer_kwargs)
       
           def __init__(
               self,
               config: ModelConfig,
               tokenizer_kwargs: dict[str, Any],
           ) -> None:
      -        super().__init__()
      +        super().__init__(config)
       
      -        self.config = config
               self.use_unified_vision_chunk = getattr(
                   config.hf_config, "use_unified_vision_chunk", False
               )
      ```
    - modified: vllm/renderers/mistral.py (+4/-6)
      ```diff
      @@ -17,7 +17,7 @@
       from vllm.utils.async_utils import make_async
       
       from .params import ChatParams
      -from .protocol import RendererLike
      +from .protocol import BaseRenderer
       
       logger = init_logger(__name__)
       
      @@ -49,23 +49,21 @@ def safe_apply_chat_template(
               raise ValueError(str(e)) from e
       
       
      -class MistralRenderer(RendererLike):
      +class MistralRenderer(BaseRenderer):
           @classmethod
           def from_config(
               cls,
               config: ModelConfig,
               tokenizer_kwargs: dict[str, Any],
      -    ) -> "RendererLike":
      +    ) -> "BaseRenderer":
               return cls(config, tokenizer_kwargs)
       
           def __init__(
               self,
               config: ModelConfig,
               tokenizer_kwargs: dict[str, Any],
           ) -> None:
      -        super().__init__()
      -
      -        self.config = config
      +        super().__init__(config)
       
               if config.skip_tokenizer_init:
                   tokenizer = None
      ```
    - modified: vllm/renderers/protocol.py (+16/-8)
      ```diff
      @@ -1,7 +1,8 @@
       # SPDX-License-Identifier: Apache-2.0
       # SPDX-FileCopyrightText: Copyright contributors to the vLLM project
       import asyncio
      -from typing import TYPE_CHECKING, Any, Protocol
      +from abc import ABC, abstractmethod
      +from typing import TYPE_CHECKING, Any
       
       from vllm.inputs import EmbedsPrompt, TextPrompt, TokensPrompt
       from vllm.tokenizers import TokenizerLike
      @@ -19,19 +20,26 @@
           )
       
       
      -class RendererLike(Protocol):
      -    config: "ModelConfig"
      -    _async_tokenizer: AsyncMicrobatchTokenizer
      -
      +class BaseRenderer(ABC):
           @classmethod
      +    @abstractmethod
           def from_config(
               cls,
               config: "ModelConfig",
               tokenizer_kwargs: dict[str, Any],
      -    ) -> "RendererLike":
      +    ) -> "BaseRenderer":
               raise NotImplementedError
       
      +    def __init__(self, config: "ModelConfig") -> None:
      +        super().__init__()
      +
      +        self.config = config
      +
      +        # Lazy initialization since offline LLM doesn't use async
      +        self._async_tokenizer: AsyncMicrobatchTokenizer | None = None
      +
           @property
      +    @abstractmethod
           def tokenizer(self) -> TokenizerLike | None:
               raise NotImplementedError
       
      @@ -43,8 +51,7 @@ def get_tokenizer(s...
      ```
    - modified: vllm/renderers/registry.py (+3/-3)
      ```diff
      @@ -7,7 +7,7 @@
       from vllm.tokenizers.registry import tokenizer_args_from_config
       from vllm.utils.import_utils import resolve_obj_by_qualname
       
      -from .protocol import RendererLike
      +from .protocol import BaseRenderer
       
       if TYPE_CHECKING:
           from vllm.config import ModelConfig
      @@ -43,7 +43,7 @@ def register(self, renderer_mode: str, module: str, class_name: str) -> None:
       
               return None
       
      -    def load_renderer_cls(self, renderer_mode: str) -> type[RendererLike]:
      +    def load_renderer_cls(self, renderer_mode: str) -> type[BaseRenderer]:
               if renderer_mode not in self.renderers:
                   raise ValueError(f"No renderer registered for {renderer_mode=!r}.")
       
      @@ -57,7 +57,7 @@ def load_renderer(
               renderer_mode: str,
               config: "ModelConfig",
               tokenizer_kwargs: dict[str, Any],
      -    ) -> RendererLike:
      +    ) -> BaseRenderer:
               renderer_cls = self.load_renderer_cls(renderer_mode)
               return renderer_cls.from_config(config, tokenizer_kwargs)
      ```
    - modified: vllm/renderers/terratorch.py (+4/-6)
      ```diff
      @@ -14,24 +14,22 @@
       from vllm.tokenizers import TokenizerLike
       
       from .params import ChatParams
      -from .protocol import RendererLike
      +from .protocol import BaseRenderer
       
       logger = init_logger(__name__)
       
       
      -class TerratorchRenderer(RendererLike):
      +class TerratorchRenderer(BaseRenderer):
           @classmethod
           def from_config(
               cls,
               config: "ModelConfig",
               tokenizer_kwargs: dict[str, Any],
      -    ) -> "RendererLike":
      +    ) -> "BaseRenderer":
               return cls(config)
       
           def __init__(self, config: ModelConfig) -> None:
      -        super().__init__()
      -
      -        self.config = config
      +        super().__init__(config)
       
               if not config.skip_tokenizer_init:
                   raise ValueError("Terratorch renderer requires `skip_tokenizer_init=True`")
      ```
    - modified: vllm/v1/engine/async_llm.py (+2/-2)
      ```diff
      @@ -24,7 +24,7 @@
       from vllm.outputs import STREAM_FINISHED, PoolingRequestOutput, RequestOutput
       from vllm.plugins.io_processors import get_io_processor
       from vllm.pooling_params import PoolingParams
      -from vllm.renderers import RendererLike, merge_kwargs
      +from vllm.renderers import BaseRenderer, merge_kwargs
       from vllm.sampling_params import RequestOutputKind, SamplingParams
       from vllm.tasks import SupportedTask
       from vllm.tokenizers import TokenizerLike
      @@ -844,7 +844,7 @@ def get_tokenizer(self) -> TokenizerLike:
               return self.input_processor.get_tokenizer()
       
           @property
      -    def renderer(self) -> RendererLike:
      +    def renderer(self) -> BaseRenderer:
               return self.input_processor.renderer
       
           async def is_tracing_enabled(self) -> bool:
      ```
    - modified: vllm/v1/engine/input_processor.py (+2/-2)
      ```diff
      @@ -29,7 +29,7 @@
       from vllm.multimodal.processing.context import set_request_id
       from vllm.multimodal.utils import argsort_mm_positions
       from vllm.pooling_params import PoolingParams
      -from vllm.renderers import RendererLike
      +from vllm.renderers import BaseRenderer
       from vllm.sampling_params import _SAMPLING_EPS, SamplingParams
       from vllm.tokenizers import TokenizerLike
       from vllm.tokenizers.mistral import MistralTokenizer
      @@ -96,7 +96,7 @@ def get_tokenizer(self) -> TokenizerLike:
               return self.input_preprocessor.get_tokenizer()
       
           @property
      -    def renderer(self) -> RendererLike:
      +    def renderer(self) -> BaseRenderer:
               return self.input_preprocessor.renderer
       
           def _validate_logprobs(
      ```
    - modified: vllm/v1/engine/llm_engine.py (+2/-2)
      ```diff
      @@ -21,7 +21,7 @@
       from vllm.outputs import PoolingRequestOutput, RequestOutput
       from vllm.plugins.io_processors import get_io_processor
       from vllm.pooling_params import PoolingParams
      -from vllm.renderers import RendererLike
      +from vllm.renderers import BaseRenderer
       from vllm.sampling_params import SamplingParams
       from vllm.tasks import SupportedTask
       from vllm.tokenizers import TokenizerLike
      @@ -367,7 +367,7 @@ def get_tokenizer(self) -> TokenizerLike:
               return self.input_processor.get_tokenizer()
       
           @property
      -    def renderer(self) -> RendererLike:
      +    def renderer(self) -> BaseRenderer:
               return self.input_processor.renderer
       
           def do_log_stats(self) -> None:
      ```
- [`0797811`](https://github.com/vllm-project/vllm/commit/079781177ae4c9fba429bf093cae73cf4cfae7a8) fix: Add SM120 (RTX Blackwell) support for FlashInfer CUTLASS NVFP4 MoE kernels (#33417) (Ren Honig)
  - Stats: +22/-110 (total 132)
  - Files:
    - modified: vllm/model_executor/layers/fused_moe/cutlass_moe.py (+6/-1)
      ```diff
      @@ -657,7 +657,12 @@ def expects_unquantized_inputs(self) -> bool:
       
           @staticmethod
           def _supports_current_device() -> bool:
      -        return current_platform.has_device_capability((10, 0))
      +        p = current_platform
      +        return p.is_cuda() and (
      +            p.is_device_capability_family(100)
      +            or p.is_device_capability_family(110)
      +            or p.is_device_capability_family(120)
      +        )
       
           @staticmethod
           def _supports_no_act_and_mul() -> bool:
      ```
    - modified: vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe.py (+2/-1)
      ```diff
      @@ -54,7 +54,8 @@ def activation_format() -> mk.FusedMoEActivationFormat:
       
           @staticmethod
           def _supports_current_device() -> bool:
      -        return current_platform.is_device_capability_family(100)
      +        p = current_platform
      +        return p.is_cuda() and p.is_device_capability_family(100)
       
           @staticmethod
           def _supports_no_act_and_mul() -> bool:
      ```
    - modified: vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py (+14/-13)
      ```diff
      @@ -84,11 +84,14 @@ def expects_unquantized_inputs(self) -> bool:
       
           @staticmethod
           def _supports_current_device() -> bool:
      +        p = current_platform
               return (
      -            current_platform.is_cuda()
      +            p.is_cuda()
                   and (
      -                current_platform.is_device_capability((9, 0))
      -                or current_platform.is_device_capability_family(100)
      +                p.is_device_capability(90)
      +                or p.is_device_capability_family(100)
      +                or p.is_device_capability_family(110)
      +                or p.is_device_capability_family(120)
                   )
                   and has_flashinfer_cutlass_fused_moe()
               )
      @@ -102,29 +105,27 @@ def _supports_quant_scheme(
               weight_key: QuantKey | None,
               activation_key: QuantKey | None,
           ) -> bool:
      -        # The following are supported by FlashInferExperts:
      -        #   * unquantized
      -        #   * fp8 static per-tensor on 9.0+
      -        #   * fp8 block on 9.0
      -        #   * nvfp4 on 10.0+
      -
               p = current_platform
               scheme = (weight_key, activation_key)
      +        # The following are supported by FlashInferExperts:
               return (
      +            #...
      ```
    - modified: vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py (+0/-1)
      ```diff
      @@ -30,7 +30,6 @@
       def _supports_current_device() -> bool:
           """Supports only Blackwell-family GPUs."""
           p = current_platform
      -    # Add check flashinfer trtllm is available
           return p.is_cuda() and p.is_device_capability_family(100)
      ```
    - modified: vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py (+0/-27)
      ```diff
      @@ -6,7 +6,6 @@
       
       import torch
       
      -import vllm.envs as envs
       import vllm.model_executor.layers.fused_moe.modular_kernel as mk
       from vllm import _custom_ops as ops
       from vllm.logger import init_logger
      @@ -24,10 +23,6 @@
           kNvfp4Static,
       )
       from vllm.platforms import current_platform
      -from vllm.utils.flashinfer import (
      -    has_flashinfer_cutedsl_grouped_gemm_nt_masked,
      -    has_flashinfer_cutlass_fused_moe,
      -)
       
       if TYPE_CHECKING:
           from vllm.model_executor.layers.fused_moe.oracle.nvfp4 import (
      @@ -38,8 +33,6 @@
       
       
       __all__ = [
      -    "is_flashinfer_fp4_cutlass_moe_available",
      -    "is_flashinfer_fp4_cutedsl_moe_available",
           "reorder_w1w3_to_w3w1",
       ]
       
      @@ -124,26 +117,6 @@ def _make_reason(reason: str) -> str:
           return True, None
       
       
      -def is_flashinfer_fp4_cutlass_moe_available() -> bool:
      -    """Return `True` when FlashInfer CUTLASS NV-FP4 kernels can be used."""
      -    return (
      -        envs.VLLM_USE_FLASHINFER_MOE_FP4
      -        and has_flashinfer_cutlass_fused_moe()
      -        and current_platform.is_cuda()
      -        and current_platform.has_device_capability(100)
      -    )
      -
      -
      -def is_flashinfer_fp4_cutedsl_moe_available() -> bool:
      -    """Return ``True`` when FlashInfe...
      ```
    - removed: vllm/model_executor/layers/quantization/utils/nvfp4_moe_support.py (+0/-67)
      ```diff
      @@ -1,67 +0,0 @@
      -# SPDX-License-Identifier: Apache-2.0
      -# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
      -from dataclasses import dataclass
      -
      -import vllm.envs as envs
      -from vllm.logger import init_logger
      -from vllm.model_executor.layers.quantization.utils.flashinfer_fp4_moe import (
      -    is_flashinfer_fp4_cutedsl_moe_available,
      -    is_flashinfer_fp4_cutlass_moe_available,
      -)
      -from vllm.model_executor.layers.quantization.utils.marlin_utils_fp4 import (
      -    is_fp4_marlin_supported,
      -)
      -from vllm.model_executor.layers.quantization.utils.nvfp4_utils import (
      -    cutlass_fp4_supported,
      -)
      -
      -__all__ = ["detect_nvfp4_moe_support", "NvFp4Support"]
      -
      -_logger = init_logger(__name__)
      -
      -
      -@dataclass(frozen=True)
      -class NvFp4Support:
      -    """Result container for NV-FP4 capability probing."""
      -
      -    cutlass_supported: bool
      -    allow_flashinfer: bool
      -    use_marlin: bool
      -
      -
      -def detect_nvfp4_moe_support(class_name: str = "") -> NvFp4Support:
      -    """Detect platform support for NV-FP4 fused-MoE path"""
      -    cutlass_supported = cutlass_fp4_supported()
      -
      -    allow_flashinfer = cutlass_supported and (
      -        is_flashinfer_fp4_cutlass_moe_available()
      -        or is...
      ```
- [`63c0889`](https://github.com/vllm-project/vllm/commit/63c0889416f0d4c3979c4046c6bf41c43143080c) [Misc] Fix flashinfer related tests (#33462) (Roy Wang)
  - Stats: +9/-8 (total 17)
  - Files:
    - modified: tests/kernels/moe/test_moe.py (+1/-1)
      ```diff
      @@ -412,7 +412,7 @@ def test_naive_block_assignment_moe(
           monkeypatch,
           workspace_init,
       ):
      -    current_platform.seed_everything(7)
      +    set_random_seed(7)
       
           monkeypatch.setenv("VLLM_FUSED_MOE_CHUNK_SIZE", str(chunk_size))
      ```
    - modified: tests/kernels/quantization/test_flashinfer_nvfp4_scaled_mm.py (+1/-1)
      ```diff
      @@ -74,7 +74,7 @@ def get_ref_results(
       @pytest.mark.parametrize("shape", SHAPES)
       @pytest.mark.parametrize("seed", SEEDS)
       @pytest.mark.parametrize("device", CUDA_DEVICES)
      -@pytest.mark.parametrize("backend", ["cutlass", "trtllm"])
      +@pytest.mark.parametrize("backend", ["cutlass", "cudnn", "trtllm"])
       @pytest.mark.parametrize("autotune", [False, True])
       @torch.inference_mode()
       def test_flashinfer_nvfp4_gemm(
      ```
    - modified: tests/kernels/quantization/test_fp8_quant.py (+2/-2)
      ```diff
      @@ -174,7 +174,7 @@ def test_static_fp8_quant_group_2d(
                   f"group_shape ({group_shape[0]}, {group_shape[1]})"
               )
       
      -    current_platform.seed_everything(seed)
      +    set_random_seed(seed)
       
           x = torch.rand(num_tokens, hidden_size, dtype=dtype, device="cuda")
           ref_out, scale = scaled_quantize(
      @@ -202,7 +202,7 @@ def test_static_fp8_quant_1d_scale(
           group_shape: tuple[int, int],
       ) -> None:
           """Test static FP8 quantization with 1D scale (per-token or per-channel)."""
      -    current_platform.seed_everything(seed)
      +    set_random_seed(seed)
       
           x = torch.rand(num_tokens, hidden_size, dtype=dtype, device="cuda")
           ref_out, scale_2d = scaled_quantize(
      ```
    - modified: vllm/model_executor/layers/quantization/utils/nvfp4_utils.py (+4/-3)
      ```diff
      @@ -154,9 +154,10 @@ def convert_to_nvfp4_linear_kernel_format(
               )
               layer.weight = torch.nn.Parameter(weight, requires_grad=False)
               layer.weight_scale = torch.nn.Parameter(weight_scale, requires_grad=False)
      -    elif (
      -        backend == NvFp4LinearBackend.VLLM_CUTLASS
      -        or backend == NvFp4LinearBackend.FLASHINFER_CUTLASS
      +    elif backend in (
      +        NvFp4LinearBackend.VLLM_CUTLASS,
      +        NvFp4LinearBackend.FLASHINFER_CUTLASS,
      +        NvFp4LinearBackend.FLASHINFER_CUDNN,
           ):
               weight, weight_scale, weights_padding_cols = prepare_weights_for_nvfp4_cutlass(
                   layer.weight.data, layer.weight_scale.data
      ```
    - modified: vllm/utils/flashinfer.py (+1/-1)
      ```diff
      @@ -521,7 +521,7 @@ def flashinfer_scaled_fp4_mm(
           assert a.stride(-1) == 1 and b.stride(-1) == 1
           assert a.shape[1] == b.shape[1]
       
      -    if backend == "cutlass":
      +    if backend in ("cutlass", "cudnn"):
               block_scale_a = block_scale_a.view(torch.uint8)
               block_scale_b = block_scale_b.view(torch.uint8)
      ```
- [`1e86c80`](https://github.com/vllm-project/vllm/commit/1e86c802d473b4ca4e889a1b2585a05aad7220ba) Fix grammar (#33121) (smashyalts)
  - Stats: +1/-1 (total 2)
  - Files:
    - modified: vllm/v1/worker/cpu_worker.py (+1/-1)
      ```diff
      @@ -141,7 +141,7 @@ def _get_autobind_cpu_ids(
                   CpuPlatform.get_allowed_cpu_core_node_list()
               )
               assert len(allowed_numa_nodes) >= self.parallel_config.world_size, (
      -            f"No enough allowed NUMA nodes to bind threads of "
      +            f"Not enough allowed NUMA nodes to bind threads of "
                   f"{self.parallel_config.world_size} CPUWorkers. "
                   f"Allowed NUMA nodes are {allowed_numa_nodes}. "
                   "Please try to bind threads manually."
      ```
- [`fedf643`](https://github.com/vllm-project/vllm/commit/fedf64332e9940c8119f584a74b9c08bab6852b7) [Bugfix]: Fix display errors in TORCH_CHECK messages (#32942) (linhaifeng)
  - Stats: +8/-8 (total 16)
  - Files:
    - modified: csrc/cpu/sgl-kernels/gemm.cpp (+1/-1)
      ```diff
      @@ -265,7 +265,7 @@ void tinygemm_kernel(
               // mb_size = 4
               case 0x42: LAUNCH_TINYGEMM_KERNEL_NN(4, 32); break;
               case 0x44: LAUNCH_TINYGEMM_KERNEL_NN(4, 64); break;
      -        default: TORCH_CHECK(false, "Unexpected block size, ", mb_size, "x", "nb_size");
      +        default: TORCH_CHECK(false, "Unexpected block size, ", mb_size, "x", nb_size);
             }
           }
         }
      ```
    - modified: csrc/cpu/sgl-kernels/gemm_fp8.cpp (+1/-1)
      ```diff
      @@ -324,7 +324,7 @@ void tinygemm_kernel(
               case 0x22: LAUNCH_TINYGEMM_KERNEL_NN(2, 32); break;
               case 0x32: LAUNCH_TINYGEMM_KERNEL_NN(3, 32); break;
               case 0x42: LAUNCH_TINYGEMM_KERNEL_NN(4, 32); break;
      -        default: TORCH_CHECK(false, "Unexpected block size, ", mb_size, "x", "nb_size");
      +        default: TORCH_CHECK(false, "Unexpected block size, ", mb_size, "x", nb_size);
             }
           }
         }
      ```
    - modified: csrc/cpu/sgl-kernels/gemm_int8.cpp (+1/-1)
      ```diff
      @@ -180,7 +180,7 @@ void tinygemm_kernel(
               // mb_size = 4
               case 0x42: LAUNCH_TINYGEMM_KERNEL_NN(4, 32); break;
               case 0x44: LAUNCH_TINYGEMM_KERNEL_NN(4, 64); break;
      -        default: TORCH_CHECK(false, "Unexpected block size, ", mb_size, "x", "nb_size");
      +        default: TORCH_CHECK(false, "Unexpected block size, ", mb_size, "x", nb_size);
             }
           }
         }
      ```
    - modified: csrc/cpu/sgl-kernels/moe.cpp (+2/-2)
      ```diff
      @@ -398,7 +398,7 @@ void tinygemm_kernel(
               case 0x32: LAUNCH_TINYGEMM_KERNEL_NN(3, 32); break;
               // mb_size = 4
               case 0x42: LAUNCH_TINYGEMM_KERNEL_NN(4, 32); break;
      -        default: TORCH_CHECK(false, "Unexpected block size, ", mb_size, "x", "nb_size");
      +        default: TORCH_CHECK(false, "Unexpected block size, ", mb_size, "x", nb_size);
             }
           }
         }
      @@ -511,7 +511,7 @@ void tinygemm_kernel(
               case 0x32: LAUNCH_TINYGEMM_KERNEL_NN2(3, 32); break;
               // mb_size = 4
               case 0x42: LAUNCH_TINYGEMM_KERNEL_NN2(4, 32); break;
      -        default: TORCH_CHECK(false, "Unexpected block size, ", mb_size, "x", "nb_size");
      +        default: TORCH_CHECK(false, "Unexpected block size, ", mb_size, "x", nb_size);
             }
           }
         }
      ```
    - modified: csrc/cpu/sgl-kernels/moe_int8.cpp (+2/-2)
      ```diff
      @@ -271,7 +271,7 @@ void tinygemm_kernel(
               case 0x22: LAUNCH_TINYGEMM_KERNEL_VNNI(2, 32); break;
               case 0x32: LAUNCH_TINYGEMM_KERNEL_VNNI(3, 32); break;
               case 0x42: LAUNCH_TINYGEMM_KERNEL_VNNI(4, 32); break;
      -        default: TORCH_CHECK(false, "Unexpected block size, ", mb_size, "x", "nb_size");
      +        default: TORCH_CHECK(false, "Unexpected block size, ", mb_size, "x", nb_size);
             }
           }
         }
      @@ -401,7 +401,7 @@ void tinygemm_kernel(
               case 0x22: LAUNCH_TINYGEMM_KERNEL_VNNI2(2, 32); break;
               case 0x32: LAUNCH_TINYGEMM_KERNEL_VNNI2(3, 32); break;
               case 0x42: LAUNCH_TINYGEMM_KERNEL_VNNI2(4, 32); break;
      -        default: TORCH_CHECK(false, "Unexpected block size, ", mb_size, "x", "nb_size");
      +        default: TORCH_CHECK(false, "Unexpected block size, ", mb_size, "x", nb_size);
             }
           }
         }
      ```
    - modified: csrc/moe/marlin_moe_wna16/ops.cu (+1/-1)
      ```diff
      @@ -770,7 +770,7 @@ torch::Tensor moe_wna16_marlin_gemm(
           b_bias = b_bias_or_none.value();
           TORCH_CHECK(b_bias.device().is_cuda(), "b_bias is not on GPU");
           TORCH_CHECK(b_bias.is_contiguous(), "b_bias is not contiguous");
      -    TORCH_CHECK(b_bias.size(1) == size_n, "b_bias.size(0) != size_n");
      +    TORCH_CHECK(b_bias.size(1) == size_n, "b_bias.size(1) != size_n");
           TORCH_CHECK(b_bias.stride(1) == 1, "b_bias.stride(1) != 1");
         } else {
           b_bias = torch::empty({0}, options);
      ```
- [`2238a12`](https://github.com/vllm-project/vllm/commit/2238a12c13b9a1e27f132473a0fff1b4b233b1dc) [Misc] support collect_env for endpoint /server_info (#33246) (Xiao Yang)
  - Stats: +9/-0 (total 9)
  - Files:
    - modified: vllm/entrypoints/serve/instrumentator/server_info.py (+9/-0)
      ```diff
      @@ -2,13 +2,16 @@
       # SPDX-FileCopyrightText: Copyright contributors to the vLLM project
       
       
      +import asyncio
      +import functools
       from typing import Annotated, Literal
       
       import pydantic
       from fastapi import APIRouter, FastAPI, Query, Request
       from fastapi.responses import JSONResponse
       
       import vllm.envs as envs
      +from vllm.collect_env import get_env_info
       from vllm.config import VllmConfig
       from vllm.logger import init_logger
       
      @@ -32,6 +35,11 @@ def _get_vllm_env_vars():
           return vllm_envs
       
       
      +@functools.lru_cache(maxsize=1)
      +def _get_system_env_info_cached():
      +    return get_env_info()._asdict()
      +
      +
       @router.get("/server_info")
       async def show_server_info(
           raw_request: Request,
      @@ -46,6 +54,7 @@ async def show_server_info(
               ),
               # fallback=str is needed to handle e.g. torch.dtype
               "vllm_env": _get_vllm_env_vars(),
      +        "system_env": await asyncio.to_thread(_get_system_env_info_cached),
           }
           return JSONResponse(content=server_info)
      ```
- [`ce0afe2`](https://github.com/vllm-project/vllm/commit/ce0afe2451b0f8a04547514f9efcac98e39beab0) Update `huggingface-hub` pin for the last time before Transformers v5 (#33473) (Harry Mellor)
  - Stats: +1/-1 (total 2)
  - Files:
    - modified: requirements/test.txt (+1/-1)
      ```diff
      @@ -332,7 +332,7 @@ httpx==0.27.2
           #   -r requirements/test.in
           #   perceptron
           #   schemathesis
      -huggingface-hub==0.34.3
      +huggingface-hub==0.36.0
           # via
           #   accelerate
           #   datasets
      ```
- [`88c3e11`](https://github.com/vllm-project/vllm/commit/88c3e114d8f564d1e88ff2ef5be56a979792e90b) [Refactor] Move MM data parsing outside processor (#33408) (Cyrus Leung)
  - Stats: +228/-139 (total 367)
  - Files (truncated):
    - modified: tests/models/multimodal/processing/test_common.py (+6/-5)
      ```diff
      @@ -176,7 +176,7 @@ def get_text_token_prompts(
           if model_type in MM_DATA_PATCHES:
               mm_data = MM_DATA_PATCHES[model_type](mm_data)
       
      -    parsed_data = processor.data_parser.parse_mm_data(mm_data)
      +    parsed_data = processor.info.parse_mm_data(mm_data)
           mm_counts = {k: len(vs) for k, vs in parsed_data.items()}
       
           text_prompt: str | None
      @@ -336,17 +336,18 @@ def _test_processing_correctness_one(
           model_type = model_config.hf_config.model_type
       
           text_prompt, token_prompt = get_text_token_prompts(baseline_processor, mm_data)
      +    mm_items = baseline_processor.info.parse_mm_data(mm_data)
           ignore_mm_keys = _IGNORE_MM_KEYS.get(model_type, set[str]())
       
           baseline_tokenized_result = baseline_processor.apply(
               token_prompt,
      -        mm_data=mm_data,
      +        mm_items=mm_items,
               hf_processor_mm_kwargs={},
           )
       
           cached_tokenized_result = cached_processor.apply(
               token_prompt,
      -        mm_data=mm_data,
      +        mm_items=mm_items,
               hf_processor_mm_kwargs={},
           )
       
      @@ -360,12 +361,12 @@ def _test_processing_correctness_one(
           if text_prompt is not None:
               baseline_text_result = baseline_processor.apply...
      ```
    - modified: tests/models/multimodal/processing/test_gemma3.py (+5/-1)
      ```diff
      @@ -175,7 +175,11 @@ def test_get_image_size_with_most_features(
       
           for asset in image_assets:
               mm_data = {"image": [asset.pil_image]}
      -        processed_inputs = processor.apply(prompt, mm_data, hf_processor_mm_kwargs)
      +        processed_inputs = processor.apply(
      +            prompt,
      +            mm_items=processor.info.parse_mm_data(mm_data),
      +            hf_processor_mm_kwargs=hf_processor_mm_kwargs,
      +        )
               mm_kwargs_data = processed_inputs["mm_kwargs"].get_data()
               num_patches_tensor = mm_kwargs_data["num_patches"]
               tokens = int(num_patches_tensor.item()) * image_seq_length
      ```
    - modified: tests/models/multimodal/processing/test_glm4_1v.py (+15/-3)
      ```diff
      @@ -52,7 +52,11 @@ def test_processor_override(
           metadata["fps"] = fps
           mm_data = {"video": [(video, metadata)]}
       
      -    processed_inputs = processor.apply(prompt, mm_data, hf_processor_mm_kwargs)
      +    processed_inputs = processor.apply(
      +        prompt,
      +        mm_items=processor.info.parse_mm_data(mm_data),
      +        hf_processor_mm_kwargs=hf_processor_mm_kwargs,
      +    )
       
           # Ensure we have the right number of placeholders per num_crops size
           hf_processor = processor.info.get_hf_processor(**hf_processor_mm_kwargs)
      @@ -100,8 +104,16 @@ def test_video_loader_consistency(
           static_mm_data = {"video": [(static_video, static_metadata)]}
           dynamic_mm_data = {"video": [(dynamic_video, dynamic_metadata)]}
       
      -    static_outputs = processor.apply(prompt, static_mm_data, hf_processor_mm_kwargs)
      -    dynamic_outputs = processor.apply(prompt, dynamic_mm_data, hf_processor_mm_kwargs)
      +    static_outputs = processor.apply(
      +        prompt,
      +        mm_items=processor.info.parse_mm_data(static_mm_data),
      +        hf_processor_mm_kwargs=hf_processor_mm_kwargs,
      +    )
      +    dynamic_outputs = processor.apply(
      +        prompt,
      +        mm_items=processor.info.parse_mm_data(dynam...
      ```
    - modified: tests/models/multimodal/processing/test_h2ovl.py (+5/-1)
      ```diff
      @@ -106,7 +106,11 @@ def _run_check(
               for image in images
           )
       
      -    processed_inputs = processor.apply(prompt, mm_data, mm_processor_kwargs)
      +    processed_inputs = processor.apply(
      +        prompt,
      +        mm_items=processor.info.parse_mm_data(mm_data),
      +        hf_processor_mm_kwargs=mm_processor_kwargs,
      +    )
       
           # Ensure we have the right number of placeholders per num_crops size
           image_token_id = tokenizer.convert_tokens_to_ids("<IMG_CONTEXT>")
      ```
    - modified: tests/models/multimodal/processing/test_idefics3.py (+5/-1)
      ```diff
      @@ -55,7 +55,11 @@ def test_processor_override(
           dummy_image = image_assets[0].pil_image.resize(dummy_image_size)
           mm_data = {"image": [dummy_image] * num_imgs}
       
      -    processed_inputs = processor.apply(prompt, mm_data, hf_processor_mm_kwargs)
      +    processed_inputs = processor.apply(
      +        prompt,
      +        mm_items=processor.info.parse_mm_data(mm_data),
      +        hf_processor_mm_kwargs=hf_processor_mm_kwargs,
      +    )
       
           # Ensure the placeholders format are correct
           hf_processor = processor.info.get_hf_processor(**hf_processor_mm_kwargs)
      ```
    - modified: tests/models/multimodal/processing/test_internvl.py (+5/-1)
      ```diff
      @@ -66,7 +66,11 @@ def _run_check(
               for image in images
           )
       
      -    processed_inputs = processor.apply(prompt, mm_data, mm_processor_kwargs)
      +    processed_inputs = processor.apply(
      +        prompt,
      +        mm_items=processor.info.parse_mm_data(mm_data),
      +        hf_processor_mm_kwargs=mm_processor_kwargs,
      +    )
       
           # Ensure we have the right number of placeholders per num_crops size
           image_token_id = tokenizer.convert_tokens_to_ids("<IMG_CONTEXT>")
      ```
    - modified: tests/models/multimodal/processing/test_llama4.py (+5/-1)
      ```diff
      @@ -49,7 +49,11 @@ def test_processor_override(
           if tokenized_prompt:
               prompt = tokenizer.encode(prompt)
       
      -    processed_inputs = processor.apply(prompt, mm_data, mm_processor_kwargs)
      +    processed_inputs = processor.apply(
      +        prompt,
      +        mm_items=processor.info.parse_mm_data(mm_data),
      +        hf_processor_mm_kwargs=mm_processor_kwargs,
      +    )
           mm_data = processed_inputs["mm_kwargs"].get_data()
       
           # place holder replacements
      ```
    - modified: tests/models/multimodal/processing/test_llava_next.py (+5/-1)
      ```diff
      @@ -87,7 +87,11 @@ def _validate_image_prompt_replacements_one(
           try:
               # The processor will throw an error if there is a mismatch
               # in the prompt replacements
      -        processed_inputs = processor.apply(prompt, mm_data, {})
      +        processed_inputs = processor.apply(
      +            prompt,
      +            mm_items=processor.info.parse_mm_data(mm_data),
      +            hf_processor_mm_kwargs={},
      +        )
       
               image_placeholders = processed_inputs["mm_placeholders"]["image"]
               assert len(image_placeholders) == num_imgs
      ```
    - modified: tests/models/multimodal/processing/test_llava_onevision.py (+5/-1)
      ```diff
      @@ -87,7 +87,11 @@ def _validate_image_prompt_replacements_one(
           try:
               # The processor will throw an error if there is a mismatch
               # in the prompt replacements
      -        processed_inputs = processor.apply(prompt, mm_data, {})
      +        processed_inputs = processor.apply(
      +            prompt,
      +            mm_items=processor.info.parse_mm_data(mm_data),
      +            hf_processor_mm_kwargs={},
      +        )
       
               image_placeholders = processed_inputs["mm_placeholders"]["image"]
               assert len(image_placeholders) == num_imgs
      ```
    - modified: tests/models/multimodal/processing/test_minimax_vl_01.py (+10/-2)
      ```diff
      @@ -29,7 +29,11 @@ def test_processor_override(
           image = Image.new("RGB", size=(364, 364))
           mm_data = {"image": [image] * num_imgs}
       
      -    processed_inputs = processor.apply(prompt, mm_data, {})
      +    processed_inputs = processor.apply(
      +        prompt,
      +        mm_items=processor.info.parse_mm_data(mm_data),
      +        hf_processor_mm_kwargs={},
      +    )
           image_placeholders = processed_inputs["mm_placeholders"]["image"]
       
           assert len(image_placeholders) == num_imgs
      @@ -46,7 +50,11 @@ def _validate_image_prompt_replacements_one(
           mm_data = {"image": [image] * num_imgs}
       
           try:
      -        processed_inputs = processor.apply(prompt, mm_data, {})
      +        processed_inputs = processor.apply(
      +            prompt,
      +            mm_items=processor.info.parse_mm_data(mm_data),
      +            hf_processor_mm_kwargs={},
      +        )
       
               image_placeholders = processed_inputs["mm_placeholders"]["image"]
               assert len(image_placeholders) == num_imgs
      ```
    - modified: tests/models/multimodal/processing/test_nemotron_vl.py (+5/-1)
      ```diff
      @@ -68,7 +68,11 @@ def _run_check(
               for image in images
           )
           print(total_expected_num_patches)
      -    processed_inputs = processor.apply(prompt, mm_data, mm_processor_kwargs)
      +    processed_inputs = processor.apply(
      +        prompt,
      +        mm_items=processor.info.parse_mm_data(mm_data),
      +        hf_processor_mm_kwargs=mm_processor_kwargs,
      +    )
       
           # Ensure we have the right number of placeholders per num_crops size
           image_token_id = tokenizer.convert_tokens_to_ids("<image>")
      ```
    - modified: tests/models/multimodal/processing/test_phi3v.py (+5/-1)
      ```diff
      @@ -47,7 +47,11 @@ def test_processor_override(
           prompt = f"<|user|>\n{img_str}<|end|>\n<|assistant|>\n"
           mm_data = {"image": [image_assets[0].pil_image] * num_imgs}
       
      -    processed_inputs = processor.apply(prompt, mm_data, hf_processor_mm_kwargs)
      +    processed_inputs = processor.apply(
      +        prompt,
      +        mm_items=processor.info.parse_mm_data(mm_data),
      +        hf_processor_mm_kwargs=hf_processor_mm_kwargs,
      +    )
       
           # Ensure we have the right number of placeholders per num_crops size
           img_tok_count = processed_inputs["prompt_token_ids"].count(_IMAGE_TOKEN_ID)
      ```
    - modified: tests/models/multimodal/processing/test_phi4mm.py (+5/-1)
      ```diff
      @@ -51,7 +51,11 @@ def test_processor_override(
           dummy_image = image_assets[0].pil_image.resize(dummy_image_size)
           mm_data = {"image": [dummy_image] * num_imgs}
       
      -    processed_inputs = processor.apply(prompt, mm_data, hf_processor_mm_kwargs)
      +    processed_inputs = processor.apply(
      +        prompt,
      +        mm_items=processor.info.parse_mm_data(mm_data),
      +        hf_processor_mm_kwargs=hf_processor_mm_kwargs,
      +    )
       
           # Ensure we have the right number of placeholders per num_crops size
           img_tok_count = processed_inputs["prompt_token_ids"].count(
      ```
    - modified: tests/models/multimodal/processing/test_qwen2_vl.py (+10/-2)
      ```diff
      @@ -42,7 +42,11 @@ def test_processor_override(
           prompt = "<|vision_start|><|image_pad|><|vision_end|>" * num_imgs
           mm_data = {"image": [image_assets[0].pil_image] * num_imgs}
       
      -    processed_inputs = processor.apply(prompt, mm_data, hf_processor_mm_kwargs)
      +    processed_inputs = processor.apply(
      +        prompt,
      +        mm_items=processor.info.parse_mm_data(mm_data),
      +        hf_processor_mm_kwargs=hf_processor_mm_kwargs,
      +    )
       
           # Ensure we have the right number of placeholders per num_crops size
           hf_processor = processor.info.get_hf_processor(**hf_processor_mm_kwargs)
      @@ -83,7 +87,11 @@ def test_get_image_size_with_most_features(
           prompt = "<|vision_start|><|image_pad|><|vision_end|>"
           for asset in image_assets:
               mm_data = {"image": [asset.pil_image]}
      -        processed_inputs = processor.apply(prompt, mm_data, hf_processor_mm_kwargs)
      +        processed_inputs = processor.apply(
      +            prompt,
      +            mm_items=processor.info.parse_mm_data(mm_data),
      +            hf_processor_mm_kwargs=hf_processor_mm_kwargs,
      +        )
               grid_thw = processed_inputs["mm_kwargs"].get_data()["image_grid_thw"].tolist()
               t, h, w = grid_...
      ```
    - modified: tests/models/multimodal/processing/test_qwen3_omni.py (+10/-2)
      ```diff
      @@ -51,7 +51,11 @@ def test_processor_with_audio_sample_rate(
           hf_processor_mm_kwargs: dict[str, Any] = {
               "audio_sample_rate": audio_sample_rate,
           }
      -    processed_inputs = processor.apply(prompt, mm_data, hf_processor_mm_kwargs)
      +    processed_inputs = processor.apply(
      +        prompt,
      +        mm_items=processor.info.parse_mm_data(mm_data),
      +        hf_processor_mm_kwargs=hf_processor_mm_kwargs,
      +    )
       
           # Verify audio tokens are generated
           hf_processor = processor.info.get_hf_processor(**hf_processor_mm_kwargs)
      @@ -90,7 +94,11 @@ def get_token_count(duration: float) -> int:
               hf_processor_mm_kwargs: dict[str, Any] = {
                   "audio_sample_rate": audio_sample_rate,
               }
      -        processed = processor.apply(prompt, mm_data, hf_processor_mm_kwargs)
      +        processed = processor.apply(
      +            prompt,
      +            mm_items=processor.info.parse_mm_data(mm_data),
      +            hf_processor_mm_kwargs=hf_processor_mm_kwargs,
      +        )
               hf_proc = processor.info.get_hf_processor(**hf_processor_mm_kwargs)
               audio_token_id = tokenizer.convert_tokens_to_ids(hf_proc.audio_token)
               return processed["prompt_token_ids"]....
      ```
    - modified: tests/models/multimodal/processing/test_smolvlm.py (+5/-1)
      ```diff
      @@ -55,7 +55,11 @@ def test_processor_override(
           dummy_image = image_assets[0].pil_image.resize(dummy_image_size)
           mm_data = {"image": [dummy_image] * num_imgs}
       
      -    processed_inputs = processor.apply(prompt, mm_data, hf_processor_mm_kwargs)
      +    processed_inputs = processor.apply(
      +        prompt,
      +        mm_items=processor.info.parse_mm_data(mm_data),
      +        hf_processor_mm_kwargs=hf_processor_mm_kwargs,
      +    )
       
           # Ensure the placeholders format are correct
           hf_processor = processor.info.get_hf_processor(**hf_processor_mm_kwargs)
      ```
- [`92924b2`](https://github.com/vllm-project/vllm/commit/92924b2dddebc6dc80b3fff6c84ff095530b8e58) [Deprecation] Remove deprecated items related to pooling (#33477) (Cyrus Leung)
  - Stats: +52/-105 (total 157)
  - Files:
    - modified: docs/models/pooling_models.md (+0/-9)
      ```diff
      @@ -352,15 +352,6 @@ We have split the `encode` task into two more specific token-wise tasks: `token_
       - `token_embed` is the same as `embed`, using normalization as the activation.
       - `token_classify` is the same as `classify`, by default using softmax as the activation.
       
      -### Remove softmax from PoolingParams
      -
      -We are going to remove `softmax` and `activation` from `PoolingParams` in v0.15. Instead, use `use_activation`, since we allow `classify` and `token_classify` to use any activation function.
      -
      -### as_reward_model
      -
      -!!! warning
      -    We are going to remove `--convert reward` in v0.15, use `--convert embed` instead.
      -
       Pooling models now default support all pooling, you can use it without any settings.
       
       - Extracting hidden states prefers using `token_embed` task.
      ```
    - modified: vllm/config/model.py (+1/-17)
      ```diff
      @@ -75,7 +75,7 @@
       logger = init_logger(__name__)
       
       RunnerOption = Literal["auto", RunnerType]
      -ConvertType = Literal["none", "embed", "classify", "reward", "mm_encoder_only"]
      +ConvertType = Literal["none", "embed", "classify"]
       ConvertOption = Literal["auto", ConvertType]
       TokenizerMode = Literal["auto", "hf", "slow", "mistral", "deepseek_v32"]
       ModelDType = Literal["auto", "half", "float16", "bfloat16", "float", "float32"]
      @@ -499,15 +499,6 @@ def __post_init__(
               )
               self.model_arch_config = self.get_model_arch_config()
       
      -        if self.convert == "mm_encoder_only":
      -            logger.warning_once(
      -                "`--convert mm_encoder_only` is deprecated and "
      -                "will be removed in v0.15. "
      -                "Please use --mm-encoder-only` instead."
      -            )
      -            mm_encoder_only = True
      -            self.convert = "none"
      -
               architectures = self.architectures
               registry = self.registry
               is_generative_model = registry.is_text_generation_model(architectures, self)
      @@ -855,13 +846,6 @@ def _get_convert_type(
               runner_type: RunnerType,
               convert: ConvertOption,
           ) -> ConvertType:
      -        if co...
      ```
    - modified: vllm/config/pooler.py (+6/-45)
      ```diff
      @@ -45,11 +45,13 @@ class PoolerConfig:
           The pooling method used for tokenwise pooling.
           """
       
      -    ## for embeddings models
      -    normalize: bool | None = None
      +    use_activation: bool | None = None
           """
      -    DEPRECATED: please use `use_activation` instead.
      +    Whether to apply activation function to the pooler outputs.
      +    `None` uses the pooler's default, which is `True` in most cases.
           """
      +
      +    ## for embedding models
           dimensions: int | None = None
           """
           Reduce the dimensions of embeddings if model
      @@ -73,19 +75,6 @@ class PoolerConfig:
           """
       
           ## for classification models
      -    softmax: float | None = None
      -    """
      -    DEPRECATED: please use `use_activation` instead.
      -    """
      -    activation: float | None = None
      -    """
      -    DEPRECATED: please use `use_activation` instead.
      -    """
      -    use_activation: bool | None = None
      -    """
      -    Whether to apply activation function to the classification outputs.
      -    Defaults to True.
      -    """
           logit_bias: float | None = None
           """
           If provided, apply classification logit biases. Defaults to None.
      @@ -105,10 +94,7 @@ class PoolerConfig:
           `math-shepherd-mistral-7b-prm` model.
          ...
      ```
    - modified: vllm/entrypoints/pooling/base/protocol.py (+20/-14)
      ```diff
      @@ -7,16 +7,18 @@
       from pydantic import Field, model_validator
       
       from vllm import PoolingParams
      -from vllm.config.pooler import get_use_activation
       from vllm.entrypoints.chat_utils import (
           ChatCompletionMessageParam,
           ChatTemplateContentFormatOption,
       )
       from vllm.entrypoints.openai.engine.protocol import OpenAIBaseModel
      +from vllm.logger import init_logger
       from vllm.renderers import ChatParams, merge_kwargs
       from vllm.utils import random_uuid
       from vllm.utils.serial_utils import EmbedDType, EncodingFormat, Endianness
       
      +logger = init_logger(__name__)
      +
       
       class PoolingBasicRequestMixin(OpenAIBaseModel):
           # --8<-- [start:pooling-common-params]
      @@ -172,39 +174,43 @@ class EmbedRequestMixin(EncodingRequestMixin):
           # --8<-- [end:embed-params]
       
           # --8<-- [start:embed-extra-params]
      +    use_activation: bool | None = Field(
      +        default=None,
      +        description="Whether to use activation for the pooler outputs. "
      +        "`None` uses the pooler's default, which is `True` in most cases.",
      +    )
           normalize: bool | None = Field(
               default=None,
      -        description="Whether to normalize the embeddings outputs. Default is True.",
      +        descri...
      ```
    - modified: vllm/entrypoints/pooling/pooling/protocol.py (+19/-3)
      ```diff
      @@ -7,7 +7,6 @@
       
       from vllm import PoolingParams
       from vllm.config import ModelConfig
      -from vllm.config.pooler import get_use_activation
       from vllm.entrypoints.openai.engine.protocol import OpenAIBaseModel, UsageInfo
       from vllm.entrypoints.pooling.base.protocol import (
           ChatRequestMixin,
      @@ -17,10 +16,13 @@
           EncodingRequestMixin,
           PoolingBasicRequestMixin,
       )
      +from vllm.logger import init_logger
       from vllm.renderers import TokenizeParams
       from vllm.tasks import PoolingTask
       from vllm.utils import random_uuid
       
      +logger = init_logger(__name__)
      +
       
       class PoolingCompletionRequest(
           PoolingBasicRequestMixin,
      @@ -43,10 +45,17 @@ def build_tok_params(self, model_config: ModelConfig) -> TokenizeParams:
               )
       
           def to_pooling_params(self):
      +        if self.normalize is not None:
      +            logger.warning_once(
      +                "`normalize` is deprecated and will be removed in v0.17. "
      +                "Please pass `use_activation` instead."
      +            )
      +            self.use_activation = self.normalize
      +
               return PoolingParams(
                   truncate_prompt_tokens=self.truncate_prompt_tokens,
      +            use_activation=self.use_activation,
              ...
      ```
    - modified: vllm/entrypoints/pooling/score/protocol.py (+1/-2)
      ```diff
      @@ -7,7 +7,6 @@
       
       from vllm import PoolingParams
       from vllm.config import ModelConfig
      -from vllm.config.pooler import get_use_activation
       from vllm.entrypoints.openai.engine.protocol import OpenAIBaseModel, UsageInfo
       from vllm.entrypoints.pooling.base.protocol import (
           ClassifyRequestMixin,
      @@ -43,7 +42,7 @@ def build_tok_params(self, model_config: ModelConfig) -> TokenizeParams:
           def to_pooling_params(self):
               return PoolingParams(
                   truncate_prompt_tokens=self.truncate_prompt_tokens,
      -            use_activation=get_use_activation(self),
      +            use_activation=self.use_activation,
               )
      ```
    - modified: vllm/model_executor/models/config.py (+2/-2)
      ```diff
      @@ -233,8 +233,8 @@ class Qwen2ForRewardModelConfig(VerifyAndUpdateConfig):
           def verify_and_update_model_config(model_config: "ModelConfig") -> None:
               pooler_config = model_config.pooler_config
       
      -        if pooler_config.softmax is None:
      -            pooler_config.softmax = False
      +        if pooler_config.use_activation is None:
      +            pooler_config.use_activation = False
       
       
       class Qwen3ForSequenceClassificationConfig(VerifyAndUpdateConfig):
      ```
    - modified: vllm/pooling_params.py (+3/-13)
      ```diff
      @@ -7,7 +7,6 @@
       import msgspec
       
       from vllm.config import ModelConfig, PoolerConfig
      -from vllm.config.pooler import get_use_activation
       from vllm.sampling_params import RequestOutputKind
       from vllm.tasks import PoolingTask
       
      @@ -24,30 +23,24 @@ class PoolingParams(
                   Set to -1 to use the model's default truncation size.
                   Set to k to keep only the last k tokens (left truncation).
                   Set to None to disable truncation.
      +        use_activation: Whether to apply activation function to the pooler outputs.
      +            `None` uses the pooler's default, which is `True` in most cases.
               dimensions: Reduce the dimensions of embeddings
                   if model support matryoshka representation.
      -        normalize: Deprecated, please use use_activation instead.
      -        softmax: Deprecated, please use use_activation instead.
      -        activation: Deprecated, please use use_activation instead.
      -        use_activation: Whether to apply activation function to
      -            the classification outputs.
           """
       
           # --8<-- [start:common-pooling-params]
           truncate_prompt_tokens: Annotated[int, msgspec.Meta(ge=-1)] | None = None
      +    use_activation: bool ...
      ```

## Notes
- OpenRouter summarize failed: 400 Client Error: Bad Request for url: https://openrouter.ai/api/v1/chat/completions
