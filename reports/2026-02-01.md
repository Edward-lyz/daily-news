# AI Infra æ—¥æŠ¥ - 2026-02-01

## æ‘˜è¦

ä»Šæ—¥æ— æ–°è®ºæ–‡ã€‚ä¸»è¦æ›´æ–°é›†ä¸­åœ¨ä¸‰ä¸ªæ ¸å¿ƒæ¨ç†æ¡†æ¶ï¼š

- **FlashInfer**: å›é€€äº† Nemotron æ”¯æŒå’Œéé—¨æ§æ¿€æ´»ç±»å‹ï¼Œç®€åŒ– MoE å®ç°ï¼›è·³è¿‡ SM110 (Thor) è®¾å¤‡ä¸Šçš„ trtllm_alltoall æµ‹è¯•
- **SGLang**: Diffusion å¼•æ“æ–°å¢é€šç”¨æ³¨æ„åŠ›åç«¯é…ç½®æ¥å£ï¼›ä¼˜åŒ– VLM DP æ³¨æ„åŠ›ã€RoPE ç´¢å¼•å’Œè‡ªå®šä¹‰ AllReduceï¼›ä¿®å¤å¤šä¸ª VLM/LoRA ç›¸å…³ bug
- **vLLM**: æ–°å¢ Step-3.5-Flash æ¨¡å‹æ”¯æŒï¼ˆå« MTP æ¨æµ‹è§£ç ï¼‰ï¼›ä¿®å¤ prefix cache å‘½ä¸­ç‡ bugï¼›æ–°å¢ unpermute-aware MoE LoRA è·¯å¾„

---

## å…·ä½“å†…å®¹åˆ†æ

### flashinfer-ai/flashinfer

#### å…³é”®æäº¤

**1. Revert "feat: Support Fused MoE non gated Relu2 NVFP4 & FP8 and support Nemotron"** ([#2451](https://github.com/flashinfer-ai/flashinfer/commit/87a45d131dc6518493718e20086cc665ed46da4f))

é‡å¤§å›é€€ï¼Œåˆ é™¤äº† 610 è¡Œä»£ç ï¼Œæ–°å¢ 304 è¡Œã€‚ä¸»è¦å˜æ›´ï¼š

- **API å˜æ›´**: `ActivationType` â†’ `GatedActType`ï¼Œä»…ä¿ç•™ `SwiGlu` å’Œ `GeGlu`
```python
# flashinfer/fused_moe/core.py
class GatedActType(IntEnum):
    SwiGlu = 0
    GeGlu = 1
```

- **ç§»é™¤ Nemotron æ”¯æŒ**: åˆ é™¤ `NumNemotronExperts = 512`ï¼Œ`MaxNumTopExperts` ä» 22 é™è‡³ 8
```cpp
// csrc/trtllm_fused_moe_routing_deepseek.cu
-static constexpr int NumNemotronExperts = 512;
-static constexpr int MaxSupportedTopExperts = 22;
+static constexpr int MaxNumTopExperts = 8;
```

- **DeepSeek è·¯ç”±é™åˆ¶æ”¶ç´§**: topK ä» â‰¤22 æ”¹ä¸º â‰¤8
```cpp
// csrc/trtllm_fused_moe_runner.cu
-FLASHINFER_CHECK(topK <= 22, "For DeepSeek routing method, must have topK <= 22");
+FLASHINFER_CHECK(topK <= 8, "For DeepSeek routing method, must have topK <= 8");
```

**è¯„ä¼°**: ğŸ”´ **é«˜é£é™©** - ç ´åæ€§ API å˜æ›´ï¼Œå½±å“ä¾èµ– Nemotron å’Œéé—¨æ§æ¿€æ´»çš„ç”¨æˆ·ã€‚å»ºè®®å…³æ³¨åç»­æ˜¯å¦æœ‰æ›¿ä»£æ–¹æ¡ˆã€‚

**2. Skip trtllm_alltoall tests on Thor** ([#2448](https://github.com/flashinfer-ai/flashinfer/commit/5d5e164c44e4332a92e2526ba01f5893d5e85353))

```python
# tests/comm/test_trtllm_alltoall.py
pytestmark = pytest.mark.skipif(
    torch.cuda.is_available()
    and get_compute_capability(torch.device("cuda:0"))[0] == 11,
    reason="Tests hang indefinitely on SM110 (Thor) devices",
)
```

**è¯„ä¼°**: ğŸŸ¡ **ä¸­é£é™©** - æš´éœ² SM110 (Thor) æ¶æ„å­˜åœ¨å…¼å®¹æ€§é—®é¢˜ï¼Œæµ‹è¯•è¢«è·³è¿‡è€Œéä¿®å¤ã€‚

---

### sgl-project/sglang

#### å…³é”®æäº¤

**1. [diffusion] cli: introduce generic attention backend configuration** ([#18036](https://github.com/sgl-project/sglang/commit/977096ae03ac4d4148842ad7a9a052c8258bf790))

æ–°å¢ `--attention-backend-config` å‚æ•°ï¼Œæ”¯æŒ JSON/YAML/é”®å€¼å¯¹æ ¼å¼é…ç½®ï¼š

```python
# python/sglang/multimodal_gen/runtime/server_args.py
parser.add_argument(
    "--attention-backend-config",
    type=str,
    help="Configuration for the attention backend. Can be a JSON string, "
         "a path to a JSON/YAML file, or key=value pairs.",
)
```

ä¾èµ–æ›´æ–°ï¼šæ–°å¢ `addict` åº“ã€‚

**è¯„ä¼°**: ğŸŸ¢ **æ­£å‘æ”¹è¿›** - ç»Ÿä¸€äº†æ³¨æ„åŠ›åç«¯é…ç½®æ¥å£ï¼Œæå‡å¯æ‰©å±•æ€§ã€‚

**2. fix: avoid double reduce in VLM dp attention** ([#17991](https://github.com/sgl-project/sglang/commit/d11ccc0a0aa86baa465072e1969b6fc9438cc04a))

ä¿®å¤ VLM æ•°æ®å¹¶è¡Œæ³¨æ„åŠ›ä¸­çš„åŒé‡ reduce é—®é¢˜ï¼š

```python
# python/sglang/srt/layers/attention/vision.py
# ç§»é™¤äº†å¤šä½™çš„ reduce æ“ä½œ
```

**è¯„ä¼°**: ğŸŸ¢ **æ€§èƒ½ä¿®å¤** - é¿å…å†—ä½™é€šä¿¡ï¼Œæå‡ VLM DP æ•ˆç‡ã€‚

**3. Optimize custom-all-reduce** ([#17674](https://github.com/sgl-project/sglang/commit/afebb7ab7893869ec8cf7ed6dd6f06981bc8ccef))

```cpp
// sgl-kernel/csrc/allreduce/custom_all_reduce.cuh
// ä¼˜åŒ–äº†è‡ªå®šä¹‰ allreduce å†…æ ¸å®ç°
```

**è¯„ä¼°**: ğŸŸ¢ **æ€§èƒ½ä¼˜åŒ–** - æ ¸å¿ƒé€šä¿¡åŸè¯­ä¼˜åŒ–ï¼Œå½±å“å…¨æ¡†æ¶æ€§èƒ½ã€‚

**4. [VLM] Optimize get_rope_index for GLM4v** ([#17420](https://github.com/sgl-project/sglang/commit/4ea4f2a20c4b7d6d78220ac5e1c80aa1d288c9fb))

é‡æ„ RoPE ç´¢å¼•è®¡ç®—ï¼Œæ–°å¢ benchmarkï¼š

```python
# python/sglang/srt/layers/rotary_embedding.py
# ä¼˜åŒ– GLM4v çš„ get_rope_index å®ç°
```

**è¯„ä¼°**: ğŸŸ¢ **æ€§èƒ½ä¼˜åŒ–** - é’ˆå¯¹ GLM4v çš„ç‰¹å®šä¼˜åŒ–ã€‚

---

### vllm-project/vllm

#### å…³é”®æäº¤

**1. [Models] Step-3.5-Flash** ([#33523](https://github.com/vllm-project/vllm/commit/c3b40dc3e74dc0f552f76a01ae38b4f1385ad0af))

é‡å¤§æ–°æ¨¡å‹æ”¯æŒï¼Œæ–°å¢ 3107 è¡Œä»£ç ï¼š

- æ–°å¢ `vllm/model_executor/models/step3p5.py` (894 è¡Œ) å’Œ `step3p5_mtp.py` (315 è¡Œ)
- æ–°å¢ `Step3p5ReasoningParser` å’Œ `Step3p5ToolParser`
- æ–°å¢ `SwiGLU2` æ¿€æ´»å‡½æ•°æ”¯æŒ

```python
# vllm/model_executor/layers/activation.py
class SwiGLU2(nn.Module):
    """SwiGLU with squared SiLU activation for Step-3.5-Flash"""
```

**è¯„ä¼°**: ğŸŸ¢ **é‡è¦åŠŸèƒ½** - æ–°æ¨¡å‹æ”¯æŒï¼Œå« MTP æ¨æµ‹è§£ç å’Œå·¥å…·è°ƒç”¨è§£æã€‚

**2. [Fix] prefix cache hit rate == 0 bug with gpt-oss style models** ([#33524](https://github.com/vllm-project/vllm/commit/a01ef3fa51a0312914ca60ecbab2bc45aa43e32c))

ä¿®å¤ gpt-oss é£æ ¼æ¨¡å‹çš„ prefix cache å‘½ä¸­ç‡é—®é¢˜ï¼š

```python
# vllm/v1/core/kv_cache_coordinator.py
# ä¿®å¤äº† prefix cache å‘½ä¸­ç‡è®¡ç®—é€»è¾‘
```

**è¯„ä¼°**: ğŸŸ¢ **å…³é”®ä¿®å¤** - å½±å“ prefix cache åŠŸèƒ½çš„æ­£ç¡®æ€§ã€‚

**3. Add unpermute-aware fused MoE LoRA path** ([#32655](https://github.com/vllm-project/vllm/commit/7320ca3942d008a59c24da3305c81810cb586c9d))

æ–°å¢æ”¯æŒ unpermute æ„ŸçŸ¥çš„ MoE LoRA è·¯å¾„ï¼š

```python
# vllm/lora/ops/triton_ops/fused_moe_lora_op.py
# æ–°å¢ unpermute-aware è·¯å¾„å®ç°
```

**è¯„ä¼°**: ğŸŸ¢ **åŠŸèƒ½å¢å¼º** - æå‡ MoE LoRA çš„å…¼å®¹æ€§å’Œæ€§èƒ½ã€‚

**4. [ModelRunner V2] Support spec decode with structured outputs** ([#33374](https://github.com/vllm-project/vllm/commit/cf0a99f84db5385722e78dc4d7ba76075545a858))

```python
# vllm/v1/worker/gpu/spec_decode/utils.py (æ–°å¢)
# æ”¯æŒæ¨æµ‹è§£ç ä¸ç»“æ„åŒ–è¾“å‡ºç»“åˆ
```

**è¯„ä¼°**: ğŸŸ¢ **åŠŸèƒ½å¢å¼º** - æ‰©å±•æ¨æµ‹è§£ç çš„åº”ç”¨åœºæ™¯ã€‚

---

## Issues æ‘˜è¦

### flashinfer-ai/flashinfer

| Issue | æè¿° | å½±å“ |
|-------|------|------|
| [#2458](https://github.com/flashinfer-ai/flashinfer/issues/2458) | è¯·æ±‚æ”¯æŒ FP4 KV cache in trtllm-gen fmha kernel | åŠŸèƒ½è¯·æ±‚ï¼Œå½“å‰æŠ¥é”™ `Unsupported Kv data type: E2M1` |

### sgl-project/sglang

| Issue | æè¿° | å½±å“ |
|-------|------|------|
| [#18080](https://github.com/sgl-project/sglang/issues/18080) | GLM-4.7-Flash åœ¨ Blackwell RTX PRO 6000 ä¸ŠåŠ è½½å¤±è´¥ | ğŸ”´ Triton å…±äº«å†…å­˜è¶…é™ |
| [#18071](https://github.com/sgl-project/sglang/issues/18071) | SM110 (Thor) ä¸æ”¯æŒ NVFP4 é‡åŒ– | ğŸ”´ Jetson Thor å¹³å°å…¼å®¹æ€§é—®é¢˜ |
| [#18072](https://github.com/sgl-project/sglang/issues/18072) | è¯·æ±‚æ·»åŠ  RISC-V RVV æ³¨æ„åŠ›åç«¯ | åŠŸèƒ½è¯·æ±‚ï¼Œæ‰©å±• CPU æ¨ç†æ”¯æŒ |
| [#18079](https://github.com/sgl-project/sglang/issues/18079) | è¯·æ±‚å®ç° Diffusion Router | æ¶æ„æ”¹è¿›ï¼Œæ”¯æŒ RL rollout |

### vllm-project/vllm

| Issue | æè¿° | å½±å“ |
|-------|------|------|
| [#33526](https://github.com/vllm-project/vllm/issues/33526) | RFC: æ¸è¿›å¼ KV Cache CPU åŠ è½½ | æ¶æ„æ”¹è¿›ï¼Œå‡å°‘ head-of-line blocking |
| [#33519](https://github.com/vllm-project/vllm/issues/33519) | GLM4.7-flash æ— æ³•åŠ è½½ LoRA é€‚é…å™¨ | ğŸ”´ MoE LoRA å…¼å®¹æ€§é—®é¢˜ |
| [#33512](https://github.com/vllm-project/vllm/issues/33512) | Responses API reasoning_tokens å§‹ç»ˆä¸º 0 | ğŸ”´ è®¡è´¹/é¥æµ‹æ•°æ®ä¸å‡†ç¡® |
| [#33497](https://github.com/vllm-project/vllm/issues/33497) | DeepSeek-V3.2 + deepseek_mtp æ¥å—ç‡ä½ | ğŸŸ¡ æ¨æµ‹è§£ç æ•ˆç‡é—®é¢˜ |

---

## æ€»ç»“

### FlashInfer: MoE å®ç°ç®€åŒ–ä¸ API æ”¶ç´§

- **å›é€€ Nemotron æ”¯æŒ**: `ActivationType` â†’ `GatedActType`ï¼Œä»…ä¿ç•™ SwiGlu/GeGluï¼ŒtopK é™åˆ¶ä» 22 é™è‡³ 8 ([#2451](https://github.com/flashinfer-ai/flashinfer/commit/87a45d131dc6518493718e20086cc665ed46da4f))
- **SM110 å…¼å®¹æ€§é—®é¢˜**: Thor è®¾å¤‡ä¸Š trtllm_alltoall æµ‹è¯•æŒ‚èµ·ï¼Œè¢«è·³è¿‡è€Œéä¿®å¤ ([#2448](https://github.com/flashinfer-ai/flashinfer/commit/5d5e164c44e4332a92e2526ba01f5893d5e85353))

### SGLang: Diffusion å¼•æ“æˆç†ŸåŒ– + VLM ä¼˜åŒ–

- **é€šç”¨æ³¨æ„åŠ›åç«¯é…ç½®**: æ–°å¢ `--attention-backend-config` CLI å‚æ•°ï¼Œæ”¯æŒ JSON/YAML/k=v æ ¼å¼ ([#18036](https://github.com/sgl-project/sglang/commit/977096ae03ac4d4148842ad7a9a052c8258bf790))
- **VLM DP ä¿®å¤**: é¿å…åŒé‡ reduceï¼Œä¼˜åŒ– GLM4v RoPE ç´¢å¼• ([#17991](https://github.com/sgl-project/sglang/commit/d11ccc0a0aa86baa465072e1969b6fc9438cc04a), [#17420](https://github.com/sgl-project/sglang/commit/4ea4f2a20c4b7d6d78220ac5e1c80aa1d288c9fb))
- **è‡ªå®šä¹‰ AllReduce ä¼˜åŒ–**: æ ¸å¿ƒé€šä¿¡åŸè¯­æ€§èƒ½æå‡ ([#17674](https://github.com/sgl-project/sglang/commit/afebb7ab7893869ec8cf7ed6dd6f06981bc8ccef))

### vLLM: æ–°æ¨¡å‹æ”¯æŒ + å…³é”® Bug ä¿®å¤

- **Step-3.5-Flash**: å®Œæ•´æ¨¡å‹æ”¯æŒï¼Œå« MTP æ¨æµ‹è§£ç ã€SwiGLU2 æ¿€æ´»ã€å·¥å…·è°ƒç”¨è§£æ ([#33523](https://github.com/vllm-project/vllm/commit/c3b40dc3e74dc0f552f76a01ae38b4f1385ad0af))
- **Prefix Cache ä¿®å¤**: gpt-oss é£æ ¼æ¨¡å‹å‘½ä¸­ç‡ä» 0 æ¢å¤æ­£å¸¸ ([#33524](https://github.com/vllm-project/vllm/commit/a01ef3fa51a0312914ca60ecbab2bc45aa43e32c))
- **MoE LoRA å¢å¼º**: æ–°å¢ unpermute-aware è·¯å¾„ ([#32655](https://github.com/vllm-project/vllm/commit/7320ca3942d008a59c24da3305c81810cb586c9d))
- **ModelRunner V2 ä¼˜åŒ–**: æ”¯æŒæ¨æµ‹è§£ç  + ç»“æ„åŒ–è¾“å‡ºï¼Œä»£ç ç®€åŒ– ([#33374](https://github.com/vllm-project/vllm/commit/cf0a99f84db5385722e78dc4d7ba76075545a858), [#33467](https://github.com/vllm-project/vllm/commit/e535d90debb39e8462cd39004fb7e0d9ded10740))
