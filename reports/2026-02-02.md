# 今日 AI Infra 新闻

## 摘要

本周 AI 基础设施领域重点关注大模型推理优化、多模态模型服务、神经形态计算以及高效训练方法。vLLM-Omni 推出了全解耦的多模态模型服务系统，FlashInfer 添加了对 Mamba 模型的支持，NVIDIA Cutlass 修复了 Blackwell 架构下的 GEMM 问题。同时，多个研究团队提出了创新的推理优化方法和高效训练框架。

## 具体内容分析

### 重要论文

1. **ReasonCACHE: Teaching LLMs To Reason Without Weight Updates**  
   提出使用 Prefix Tuning 让 LLM 无需权重更新即可学习推理能力，通过将演示蒸馏为固定的键值缓存，在保持推理效率的同时提升了复杂推理任务的性能。

2. **vLLM-Omni: Fully Disaggregated Serving for Any-to-Any Multimodal Models**  
   针对任何到任何多模态模型推出全解耦服务系统，通过阶段抽象和图执行优化，将任务完成时间减少高达 91.4%。

3. **Energy-Efficient Neuromorphic Computing for Edge AI: A Framework with Adaptive Spiking Neural Networks**  
   提出 NeuEdge 框架，结合自适应 SNN 模型和硬件感知优化，在边缘设备上实现 91-96% 的准确率和 847 GOp/s/W 的能效。

4. **Backpropagation as Physical Relaxation: Exact Gradients in Finite Time**  
   证明反向传播是物理动力系统在有限时间内的精确松弛，为模拟和神经形态 substrate 中的梯度计算提供了理论基础。

### 关键代码库更新

#### [vllm-project/vllm](https://github.com/vllm-project/vllm)
- **新增 Step-3.5-Flash 模型支持** ([PR #33523](https://github.com/vllm-project/vllm/pull/33523))
- **修复推理内容重命名导致的回归** ([Issue #33616](https://github.com/vllm-project/vllm/issues/33616))
- **优化 DeepSeekR1 在 Blackwell 上的吞吐量** ([Issue #33583](https://github.com/vllm-project/vllm/issues/33583))
- **添加 DeepSeek-OCR-2 支持** ([PR #33165](https://github.com/vllm-project/vllm/pull/33165))

#### [flashinfer-ai/flashinfer](https://github.com/flashinfer-ai/flashinfer)
- **添加 Mamba 模型支持** ([PR #2444](https://github.com/flashinfer-ai/flashinfer/pull/2444))
- **新增 SM103 特定调度器** ([PR #2303](https://github.com/flashinfer-ai/flashinfer/pull/2303))
- **修复 FP8 MoE 路由 logits 类型问题** ([Issue #2469](https://github.com/flashinfer-ai/flashinfer/issues/2469))
- **添加 SM90 fence PTX 保护** ([PR #2439](https://github.com/flashinfer-ai/flashinfer/pull/2439))

#### [sgl-project/sglang](https://github.com/sgl-project/sglang)
- **支持 Mooncake 节点内 NVLink KV 传输** ([PR #17866](https://github.com/sgl-project/sglang/pull/17866))
- **添加 Step-3.5-Flash 模型支持** ([PR #18084](https://github.com/sgl-project/sglang/pull/18084))
- **修复 HiCache DeepSeek v32 CPU 卸载** ([PR #17415](https://github.com/sgl-project/sglang/pull/17415))
- **优化基数缓存驱逐性能** ([PR #14339](https://github.com/sgl-project/sglang/pull/14339))

#### [NVIDIA/cutlass](https://github.com/NVIDIA/cutlass)
- **修复 SM100 block-scale GEMM 重叠累加器问题** ([PR #2995](https://github.com/NVIDIA/cutlass/pull/2995))
- **添加 Blackwell 架构支持** ([Issue #2994](https://github.com/NVIDIA/cutlass/issues/2994))

### 值得关注的问题

1. **vLLM 依赖兼容性问题** ([Issue #33599](https://github.com/vllm-project/vllm/issues/33599))  
   vLLM 严格的依赖约束与下游生态系统（如 Ray）存在兼容性冲突，需要改进依赖管理策略。

2. **Flash Attention Blackwell 架构支持** ([Issue #2220](https://github.com/Dao-AILab/flash-attention/issues/2220))  
   用户在 NVIDIA AGX Thor（Blackwell 架构）上运行时遇到 `cudaErrorNoKernelImageForDevice` 错误。

3. **SGLang KV 缓存卸载问题** ([Issue #18135](https://github.com/sgl-project/sglang/issues/18135))  
   GLM 4.6 FP8 模型在使用 Flash Infer 后端时出现 KV 缓存卸载问题。

4. **推理内容字段命名回归** ([Issue #33616](https://github.com/vllm-project/vllm/issues/33616))  
   `reasoning_content` 到 `reasoning` 的重命名导致依赖 `reasoning_content` 的聊天模板出现兼容性问题。

## 总结

AI 基础设施本周在多模态服务、边缘计算和高效训练方面取得重要进展。vLLM-Omni 的全解耦架构为复杂多模态模型提供了高效服务方案，FlashInfer 对 Mamba 和 SM103 的扩展提升了特定场景的性能，而 NeuEdge 则为边缘 AI 计算提供了新的思路。同时，依赖兼容性和架构支持等挑战仍需社区共同解决。
