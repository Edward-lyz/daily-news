今日的 AI Infra 的新闻如下。

## 摘要
本期更新聚焦于多项底层性能优化与功能扩展。DeepGEMM 在 SM100 MQA 相关实现中加入跨 warp‑group 同步点，以提升并行计算的稳定性；CUTLASS 4.4 版发布，强化 CuTe DSL 对 CUDA 13.1 的支持并引入 Blackwell 示例，进一步提升 GEMM 调度与内存同步的可靠性。FlashInfer 通过在 Hopper GPU 上引入 delta‑rule 加速 GDN prefill，并在多模态 MoE、Top‑K 与 HiCache 等模块实现了显著的性能提升与新特性。SGLang 侧重于多模态会话管理、MTGPU 分布式支持以及工具链的 CI/CD 与文档完善，提升了跨平台部署的可用性。VLLM 则在响应 API、调度器改进、FP8 MoE 兼容性以及多模型并行等方面进行大量代码重构，显著扩展了模型兼容性与推理效率。cuTile‑Python 增加了自定义 mask 支持的 gather/scatter 功能，提升了张量操作的灵活性。

## 具体内容分析
### deepseek-ai/DeepGEMM
提交  
- **PR #285** – Fix a sync issue in SM100 MQA logits  
  <https://github.com/deepseek-ai/DeepGEMM/pull/285>  

  **修改文件**  

  1. `deep_gemm/include/deep_gemm/impls/sm100_fp8_mqa_logits.cuh`  
     - 在 `empty_umma_barriers[i]->wait(((num_total_kv_blocks + kv_block_idx) & 1) ^ 1);` 之后插入 `tcgen05_after_thread_sync();`，用于在跨 warp‑group 同步后显式触发线程同步。  
     - 同样在 `full_umma_barriers[warpgroup_idx]->wait((num_total_kv_blocks + kv_block_idx) & 1);` 之后加入同样的同步调用。  
     ```cpp
     empty_umma_barriers[i]->wait(((num_total_kv_blocks + kv_block_idx) & 1) ^ 1);
     tcgen05_after_thread_sync();   // 新增同步点

     // ...

     full_umma_barriers[warpgroup_idx]->wait((num_total_kv_blocks + kv_block_idx) & 1);
     tcgen05_after_thread_sync();   // 新增同步点
     ```
  
  2. `deep_gemm/include/deep_gemm/impls/sm100_fp8_paged_mqa_logits.cuh`  
     - 在 `if (q_idx != next_q_idx)` 条件块内部加入 `full_q_barriers[q_stage_idx]->wait(q_phase);`，确保在切换 Q 阶段时完成完整的 barrier 同步。  
     - 在 `empty_umma_barriers[i]->wait(umma_phase);` 之后同样插入 `tcgen05_after_thread_sync();`。  
     ```cpp
     if (q_idx != next_q_idx) {
         CUTE_TIE(get_q_pipeline(q_iter_idx ++), q_stage_idx, q_phase);
         full_q_barriers[q_stage_idx]->wait(q_phase);   // 新增同步点
     }

     // ...

     empty_umma_barriers[i]->wait(umma_phase);
     tcgen05_after_thread_sync();   // 新增同步点
     ```

  **说明**：`patch_truncated` 为 true，以上仅展示了关键改动片段，完整 diff 未提供。

Issues  
- 暂无
### deepseek-ai/FlashMLA
昨日无更新。
### NVIDIA/cutlass
提交  

### Issues  

| 编号 | 标题 | 作者 | 创建时间 | 简要说明 |
|------|------|------|----------|----------|
| [#2997](https://github.com/NVIDIA/cutlass/issues/2997) | warning #2908-D: the implicit by‑copy capture of “this” is deprecated | Algy | 2026‑02‑03 04:02:22 UTC | 在使用 C++20 编译 CUTLASS 时，`sm100_blockscaled_mma_array_warpspecialized_rcggemm.hpp` 中出现 lambda 隐式复制捕获 `this` 的警告。 |
| [#2996](https://github.com/NVIDIA/cutlass/issues/2996) | How to optimize cutlass int8 fprop2d kernel with NCxHWx interleaved layout, which is ~12% slower than TensorRT on Jetson Orin | JJXiangJiaoJun | 2026‑02‑03 02:55:31 UTC | 在 Jetson AGX Orin 上，CUTLASS 的 INT8 Conv2d Fprop（NCxHWx<32>）比 TensorRT 慢约 12%。需要调优建议。 |

> **说明**：#2997 的 issue 内容在提供的 JSON 中被截断，未能完整展示全部细节。  

---

### Commits  

| SHA | PR 链接 | 简要描述 | 影响的模块/文件（示例代码） |
|-----|---------|----------|----------------------------|
| `6b3e607` | [#2999](https://github.com/NVIDIA/cutlass/pull/2999) | CUTLASS 4.4 发布更新，重点增强 CuTe DSL（支持 CUDA 13.1、GB300、实验性 `cute.experimental` 层、AoT 编译等），并加入大量 Blackwell 示例。 | - **CHANGELOG.md** – 新增 “CuTe DSL 现在支持 CUDA toolkit 13.1” 等条目。<br>`+  - CuTe DSL now supports CUDA toolkit 13.1!`<br>- **README.md** – 同步 “What’s New in CUTLASS 4.4” 内容。<br>`+ ## CuTe DSL`<br>- **examples/python/CuTeDSL/blackwell/sm103_dense_blockscaled_gemm_persistent.py** – 新增 3000 行示例，演示 SM103 块缩放 GEMM 持久化调度。<br>- **examples/python/CuTeDSL/experimental/blackwell/dense_gemm.py** – 新增 1410 行实验性 Dense GEMM 示例。<br>- **include/cutlass/gemm/kernel/sm100_gemm_array_tma_warpspecialized.hpp** – 在调度前加入 `cutlass::arch::wait_on_dependent_grids();` 以防止未刷新全局内存。<br>`+ cutlass::arch::wait_on_dependent_grids();`<br>- **include/cutlass/gemm/kernel/sm100_tile_scheduler_group.hpp** – 添加注释说明调度器构造可能触及前置 kernel 写入的全局内存。<br>`+ // Note: constructing this tile scheduler can touch global memory …` |
| `1cfbb53` | [#2995](https://github.com/NVIDIA/cutlass/pull/2995) | 修复 Blackwell SM100 块缩放 GEMM 中累加器重叠导致的错误。 | - **examples/python/CuTeDSL/blackwell/dense_blockscaled_gemm_persistent.py** – 调整线程计数与屏障配置，加入 `ceil_div` 辅助函数。<br>`self.threads_per_warp = 32`<br>`self.threads_per_cta = self.threads_per_warp * len(...)`<br>- **examples/python/CuTeDSL/blackwell/dense_blockscaled_gemm_persistent_prefetch.py** – 同步更新，加入相同的 `ceil_div` 实现。 |

> **说明**：在提交 `6b3e607` 中，多个文件的 diff 被标记为 `patch_truncated: true`（如 `CHANGELOG.md`、`README.md`、若干示例文件），因此这里只展示了关键片段，完整改动请参考对应 PR。  

---  

*以上信息基于 2026‑02‑03 的仓库快照整理，供内部技术团队快速了解近期 CUTLASS 代码库的变动情况。*
### flashinfer-ai/flashinfer
## 提交

| SHA | 作者 | 日期 | 关键改动 | 影响文件 |
|-----|------|------|----------|----------|
| [f84ac1c](https://github.com/flashinfer-ai/flashinfer/commit/f84ac1c97e2e1a4391150ca73971b973ee569e5a) | Brian K. Ryu | 2026‑02‑04 | 修正 MLA 基准中的内存带宽计算公式。 | `benchmarks/bench_trtllm_gen_mla.py`、`benchmarks/routines/attention.py` |
| [6ae5bfe](https://github.com/flashinfer-ai/flashinfer/commit/6ae5bfe6a48c591a171f32888e5de28b9ca0207b) | Zihao Ye | 2026‑02‑04 | 为 Hopper GPU 的 GDN prefill 引入 delta‑rule 编译加速，并整理文档字符串。 | `csrc/flat_prefill_kernel_delta_rule_sm90_extern.inc`、`csrc/gdn_prefill_launcher.cu`、`csrc/gdn_prefill_sm90_kernel_inst.jinja`、`flashinfer/gdn_prefill.py`、`flashinfer/jit/core.py`、`flashinfer/jit/gdn.py` 等（文件大量重命名） |
| [0eb69bb](https://github.com/flashinfer-ai/flashinfer/commit/0eb69bb00e5561b076ce288f6dfdf20f5d6783b5) | Zack Yu | 2026‑02‑03 | 自动调优器在 OOM 场景下的防护逻辑升级，加入内存上限检查与回退策略。 | `flashinfer/autotuner.py` |
| [9e069e7](https://github.com/flashinfer-ai/flashinfer/commit/9e069e76f63e287ccc8894e1edd67f011869dc52) | Julien Debache | 2026‑02‑03 | 让 block‑scale MoE 基准支持非‑DS（Data‑Parallel）路由路径。 | `benchmarks/routines/moe.py` |
| [fb671fd](https://github.com/flashinfer-ai/flashinfer/commit/fb671fdb79d53aaf8a1d3baca1c9aa35d080d510) | Yong Wu | 2026‑02‑03 | CI/CD 迁移至内部 ci‑infra runner，统一 Nightly 与 Release 工作流。 | `.github/workflows/nightly-release.yml`、`.github/workflows/release-ci-docker.yml`、`.github/workflows/release.yml` |
| [5e5a866](https://github.com/flashinfer-ai/flashinfer/commit/5e5a8668bede32d86035dba98f460a278332e366) | Zihao Ye | 2026‑02‑03 | 大幅提升 GDN decode 的 cute‑dsl 内核性能，重写 kernel 并优化基准脚本。<br>**代码片段（示例）**：<br>```cpp\n// 关键改动：使用更紧凑的 TMA 布局\nusing TileShape = cute::Shape<int32_t, int32_t, int32_t>;\n``` | `benchmarks/bench_gdn_decode.py`、`flashinfer/gdn_decode.py` |
| [2d7a987](https://github.com/flashinfer-ai/flashinfer/commit/2d7a987c9ca4251d0a3e4c1249614f2474353884) | jhalabi‑nv | 2026‑02‑03 | 在 `moeAlltoAllKernels.cu` 中加入 SM90 guard，防止在不支持的架构上编译。 | `csrc/nv_internal/tensorrt_llm/kernels/communicationKernels/moeAlltoAllKernels.cu` |
| [b7404d0](https://github.com/flashinfer-ai/flashinfer/commit/b7404d064d27f0c1805dc7ecfc8f7fbd5db1b053) | Igor Shovkun | 2026‑02‑03 | 为 Mamba 引入 MTP（Multi‑Threaded Pipeline）实现，新增大量 CUDA 核心文件与测试。<br>**代码片段（示例）**：<br>```cpp\n// selective_state_update.cuh 中的核心循环\nfor (int i = 0; i < seq_len; ++i) {\n    state[i] = update(state[i], input[i]);\n}\n``` | `csrc/flashinfer_mamba_binding.cu`、`csrc/selective_state_update.cu`、`flashinfer/mamba/selective_state_update.py`、`include/flashinfer/mamba/*.cuh`、`tests/mamba/*` |
| [273c09c](https://github.com/flashinfer-ai/flashinfer/commit/273c09cd03cde5f84f561807d7178ad6b8de7a25) | FlashInfer Bot | 2026‑02‑03 | 更新 CI Docker 镜像标签至 `20260203-9b5901e`，保持镜像可追溯性。 | `ci/docker-tags.yml` |
| [95d8511](https://github.com/flashinfer-ai/flashinfer/commit/95d851180671ebc178eb18bea8fc437fdbeff313) | Julian Huang | 2026‑02‑03 | 在 Top‑K 基准中加入 `sgl_kernel.fast_topk_v2` 实现，提升大批量 Top‑K 效率。 | `benchmarks/bench_topk.py` |
| [9b5901e](https://github.com/flashinfer-ai/flashinfer/commit/9b5901eb007633ebbb38fbe6ee0a014caca88e0a) | Brian K. Ryu | 2026‑02‑03 | Dockerfile 中设置 `LD_LIBRARY_PATH`，确保 cuBLAS 正确加载。 | `docker/Dockerfile.cu126`、`docker/Dockerfile.cu128`、`docker/Dockerfile.cu129`、`docker/Dockerfile.cu130` |
| [c7761ad](https://github.com/flashinfer-ai/flashinfer/commit/c7761add6e6b7703ce5f12b03be387e78048f36b) | Roberto L. Castro | 2026‑02‑03 | 为 SM103 GPU 添加专用的 NVFP4 CUTLASS 调度器，实现 FP4 GEMM 的硬件加速。<br>**代码片段（示例）**：<br>```cpp\n// fp4_gemm_cutlass_sm103.cu 中的调度器注册\nCUTLASS_GEMM_REGISTER(SM103, fp4, cutlass::arch::Sm103);\n``` | `csrc/fp4_gemm_cutlass_sm103.cu`、`include/flashinfer/gemm/fp4_gemm_template_sm103.h`、`flashinfer/gemm/gemm_base.py`、`flashinfer/jit/gemm/*` |

> **说明**：部分提交的 `patch_truncated` 为 `true`，完整 diff 未在本次摘要中展示。如需查看完整改动，请访问对应的 commit 链接。

## Issues

| 编号 | 标题 | 提出者 | 创建时间 | 简要需求 |
|------|------|--------|----------|----------|
| #2483 | **[Feature Request] Skip‑Softmax (BLASST) Optimization in XQA** | jimmyzho | 2026‑02‑03 22:08:34Z | TensorRT‑LLM 已实现 Skip‑Softmax Attention（参考论文 https://www.arxiv.org/pdf/2512.12087），希望在 FlashInfer 中加入对应后端（fmha_v2 / xqa），并在 API 层提供开关，以提升 Hopper 解码阶段的吞吐。 |
| #2480 | **[DSR1] DSR1 router gemm performance issue when upstream to SGL** | nv‑yunzheq | 2026‑02‑03 19:18:47Z | SGL PR‑17707 中的路由 GEMM 在性能上不及 SGL 自研 TRT‑LLM 复制内核，需要定位瓶颈并进行优化，确保两者功能等价后合并。 |
| #2474 | **would it be support Ascend/NPU** | L4‑1024 | 2026‑02‑03 07:15:54Z | 询问 FlashInfer 是否计划在未来支持华为 Ascend / 通用 NPU 加速器。 |
| #2473 | **SageAttention support** | YangXu1990uiuc | 2026‑02‑03 06:43:00Z | 需求在 SageAttention 2（配合 FP8 PV）上实现：<br>1. Q·K 使用 int8 IMMA（仅限 gb200），输出 int32；<br>2. 对 Q、K 进行 16‑token 级别的缩放（点乘后实现）；<br>3. Softmax 使用 fp32；<br>4. PV（V）保持 fp8；<br>5. 在 epilogue 对 V 进行缩放；<br>6. 对 gb200 与 gb300 两种 GPU 进行不同的实现路径。 |

---  
*以上信息仅供内部技术团队快速了解本次代码库变动与待处理需求。*
### Dao-AILab/flash-attention
**提交**  
- 暂无提交记录（本日未检测到代码变动）。

**Issues**  

1. **#2226** – *what's the full name of SPT?*  
   - 作者：endurehero，创建于 2026‑02‑03 23:43:43 UTC。  
   - 内容概述：在 `SingleTileLPTBwdScheduler` 中的 `spt` 参数用于在因果且确定性场景下逆序遍历 n tiles，以提升性能，询问其全称。  
   - 链接：<https://github.com/Dao-AILab/flash-attention/issues/2226>  
   - 关联 PR：暂无。

2. **#2225** – *run code error after pip install cuda 12.6 flash‑attn 2.8.2*  
   - 作者：cqray1990，创建于 2026‑02‑03 06:58:49 UTC。  
   - 内容概述：在 Ubuntu 20 上使用 `pip install flash_attn-2.8.2+cu128torch2.8-cp312-cp312-linux_x86_64.whl` 后，导入时报错缺少 `GLIBC_2.32`。  
   - 链接：<https://github.com/Dao-AILab/flash-attention/issues/2225>  
   - 关联 PR：暂无。
### sgl-project/sglang
**提交**  

- `c1d529c` – Fix Session for multimodal and expose it through Engine ([PR](https://github.com/sgl-project/sglang/commit/c1d529c19605cbf1f9be8db6d6d225b1465ea2e0))  
  - 关键文件：`python/sglang/srt/entrypoints/engine.py`（新增 37 行）  
  - 关键文件：`python/sglang/srt/managers/schedule_batch.py`（删除 1 行）  
  - 关键文件：`python/sgl​ang/srt/managers/scheduler.py`（新增 17 行）  
  - 说明：实现多模态会话的修复并在 Engine 中对外暴露，具体实现细节在 `engine.py` 中加入了会话管理逻辑（patch 已截断，无法展示完整代码）。

- `1f72f66` – 修正 README 中的拼写错误 ([PR](https://github.com/sgl-project/sglang/commit/1f72f66c6d6ef0eff589230b9ca06cf4e44ecdd2))  
  - 文件：`README.md`（改动 1 行）  

- `da758ed` – 修复连续动态请求下的 server cache‑dit bug（[#17140]）([PR](https://github.com/sgl-project/sglang/commit/da758ed601270b21e1cfb404306ff0ca5c816a3f))  
  - `python/sglang/multimodal_gen/runtime/cache/cache_dit_integration.py`（新增 67 行，删除 1 行）  
  - `python/sglang/multimodal_gen/runtime/pipelines_core/stages/denoising.py`（新增 71 行，删除 40 行）  

- `ae004e1` – 确保 nightly wheel 打标签使用最新 commit（[#18204]）([PR](https://github.com/sgl-project/sglang/commit/ae004e15c98b36dda9c460d4d0ee9891889a5adf))  
  - 文件：`.github/workflows/release-pypi-nightly.yml`（改动 1 行）  

- `793bf9f` – 更新 Qwen3 Embeddings 的权重重命名检查（[#17535]）([PR](https://github.com/sgl-project/sglang/commit/793bf9fc06499cb1ba236444a7a3dde0ea5b7e49))  
  - 文件：`python/sglang/srt/models/qwen3.py`（新增 5 行，删除 1 行）  

- `e867040` – 新增流式并行工具调用测试用例（[#18097]）([PR](https://github.com/sgl-project/sglang/commit/e867040fc6624445ffa4b567c157b598d9aed2c8))  
  - 文件：`python/sglang/test/tool_call_test_runner.py`（新增 32 行）  

- `7de650c` – 支持 MTGPU 上的 diffusion 模型（文档第 6/7 部分）[#17346]([PR](https://github.com/sgl-project/sglang/commit/7de650c83c4d63b9107184bd5cf36303d89e8d28))  
  - `python/sglang/multimodal_gen/README.md`（新增 7 行，删除 2 行）  
  - `python/sglang/multimodal_gen/docs/install.md`（新增 3 行，删除 1 行）  
  - `python/sglang/multimodal_gen/docs/install_musa.md`（新增 24 行）  

- `ec2461b` – 支持 MTGPU 多 GPU（第 5/7 部分）[#17318]([PR](https://github.com/sgl-project/sglang/commit/ec2461bc16e59d8a5738340fb559ccf396cd9af7))  
  - `python/sglang/multimodal_gen/runtime/distributed/device_communicators/pynccl_wrapper.py`（改动 1 行）  
  - `python/sglang/multimodal_gen/runtime/models/encoders/clip.py`（新增 3 行，删除 1 行）  
  - `python/sglang/multimodal_gen/utils.py`（新增 5 行，删除 3 行）  

- `acf724b` – 仅在自定义 CUDA 路径中导入 `sgl_kernel`（SiluAndMul、RMSNorm）[#15592]([PR](https://github.com/sgl-project/sglang/commit/acf724b036c595292b036942db69fb169efffc45))  
  - `python/sglang/multimodal_gen/runtime/layers/activation.py`（新增 6 行，删除 1 行）  
  - `python/sglang/multimodal_gen/runtime/layers/layernorm.py`（新增 6 行，删除 1 行）  

- `e166ca8` – HiCache 增加详细缓存命中拆解及 Prometheus 指标（[#17648]）([PR](https://github.com/sgl-project/sglang/commit/e166ca87584368699ac35ccb5e518211631849cf))  
  - 关键文件包括 `protocol.py`、`serving_chat.py`、`serving_completions.py`、`usage_processor.py`、`utils.py`、`detokenizer_manager.py`、`io_struct.py`、`multi_tokenizer_mixin.py`、`schedule_batch.py`、`scheduler.py`、`scheduler_output_processor_mixin.py`、`tokenizer_manager.py`、`hiradix_cache.py`、`collector.py`（累计改动 407 行，新增 333 行）  

- `d48bbe3` – NPU CI 中修复 `sgl‑kernel` 导入错误（[#18173]）([PR](https://github.com/sgl-project/sglang/commit/d48bbe3beda74106e27ab4e83eebbc235a5fbd59))  
  - 文件：`python/sglang/srt/layers/moe/moe_runner/flashinfer_trtllm.py`（新增 5 行，删除 1 行）  

- `495290a` – 为 XPU 设备启用单元测试（[#11712]）([PR](https://github.com/sgl-project/sglang/commit/495290aefd1f5e8f7872a218473ac0b4c7dcc2f6))  
  - 影响多个测试文件和手动测试脚本，累计改动 388 行（新增 237 行，删除 151 行），部分文件已截断（patch_truncated 为 true）。  

- `0a69256` – 改进 cu13 构建的 Docker 镜像（[#18194]）([PR](https://github.com/sgl-project/sglang/commit/0a6925639b3f2d7b70503f06170eb490c09dac4c))  
  - `.github/workflows/release-docker-cu13-framework.yml`（新增 6 行）  
  - `docker/Dockerfile`（新增 1 行，删除 3 行）  

- `0db6fd4` – 恢复被误删的 sgl_kernel 排除规则（[#18193]）([PR](https://github.com/sgl-project/sglang/commit/0db6fd4dbec5e9eaac6ee35dfba257720ae65ea5))  
  - 四个 CI 工作流文件均删除了多余的排除规则（累计删除 47 行）  

- `820df54` – 为 cu13 添加开发容器并更新发布工作流（[#18192]）([PR](https://github.com/sgl-project/sglang/commit/820df545f2c48ea92911e2078389cc1a447bde3f))  
  - 新增 `.github/workflows/release-docker-cu13-framework.yml`（新增 157 行）  
  - `release-docker.yml`（新增 52 行，删除 2 行）  

- `99fab2c` – 修复 Mistral Large 3 NVFP4 TRTLLM MoE（[#18065]）([PR](https://github.com/sgl-project/sglang/commit/99fab2ce673eeae87736cee44844777a3c9ae304))  
  - `python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors_moe.py`（新增 94 行，删除 103 行）  
  - `test/registered/8-gpu-models/test_mistral_large3.py`（新增 21 行，删除 8 行）  

- `a45647b` – 支持 Mooncake intra‑node NVLink KV 传输（[#17866]）([PR](https://github.com/sgl-project/sglang/commit/a45647bce16ddf159713a65c3b281c7e66700bcc))  
  - `python/sglang/srt/disaggregation/mooncake/utils.py`（新增 3 行，删除 1 行）  
  - `python/sglang/srt/disaggregation/utils.py`（新增 3 行）  

- `cc69ac9` – 动态 chunk 大小的预热以提升 prefill 延迟测量（[#17198]）([PR](https://github.com/sgl-project/sglang/commit/cc69ac9e7a7d94fac0b94b8d4d19bf62af69df15))  
  - 文件：`python/sglang/srt/managers/scheduler_pp_mixin.py`（新增 3 行，删除 2 行）  

- `8e933e1` – AMD PD/D PR CI（[#17183]）([PR](https://github.com/sgl-project/sglang/commit/8e933e1914b6b335e58bb1442b3931e2303192bb))  
  - 关键新增文件：`scripts/ci/amd/amd_ci_start_container_disagg.sh`（新增 244 行）  
  - 新增 AMD 相关的 disaggregation 测试 `test/registered/amd/disaggregation/*.py`（累计新增约 713 行）  

- `25508d1` – Dockerfile 移除硬编码时区，默认 UTC（[#18121]）([PR](https://github.com/sgl-project/sglang/commit/25508d11c03cb1344a1cffc8e4e9e517181388f0))  
  - `docker/Dockerfile`（删除 5 行，新增 1 行）  
  - `docker/gateway.Dockerfile`（删除 3 行，新增 1 行）  

- `6f6b9c6` – 多线程加载器使用 safetensors `load_file`（[#18124]）([PR](https://github.com/sgl-project/sglang/commit/6f6b9c6e42a927f087276536f225147eb8507714))  
  - 文件：`python/sglang/srt/model_loader/weight_utils.py`（删除 6 行，新增 2 行）  

- `7a9d9c7` – HiCache 在 Mooncake `batch_exists` 中应用 `extra_backend_tag`（[#17265]）([PR](https://github.com/sgl-project/sglang/commit/7a9d9c79d15dfa7c2c1ff9685ea7fba3d5c599ad))  
  - 文件：`python/sglang/srt/mem_cache/storage/mooncake_store/mooncake_store.py`（新增 5 行）  

- `74f716d` – Gigachat 3 工具解析器及测试（[#14765]）([PR](https://github.com/sgl-project/sglang/commit/74f716dbd711bef3bd7bc9d6eab7ebc675e47e65))  
  - `docs/advanced_features/server_arguments.md`（改动 1 行）  
  - `python/sglang/srt/function_call/function_call_parser.py`（新增 2 行）  
  - 新增 `python/sglang/srt/function_call/gigachat3_detector.py`（新增 209 行）  
  - 新增测试 `test/registered/function_call/test_function_call_parser.py`（新增 506 行）  

- `4181290` – 为 `run_eval.py` 添加 `--top‑k` 参数（[#18025]）([PR](https://github.com/sgl-project/sglang/commit/4181290efd668082109055c53261a5c956817caa))  
  - 文件：`python/sglang/test/run_eval.py`（新增 10 行，删除 1 行）  

- `fe57a88` – 使用单元测试覆盖 LoRA 重叠加载（[#18140]）([PR](https://github.com/sgl-project/sglang/commit/fe57a887b110a52670fcaa5c616cd0a8dce12615))  
  - 文件：`test/registered/lora/test_lora_overlap_loading.py`（新增 127 行，删除 9 行）  

- `f032c4f` – 支持 Markdown/Notebook‑友好的文档导出（[#18131]）([PR](https://github.com/sgl-project/sglang/commit/f032c4f3d66605edd3177ac7d242c3d9704888bf))  
  - `.github/workflows/release-docs.yml`（新增 1 行）  
  - `docs/Makefile`（新增 20 行）  
  - `docs/README.md`（新增 68 行，删除 1 行）  

- `78bf13d` – MoE 重构：`modelopt_quant.py` → `flashinfer_trtllm.py`（[#16685]）([PR](https://github.com/sgl-project/sglang/commit/78bf13db4447b98eb9d8169c400448d1dcad12a3))  
  - `python/sglang/srt/layers/moe/moe_runner/flashinfer_trtllm.py`（新增 277 行，删除 15 行）  
  - `python/sglang/srt/layers/quantization/fp8.py`（新增 1 行）  
  - `python/sglang/srt/layers/quantization/modelopt_quant.py`（新增 64 行，删除 176 行）  

- `eedd472` – 修复 diffusion `image_edit` 输入图像的 bug（[#18109]）([PR](https://github.com/sgl-project/sglang/commit/eedd472025c4f3d0247c5ef4f05dce54663a09c9))  
  - 文件：`python/sglang/multimodal_gen/runtime/pipelines/diffusers_pipeline.py`（新增 3 行）  

- `e484c90` – 为 GLM‑4.7‑FP8 tp8 H20/H20‑3e 添加 Triton‑fused‑MoE 配置（[#18091]）([PR](https://github.com/sgl-project/sglang/commit/e484c90cc7aa3340b073be1d09e0731fb414c9ca))  
  - 两个 JSON 配置文件均新增 146 行（共 292 行）  

- `9b1619c` – JIT‑kernel：新增 concat MLA 内核及基准（[#17889]）([PR](https://github.com/sgl-project/sglang/commit/9b1619c1480a800e9ddb3d2e42ff32ee2dd2679))  
  - `python/sglang/jit_kernel/benchmark/bench_concat_mla.py`（新增 163 行）  
  - `python/sglang/jit_kernel/concat_mla.py`（新增 65 行）  
  - `python/sglang/jit_kernel/csrc/elementwise/concat_mla.cuh`（新增 325 行）  
  - `python/sglang/jit_kernel/tests/test_concat_mla.py`（新增 169 行）  

- `62004fd` – Diffusion UX：改进日志输出（[#18122]）([PR](https://github.com/sgl-project/sglang/commit/62004fd2beb77daa10502e292e66a0d2f5662e3e))  
  - 影响 `gpu_worker.py`、`diffusers_pipeline.py`、`composed_pipeline_base.py`、`latent_preparation.py`、`logging_utils.py`（累计改动 59 行）  

- `1805943` – HiCache：支持 DeepSeek v32 CPU offloading（[#17415]）([PR](https://github.com/sgl-project/sglang/commit/180594358b990b2a5ce8140fb64aae90d73910fd))  
  - `python/sglang/srt/mem_cache/hiradix_cache.py`（新增 15 行，删除 1 行）  
  - `python/sglang
### vllm-project/vllm
**提交**  

- **[#32712]** 初始实现 ResponsesAPI 解析器（`e1bf04b`）  
  - 新增 `vllm/parser/` 包：`abstract_parser.py`、`minimax_m2_parser.py`、`parser_manager.py`、`__init__.py`。  
  - 相关测试文件 `tests/entrypoints/openai/*` 各加 1 行，用于验证错误处理。  

- **[#33701]** 修复 async 调度下 torchrun 参数服务器广播死锁（`0208017`）  
  - 修改 `vllm/v1/worker/gpu_model_runner.py`（+4‑1 行）以及分布式测试 `tests/distributed/*`（各删 3 行）。  

- **[#33060]** 统一 Pooling 接口请求 schema（`1b8fe6f`）  
  - 关键改动：`vllm/entrypoints/llm.py`（‑73 +37 行），`vllm/entrypoints/pooling/score/*`（共 178 行改动），以及对应示例和测试文件。  

- **[#33729]** 修复 Spec Decoding 中负数接受 token 指标崩溃（`52ee210`）  
  - 仅修改 `vllm/v1/core/sched/scheduler.py`（+1 ‑1 行）和测试 `tests/v1/core/test_scheduler.py`（+60 行）。  

- **[#33351]** 移除依赖文件中的 Ray 注释（`655efb3`）  
  - `requirements/cuda.txt`、`requirements/rocm.txt` 各删 1 行、加 1 行。  

- **[#33579]** 修复稀疏 MLA 元数据构建错误（`bd8da29`）  
  - 影响 `vllm/model_executor/layers/attention/mla_attention.py`（‑31 +22 行）。  

- **[#33613]** 在 TRTLLM FP8 MoE 中禁用不兼容的路由方法（`2a99c5a`）  
  - 主要改动 `vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py`（‑4 +31 行）以及量化层相关文件。  

- **[#33716]** Voxtral Realtime 名称更改（`3f7662d`）  
  - 更新示例 `examples/online_serving/openai_realtime_*.py`（±2 行）和对应测试。  

- **[#33257]** 修复量化 Mamba 模型的 Tensor Parallelism（`a372f3f`）  
  - 大幅改动 `vllm/model_executor/layers/mamba/mamba_mixer2.py`（‑118 +84 行）。  

- **[#31541]** 将 `@config` 标记为 `dataclass_transform`（`61e632a`）  
  - 影响多处配置文件及测试，共 155 行新增、193 行删除。  

- **[#33641]** 大幅提升 `torch.compile` 冷启动速度（`b1bb18d`）  
  - `vllm/compilation/backends.py`（+36 ‑14 行）等文件共 41 行新增。  

- **[#23465]** FA3 加入新 swizzle 优化（`2267cb1`）  
  - 触及 `vllm/v1/attention/backends/flash_attn.py`、`mla/flashattn_mla.py`（共 +13 ‑3 行）。  

- **[#31034]** 重构 Mooncake 连接器并新增 bootstrap 服务器（`0d6ccf6`）  
  - 新增 `examples/online_serving/disaggregated_serving/mooncake_connector/*`（+598 行），并在 `vllm/distributed/kv_transfer/kv_connector/v1/mooncake/*` 中迁移实现（+771 ‑194 行）。  

- **[#33699]** 修复 Granite Speech 启动卡死（`18e7cbb`）  
  - `vllm/multimodal/budget.py`（±8 行）。  

- **[#33576]** Voxtral 跳过 warm‑up，避免误报（`f0d5251`）  
  - 修改 `vllm/entrypoints/openai/translations/speech_to_text.py`（+4 ‑3 行）以及模型文件。  

- **[#33674]** 为 MMEncoderAttention 添加 `prefix` 参数（`5c4f2dd`）  
  - 多模型文件均加入 4‑1 行改动（共 58 行新增）。  

- **[#33647]** 修正仅图像输入时多模态文本提示多余换行（`f3d8a34`）  
  - `vllm/entrypoints/chat_utils.py`（+4 ‑1 行）。  

- **[#33345]** 添加 Nemotron Nano v3 测试配置（`4bc913a`）  
  - 新增两份 Buildkite 配置文件，`tests/config/base_model_arch_groundtruth.json` 增加 17 行。  

- **[#33377]** 防止异步 KV 缓存转移时双重释放（`fbb3cf6`）  
  - `vllm/v1/core/sched/scheduler.py`（+6 ‑2 行）。  

- **[#33552]** 文档说明 NixlConnector 后端选择（`2df2b34`）  
  - 更新 `docs/features/nixl_connector_usage.md`（+29 行）。  

- **[#33673]** 修复 Gemma3n 音频编码器兼容 Transformers v5（`2a8d84e`）  
  - `vllm/model_executor/models/gemma3n_mm.py`（+9 ‑4 行）。  

- **[#33636]** 新增 Intern‑S1‑Pro 模型支持（`a3acfa1`）  
  - 关键文件 `vllm/model_executor/models/interns1_pro.py`（+633 行）以及 rotary embedding 新实现 `fope.py`（+199 行）。  

- **[#33683]** 修复 Gemma3 GGUF 在 Transformers v5 下的加载（`be8168f`）  
  - `vllm/transformers_utils/gguf_utils.py`（+1 ‑1 行）。  

- **[#33620]** 禁用 TRTLLM 中对 Renormalize 系列路由的 FP8 支持（`e346e2d`）  
  - `vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py`（+4 ‑2 行）。  

- **[#33665]** 清理 Pooling 串行工具并重构实现（`83449a5`）  
  - 新增 `vllm/entrypoints/pooling/utils.py`（+124 行），`vllm/utils/serial_utils.py` 大幅删改（‑157 +41 行）。  

- **[#33642]** 修复 DeepSeek‑VL2 chat 模板缺少 BOS token（`dad2d6a`）  
  - `vllm/transformers_utils/configs/deepseek_vl2.py`（+5 ‑4 行）。  

- **[#33541]** 将 `@config` 标记为 `dataclass_transform`（已列于上）  

- **[#33571]** 文档说明 `torch.compile` standalone_compile 的 workaround（`fd9c83d`）  
  - 更新 `docs/design/debug_vllm_compile.md`（+9 行）和 `vllm/compilation/compiler_interface.py`（+19 行）。  

- **[#33535]** 移除已废弃的 `VLLM_ALL2ALL_BACKEND` 环境变量（`b95cc50`）  
  - `vllm/config/parallel.py` 删除 9 行，`vllm/envs.py` 删除 33 行。  

- **[#33597]** Scheduler 代码简化（`6139789`）  
  - `vllm/v1/core/sched/scheduler.py`（‑29 +19 行）。  

- **[#33536]** 移除已废弃的 profiler 环境变量（`ef248ff`）  
  - `vllm/config/profiler.py` 删除 81 行，`vllm/envs.py` 删除 14 行。  

- **[#33379]** XPU 平台弃用 ipex，切换至 `vllm-xpu-kernels`（`e106044`）  
  - 大量文件改动，核心是 `vllm/_ipex_ops.py`（‑360 +73 行）以及量化层相关文件。  

- **[#33635]** 兼容 Interleaved thinking 中的 `reasoning_content`（`bf001da`）  
  - `vllm/entrypoints/chat_utils.py`（+3 行）。  

- **[#33553]** 移除 Dockerfile 中硬编码时区（`a0a984a`）  
  - `docker/Dockerfile*` 各删 9‑6 行，新增 5 行。  

- **[#32728]** 修复量化 Falcon‑H1 模型加载问题（`f1cb9b5`）  
  - `vllm/model_executor/models/falcon_h1.py`（+14 ‑2 行）。  

- **[#32609]** 为 Responses API 添加采样参数（`4c4b6f7`）  
  - 新增测试 `tests/entrypoints/openai/responses/test_sampling_params.py`（+113 行），并在 `vllm/entrypoints/openai/responses/protocol.py` 中加入 28 行改动。  

- **[#33634]** 修正 Qwen Omni 模型的多模态预算设置（`10546f9`）  
  - `vllm/multimodal/budget.py`（+5 行）。  

- **[#30329]** CPU 后端 ARM 向量化优化（`e69c990`）  
  - 关键改动 `csrc/cpu/cpu_types_arm.hpp`（‑579 +551 行），其他 CPU 相关实现少量增删。  

- **[#33624]** 在 speculative decoding 场景下禁用 fast MoE 冷启动优化（`5eac9a1`）  
  - `vllm/config/compilation.py`（+18 行），`vllm/forward_context.py`（+14 ‑1 行）。  

- **[#32032]** 为 CPU 镜像上传 Docker Hub 添加说明脚本（`1b60b45`）  
  - `.buildkite/scripts/annotate-release.sh`（+23 ‑1 行）。  

- **[#32739]** DPMetadata 在 dense 模型上抛出断言错误的修复（`4b3803d`）  
  - `vllm/forward_context.py`（+5 ‑3 行）。  

**Issues**  

- **[#33741]** 优化 `--help` 性能：避免在帮助信息展示时导入 torch（AbhiOnGithub）  
  - 通过延迟导入 `torch`，可减少 1‑3 秒启动延迟。  

- **[#33733]** 跟踪 DeepSeek flashinfer 优化集成进度（hjjq）  
  - 列出多个待集成的 flashinfer PR，标记当前进展。  

- **[#33708]** 更新 CPU 镜像文档以使用 Docker Hub（nathan‑weinberg）  
  - 需在文档中切换至最新发布的 CPU 镜像链接。  

- **[#33704]** LoRA 在 XPU/CPU punica 包装器中 dtype 不匹配（plugyawn）  
  - 现有实现强制使用 `torch.float32`，导致 bfloat16 模型 LoRA 失效。  

- **[#33702]** PD Disaggregation 与 `NixlConnector` 路线图（NickLucche）  
  - 规划未来的分布式 KV 转移实现路径。  

- **[#33697]** 模型架构 `CostWiseGemmaForCausalLM` 暂不受支持（CAPOJ）  
  - 需要在模型注册表中添加对应实现。  

- **[#33696]** CPU 后端 Whisper W8A8 推理失败（aditew01）  
  - 讨论了量化实现与 CPU kernel 的兼容性问题。  

- **[#33692]** Flash Attn MLA 后端不支持 `headdim!=576`（Yuxin1999）  
  - 需要在 MLA 实现中加入对不同 head 维度的适配。
### NVIDIA/cutile-python
**提交**  

- **[7bad878](https://github.com/NVIDIA/cutile-python/commit/7bad87895b5b9501df3a7f760e23655aea477e04)** – Ziheng Deng, 2026‑02‑02 18:52 UTC  
  - **说明**：为 gather / scatter 添加自定义 mask 支持。  
  - **改动文件**  
    1. `changelog.d/gather-scatter-mask.md` – 新增 (4 行)  
    2. `src/cuda/tile/_ir/ops.py` – 修改 (新增 61 行，删除 12 行)  
    3. `src/cuda/tile/_stub.py` – 修改 (新增 25 行，删除 6 行)  
    4. `test/test_gather_scatter.py` – 修改 (新增 195 行)  

  > **代码片段**：由于 `patch_truncated` 为 true，未提供具体 diff。若需查看实现细节，请访问对应的 commit 链接。

**Issues**  

- **[BUG] There’s a bug in the swizzle computation in MatMul.py for misaligned (non-aligned) cases** – 作者: zhao008, 2026‑02‑03 06:59 UTC  
  - **链接**：<https://github.com/NVIDIA/cutile-python/issues/68>  
  - **说明**：Issue 内容缺失（`body_truncated` 为 true），请在 Issue 页面查看完整描述。

## 总结
整体来看，这些改动在算子同步、编译 DSL、分布式调度以及硬件特化方面同步推进，预示着 AI 基础设施正向更高的吞吐与更广的硬件兼容性迈进。开发者可期待在 Hopper/Blackwell 等新一代 GPU 上获得更低的延迟与更高的算力利用率，同时多模态与分布式部署的成熟度也将进一步提升。后续关注这些项目在实际生产环境中的性能验证以及跨平台（如 XPU、Ascend）支持的进展，将为下一波 AI 基础设施升级提供重要参考。
