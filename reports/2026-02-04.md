今日的 AI Infra 的新闻如下。

## 摘要
```json
{
  "summary": "NVIDIA/cutlass 发布 v4.4 更新，优化 Blackwell GEMM 示例但存在 cuTensorMapEncodeTiled 性能开销问题；flashinfer-ai/flashinfer 新增 FP8 MoE 支持并修复 CI 权限错误；Dao-AILab/flash-attention 标记 Flash Attention 3 为稳定版，修复共享内存竞争条件；sgl-project/sglang 引入 Layerwise Offload for MoVA 和 InternS1-Pro 模型支持，优化线性层融合；vLLM 并行化 CPU CI 测试，集成 ColBERT 模型但报告 FA4 SM90 Decode 性能下降 20%。",
  "conclusion": "风险点：Cutlass 存在 segmentation fault 和 import error（#3001/#3002），可能影响稳定性；vLLM 报告 Docker 兼容性问题（#33833）和 DeepSeek benchmark 失败（#33831）；SGLang 需更新 speculative decoding 文档（#18268）并解决 FA4 性能瓶颈。"
}
```

## 具体内容分析
### deepseek-ai/DeepGEMM
昨日无更新。
### deepseek-ai/FlashMLA
昨日无更新。
### NVIDIA/cutlass
提交：
- v4.4 release update v2. (#2999) [`6b3e607`](https://github.com/NVIDIA/cutlass/commit/6b3e607b852f1543dc21323155a2ad71473c8642)
- 受影响文件: CHANGELOG.md, README.md, examples/python/CuTeDSL/blackwell/dense_gemm_persistent_dynamic.py, examples/python/CuTeDSL/blackwell/sm103_dense_blockscaled_gemm_persistent.py, examples/python/CuTeDSL/experimental/ampere/memcpy_simt_universal_copy.py, examples/python/CuTeDSL/experimental/blackwell/dense_block_scaled_gemm.py, examples/python/CuTeDSL/experimental/blackwell/dense_gemm.py, examples/python/CuTeDSL/experimental/blackwell/dense_gemm_2sm.py, examples/python/CuTeDSL/experimental/blackwell/dense_gemm_cute_pipeline.py, examples/python/CuTeDSL/experimental/blackwell/dense_gemm_ptr_array.py, include/cutlass/gemm/kernel/sm100_gemm_array_tma_warpspecialized.hpp, include/cutlass/gemm/kernel/sm100_gemm_array_tma_warpspecialized_input_transform.hpp, include/cutlass/gemm/kernel/sm100_gemm_array_tma_warpspecialized_mma_transform.hpp, include/cutlass/gemm/kernel/sm100_tile_scheduler_group.hpp, include/cutlass/gemm/kernel/sm103_blockscaled_gemm_array_tma_warpspecialized.hpp, include/cutlass/gemm/kernel/sm90_gemm_array_tma_warpspecialized_cooperative.hpp
- 文件列表已截断。
Issues：
- Performance overhead: Frequent cuTensorMapEncodeTiled and cudaGetDriverEntryPoint calls during Blackwell GEMM loops (https://github.com/NVIDIA/cutlass/issues/3003)
- Issue 内容已截断。
- [BUG] segmentation fault when calling `cute.print_tensor` on fp8 shared-memory tensor (https://github.com/NVIDIA/cutlass/issues/3002)
- Issue 内容已截断。
- [BUG] nvidia-cutlass-dsl v4.4.0.dev1 import error (https://github.com/NVIDIA/cutlass/issues/3001)
- Issue 内容已截断。
- [QST] is cutlass support on nvidia GPU Tesla V100 ? (https://github.com/NVIDIA/cutlass/issues/3000)
### flashinfer-ai/flashinfer
### Commits
- [Fix permission errors in release workflow on ci-infra runner](https://github.com/flashinfer-ai/flashinfer/commit/cdbb2c3c73598bbb04a0e2dc2d7cf6ee9e5aeda5) - Yong Wu. Removed `chown` commands and `--user` flag in Docker workflows to resolve CI permission issues. Affected files: `.github/workflows/nightly-release.yml`, `.github/workflows/release.yml`.
- [Support Fused MoE non-gated Relu2 NVFP4 & FP8 and Nemotron](https://github.com/flashinfer-ai/flashinfer/commit/e284274e2eb67538bb9c884f2ed9c0143772d3ac) - amitz-nv. Added FP8 block-scale MoE support and updated benchmarks. Key changes: `bench_trtllm_gen_fused_moe_autotuner.py` (added `ActivationType` handling), `trtllm_fused_moe_runner.cu` (routing logic). Diff truncated—see commit for full changes.
- [Add/update multi-node/multi-GPU test scripts](https://github.com/flashinfer-ai/flashinfer/commit/9bf007d7403f0a394fda44abd4170b354cac3f05) - Jonathan Dierksen. Replaced `task_test_blackwell_kernels.sh` with new scripts: `task_run_unit_tests.sh`, `task_test_multi_gpu_comm_kernels.sh`, and `test_utils.sh`. Added shared utility functions.
- [Rename tests/mamba/test_utils.py to fix CI test discovery](https://github.com/flashinfer-ai/flashinfer/commit/567ded1f612c38bd6a921a4a7ce4ecc6db2b9a9e) - Brian K. Ryu. Fixed test imports by renaming `test_utils.py` to `utils.py`. Updated references in `test_selective_state_update_*.py`.
- [Fix memory bandwidth calculation in MLA benchmarks](https://github.com/flashinfer-ai/flashinfer/commit/f84ac1c97e2e1a4391150ca
### Dao-AILab/flash-attention
- 标记Flash Attention 3主版本为v3.0.0稳定版（PR [#2223](https://github.com/Dao-AILab/flash-attention/pull/2223)），由Luca Wehrstedt于2026-02-05提交。修改文件：`hopper/__init__.py`（1行添加，1行删除）。注意：diff截断，无法显示完整代码。
- 为flex flash添加简短README文档（PR [#2231](https://github.com/Dao-AILab/flash-attention/pull/2231)），由Markus Hoehnerbach于2026-02-05提交。修改文件：`flash_attn/cute/README.md`（26行添加）。注意：diff截断。
- 在构建配置中用`TORCH_TARGET_VERSION`替换`TORCH_STABLE_ONLY`（PR [#2155](https://github.com/Dao-AILab/flash-attention/pull/2155)），由Jane (Yuan) Xu于2026-02-04提交。修改文件：`hopper/setup.py`（1行添加，1行删除）。注意：diff截断。
- 修复共享内存竞争条件（PR [#2229](https://github.com/Dao-AILab/flash-attention/pull/2229)），由Driss Guessous于2026-02-04提交。修改文件：`flash_attn/cute/block_sparse_utils.py`（1行添加，1行删除）和`flash_attn/cute/flash_fwd_sm100.py`（3行添加，3行删除）。注意：diff截断。
### sgl-project/sglang
# SGLang Newsletter - 2026-02-04

## Core Improvements

- **Layerwise Offload for MoVA** ([dff3ba2](https://github.com/sgl-project/sglang/commit/dff3ba202ad034630a8faa53ae6550a06b981e90)): Added support for layerwise offloading in MoVA models to optimize memory usage.

- **InternS1-Pro Model Support** ([3e7ecb7](https://github.com/sgl-project/sglang/commit/3e7ecb78a60f8e1d889cfe25c88006577783d903)): Added support for the InternS1-Pro model with new model implementation and multimodal processor.

- **Linear Layer Fusion** ([37c33cc](https://github.com/sgl-project/sglang/commit/37c33cc0aa6213fd4abcfb40c3e1d71dde484295)): Implemented fusion of qkvbfg linear operations into single GEMM and f_b g_b into batched GEMM for improved performance.

- **TopK Ragged Optimization** ([760ae93](https://github.com/sgl-project/sglang/commit/760ae933bb3878a6897e7e552a746929c29e9d90)): Optimized get_topk_ragged by fusing get k and k_scale operations in a Triton kernel.

## Diffusion Enhancements

- **Kernel Fusion for Diffusion Models** ([4739f2e](https://github.com/sgl-project/sglang/commit/4739f2e8d5732f7464d1af75d31b4d44c61783b6)): Implemented gated residual layernorm scale shift and layernorm scale shift kernel fusion for Qwen-Image, WAN and HunyuanVideo models.

- **MOVA Code Cleanup** ([0c9a0ad](https://github.com/sgl-project/sglang/commit/0c9a0adc5329d393435097337fa113174363299e)): Cleaned up MOVA code by removing redundant implementations in audio and video DIT models.

- **GPU Memory Fix** ([4c40304](https://github.com/sgl-project/sglang/commit/4c403045ec690dbcf3b63a941356e004201ba337)): Fixed redundant memory usage on GPU-0 in the diffusion module.

## Platform Support

- **AMD GPU Improvements** ([6fd878b](https://github.com/sgl-project/sglang/commit/6fd878b41df0153bd28f0185920e1b2d9dcc7480)): Added Kimi MI35X nightly tests, reorganized test folders, and implemented stability fixes for AMD GPUs.

- **NPU Allocator Refactoring** ([84c0991](https://github.com/sgl-project/sglang/commit/84c09913eb1458278f62f9dc393007141d3b67c3)): Moved _alloc_extend_naive out of NPU allocator to improve memory management.

## Documentation & Usability

- **Markdown Documentation Export** ([e616d35](https://github.com/sgl-project/sglang/commit/e616d3584737686f6d221ce3f21c67b98a936827), [669a9bd](https://github.com/sgl-project/sglang/commit/669a9bd1809a344fae5b9d962d2cd6f842cb50c1)): Enhanced documentation export to support Markdown/Notebook-friendly formats for downstream integration.

- **Special Tokens Support** ([a6f53cc](https://github.com/sgl-project/sglang/commit/a6f53cc5e3ac7eb8ae9e4236d9834897684505ad)): Added support for passing spaces_between_special_tokens per request in the OpenAI API protocol.

- **Documentation Fixes** ([de6a032](https://github.com/sgl-project/sglang/commit/de6a03260f59fd33a9eeb8f67e7e6e2cf235a70f)): Fixed misspellings and typos across multiple documentation files.

## Performance Optimizations

- **KV Offset Calculation Improvement** ([f730c18](https://github.com/sgl-project/sglang/commit/f730c186799d966a62531269ce46178364c85dc3)): Enhanced KV offset calculation for MHA models with different tensor parallel sizes.

- **Memory Management Fix** ([315306d](https://github.com/sgl-project/sglang/commit/315306d8a9f93ed24e3a6d532b3c1da41e83ca9d)): Ensured symmetric memory is disabled without dp padding to prevent unnecessary memory usage.

- **DeepGemm Fast Warmup** ([d279520](https://github.com/sgl-project/sglang/commit/d279520ba5771e0bd361c6a762b653391bb1bc09)): Added a flag for fast warmup in DeepGemm to reduce initialization time.

## Bug Fixes

- **Test Fixes** ([c910829](https://github.com/sgl-project/sglang/commit/c910829708c7b71f82e646393c6503b17501e396), [2e87c2b](https://github.com/sgl-project/sglang/commit/2e87c2bd5e43bfad57150ff878761bc6cffc0ab8)): Fixed tests for routed experts and MockModelRunner in attention tests.

- **Session Fix for Multimodal** ([c1d529c](https://github.com/sgl-project/sglang/commit/c1d529c19605cbf1f9be8db6d6d225b1465ea2e0)): Fixed Session handling for multimodal models and exposed it through the Engine.

- **Logic Error Fix** ([c1d5cc3](https://github.com/sgl-project/sglang/commit/c1d5cc3b24ada6857bc32af13e0a0528a01fcb70)): Fixed an obvious logic error in pyproject.toml files.

## Notable Issues

- **Speculative Decoding Documentation** ([#18268](https://github.com/sgl-project/sglang/issues/18268)): Need to update documentation to include SGLANG_ENABLE_SPEC_V2 and ngram support.

- **Piecewise CUDA Graph Documentation** ([#18267](https://github.com/sgl-project/sglang/issues/18267)): Request for documentation of the --enable-piecewise-cuda-graph feature.

- **Pi0 VLA Model Support** ([#18266](https://github.com/sgl-project/sglang/issues/18266)): Feature request for adding Pi0/Pi0-FAST VLA model inference support for robotics.

- **FA4 SM90 Decode Integration** ([#18265](https://github.com/sgl-project/sglang/issues/18265)): Integration of FA4 SM90 Decode, currently 20% slower than FA3.

- **AITER Attention OOM Errors** ([#18262](https://github.com/sgl-project/sglang/issues/18262)): OOM errors due to max_total_num_tokens not accounting for attention backend memory requirements.
### vllm-project/vllm
提交：
- [CI/Build] Parallelize CPU CI tests (#33778) [`07daee1`](https://github.com/vllm-project/vllm/commit/07daee132b30140bb7c5b28d7f8c856036d2baad)
- 受影响文件: .buildkite/hardware_tests/arm.yaml, .buildkite/hardware_tests/cpu.yaml, .buildkite/hardware_tests/intel.yaml, .buildkite/scripts/hardware_ci/run-cpu-distributed-smoke-test.sh, .buildkite/scripts/hardware_ci/run-cpu-test.sh, vllm/v1/worker/cpu_worker.py
- [2/N] move responses/serving _make_response_output_items logic to parser (#33281) [`9595afd`](https://github.com/vllm-project/vllm/commit/9595afda183bdd79b0ee2d38d2b0049fe86e6628)
- 受影响文件: vllm/entrypoints/openai/responses/serving.py, vllm/parser/abstract_parser.py
- [CI][AMD][BugFix] Ensure VLLM_ROCM_USE_AITER is set so test_rocm_aiter_topk.py can run correctly (#33840) [`c1395f7`](https://github.com/vllm-project/vllm/commit/c1395f72cd22d97eb39ecd67d9d22f2af3d20bda)
- 受影响文件: tests/kernels/moe/test_rocm_aiter_topk.py
- [docs] fix unintentional misspellings (#33863) [`007b183`](https://github.com/vllm-project/vllm/commit/007b183d745f5b37aeb6cdf936c3b590b0c29fde)
- 受影响文件: docs/contributing/model/basic.md, docs/contributing/model/multimodal.md, docs/getting_started/quickstart.md
- [Minor] Include `StreamingInput` in inputs package (#33856) [`add9f1f`](https://github.com/vllm-project/vllm/commit/add9f1fbd920611c2b909fe10d9b44b59650f8b7)
- 受影响文件: tests/v1/e2e/test_streaming_input.py, tests/v1/streaming_input/test_async_llm_streaming.py, vllm/inputs/__init__.py, vllm/v1/engine/async_llm.py
- Revert "[Attention][FA3] Update FA3 to include new swizzle optimization" (#33841) [`e3bf79f`](https://github.com/vllm-project/vllm/commit/e3bf79ffa080a5052aa61fce71b70b11fb7f9d1e)
- 受影响文件: cmake/external_projects/vllm_flash_attn.cmake, vllm/v1/attention/backends/flash_attn.py, vllm/v1/attention/backends/mla/flashattn_mla.py
- [CI][Bugfix]: return McpCall for built-in MCP tools in non-streaming mode (#32762) [`fb1270f`](https://github.com/vllm-project/vllm/commit/fb1270f1f821603402a8868e3067b3c3342455e7)
- 受影响文件: tests/entrypoints/openai/responses/test_harmony.py, tests/entrypoints/openai/responses/test_mcp_tools.py, tests/entrypoints/openai/responses/test_parsable_context.py, tests/entrypoints/openai/responses/test_simple.py, tests/utils.py, vllm/entrypoints/openai/parser/harmony_utils.py
- [release] Minor fixes to release annotation (#33849) [`72bb24e`](https://github.com/vllm-project/vllm/commit/72bb24e2db2acd98a49adcb9e3f1dc6f1bbef4c0)
- 受影响文件: .buildkite/scripts/annotate-release.sh
- [Bugfix] fix DeepSeek R1 with CUTLASS MLA Broken on B200 (#33637) [`a7be77b`](https://github.com/vllm-project/vllm/commit/a7be77beef5f59d9d349818b4f2860483551b255)
- 受影响文件: vllm/model_executor/layers/attention/mla_attention.py
- [Bugfix] Disable TRTLLM attention when KV transfer is enabled (#33192) [`bbe0574`](https://github.com/vllm-project/vllm/commit/bbe0574d8e51c1c5935aeff9e92040c61d1d59c5)
- 受影响文件: vllm/v1/attention/backends/flashinfer.py
- [CI][torch.compile] Reduce e2e fusion test time (#33293) [`4d95135`](https://github.com/vllm-project/vllm/commit/4d9513537d00a9b6678a2b1ed3c3566a81f7dd77)
- 受影响文件: .buildkite/test-amd.yaml, .buildkite/test-pipeline.yaml, .buildkite/test_areas/compile.yaml, .buildkite/test_areas/distributed.yaml, .buildkite/test_areas/pytorch.yaml, tests/compile/distributed/test_fusions_e2e.py, tests/compile/fusion_test_utils.py, tests/compile/fusions_e2e/__init__.py, tests/compile/fusions_e2e/common.py, tests/compile/fusions_e2e/conftest.py, tests/compile/fusions_e2e/models.py, tests/compile/fusions_e2e/test_tp1_quant.py, tests/compile/fusions_e2e/test_tp2_ar_rms.py, tests/compile/fusions_e2e/test_tp2_async_tp.py, tests/compile/test_fusion_attn.py, tests/test_config.py
- 文件列表已截断。
- feat: Add ColBERT late interaction model support (#33686) [`439afa4`](https://github.com/vllm-project/vllm/commit/439afa4eea14db2be232a9ce78eacc2c7bbfac77)
- 受影响文件: docs/models/pooling_models.md, examples/pooling/score/colbert_rerank_online.py, tests/entrypoints/pooling/score/test_online_colbert.py, tests/models/language/pooling/test_colbert.py, tests/models/registry.py, vllm/config/model.py, vllm/entrypoints/llm.py, vllm/entrypoints/pooling/__init__.py, vllm/entrypoints/pooling/score/serving.py, vllm/entrypoints/pooling/score/utils.py, vllm/model_executor/models/colbert.py, vllm/model_executor/models/interfaces.py, vllm/model_executor/models/registry.py
- [Core] Don't schedule spec tokens with prefill chunks (#33652) [`fa4e0fb`](https://github.com/vllm-project/vllm/commit/fa4e0fb028460cf5f4eb9cc90e206d0d6f35b026)
- 受影响文件: tests/v1/core/test_scheduler.py, vllm/v1/core/sched/async_scheduler.py, vllm/v1/core/sched/scheduler.py, vllm/v1/request.py, vllm/v1/worker/gpu/spec_decode/utils.py
- Change the type signature of MixtureOfExperts.expert_weights to MutableSequence[Sequence[Tensor]] (#33573) [`ce498a6`](https://github.com/vllm-project/vllm/commit/ce498a6d61a083b1bbbd1cc92754961b43860ff6)
- 受影响文件: vllm/distributed/eplb/rebalance_execute.py, vllm/model_executor/models/interfaces.py
- Revert "[torch.compile] Significantly speed up cold start times" (#33820) [`9f14c92`](https://github.com/vllm-project/vllm/commit/9f14c9224d3d6664e2f5a2e7fecd012fd048fcb1)
- 受影响文件: tests/compile/test_cold_start.py, vllm/compilation/backends.py, vllm/compilation/compiler_interface.py
- [Model] Add transcription support for Qwen3-Omni (#29828) [`535de06`](https://github.com/vllm-project/vllm/commit/535de06cb1d90ed1c48246a512e74c87fe1768e4)
- 受影响文件: docs/contributing/model/transcription.md, docs/models/supported_models.md, vllm/model_executor/models/qwen3_omni_moe_thinker.py
- [Bugfix] Support `RotaryEmbedding` CustomOp for gpt-oss (#33800) [`4292c90`](https://github.com/vllm-project/vllm/commit/4292c90a2a188121ccbfd132def62031283d9d8a)
- 受影响文件: tests/compile/test_rotary_embedding_compile.py, vllm/model_executor/layers/rotary_embedding/base.py, vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope.py, vllm/model_executor/layers/rotary_embedding/mrope.py
- Implement zero-copy GQA for multimodal and CPU (#33732) [`6e98f6d`](https://github.com/vllm-project/vllm/commit/6e98f6d8b64984d5e3a8a9b323321f1095bac8b3)
- 受影响文件: vllm/model_executor/layers/attention/mm_encoder_attention.py, vllm/model_executor/models/molmo2.py, vllm/v1/attention/backends/cpu_attn.py, vllm/v1/attention/ops/vit_attn_wrappers.py
- [rocm][ray] Fix: Unify Ray device visibility handling across CUDA and ROCm (#33308) [`2f6d17c`](https://github.com/vllm-project/vllm/commit/2f6d17cb2f4a49e29aae5c3c1ff64623d66d0257)
- 受影响文件: docker/Dockerfile.rocm, tests/config/test_config_generation.py, vllm/platforms/cuda.py, vllm/platforms/interface.py, vllm/platforms/rocm.py, vllm/v1/executor/ray_executor.py, vllm/v1/worker/gpu_worker.py, vllm/v1/worker/worker_base.py
- [Bugfix] Fix interns1-pro initialization and PP (#33793) [`192ad46`](https://github.com/vllm-project/vllm/commit/192ad4648b2066ebdf1fa04ad84f24bdf0cd6533)
- 受影响文件: tests/models/multimodal/processing/test_common.py, tests/models/multimodal/processing/test_tensor_schema.py, tests/models/registry.py, vllm/model_executor/models/interns1_pro.py, vllm/model_executor/models/qwen3_vl.py, vllm/model_executor/models/qwen3_vl_moe.py
- [Misc] Delay deprecation of CommonAttentionMetadata properties (#33801) [`0e92298`](https://github.com/vllm-project/vllm/commit/0e922986222d9c25ce320fbdd0aff20279c2cb93)
- 受影响文件: vllm/v1/attention/backend.py
- [Bugfix] Fix ubatch wrapper num_tokens calculate (#33694) [`87d9a26`](https://github.com/vllm-project/vllm/commit/87d9a261664705e0c9635014b4e2d49eddc8a056)
- 受影响文件: vllm/v1/worker/gpu_ubatch_wrapper.py
- [Bugfix] Fix `normalize` still being passed to `PoolerConfig` (#33794) [`80f921b`](https://github.com/vllm-project/vllm/commit/80f921ba4bab2ea251d149305ea0f912c6fc218a)
- 受影响文件: tests/models/language/pooling/test_embedding.py, vllm/entrypoints/llm.py
- [Perf] Optimize spec decoding + async scheduling, 1.5% Throughput improvement (#33612) [`711edaf`](https://github.com/vllm-project/vllm/commit/711edaf0d089a15df5fa2b99248c516e53929bd2)
- 受影响文件: vllm/v1/core/sched/async_scheduler.py, vllm/v1/core/sched/scheduler.py
- [Bugfix][ROCm] Include float8_e4m3fnuz in NCCL Dtype Dispatching (#33713) [`1d367a7`](https://github.com/vllm-project/vllm/commit/1d367a738e9098ad4af1f6865747914ccd2c65ca)
- 受影响文件: vllm/distributed/device_communicators/pynccl_wrapper.py
- Apply #33621 to main (#33758) [`32a02c7`](https://github.com/vllm-project/vllm/commit/32a02c7ca29180f70c1c8c73d0f57445231b17b5)
- 受影响文件: requirements/common.txt, requirements/rocm-test.txt, requirements/test.txt
- [Perf] Optimize chat completion streaming performance (#33782) [`f67ee8b`](https://github.com/vllm-project/vllm/commit/f67ee8b859215df4b521c67b9f26e27f30c9739f)
- 受影响文件: vllm/entrypoints/openai/chat_completion/serving.py
- [Model] Apply #32631 for recent models (#33785) [`e57ef99`](https://github.com/vllm-project/vllm/commit/e57ef99b409ba651695a515f6022c9badf25b2a2)
- 受影响文件: vllm/model_executor/models/eagle2_5_vl.py, vllm/model_executor/models/funaudiochat.py, vllm/model_executor/models/openpangu_vl.py, vllm/model_executor/models/qwen3_asr.py
- [Bugfix][Model] Fix audio-in-video support for Qwen2.5-Omni and Qwen3-Omni       (#33605) [`f8516a1`](https://github.com/vllm-project/vllm/commit/f8516a1ab95febcf131a37478914031f50fdd9db)
- 受影响文件: vllm/model_executor/models/qwen2_5_omni_thinker.py, vllm/model_executor/models/qwen3_omni_moe_thinker.py
- [PERF] Change GDN Attention State Layout from [N, HV, K, V] to [N, HV, V, K] (#33291) [`8240580`](https://github.com/vllm-project/vllm/commit/824058076c56164a3772a5f5829bd9662507e5a3)
- 受影响文件: vllm/model_executor/layers/fla/ops/chunk.py, vllm/model_executor/layers/fla/ops/chunk_delta_h.py, vllm/model_executor/layers/fla/ops/chunk_o.py, vllm/model_executor/layers/fla/ops/fused_recurrent.py, vllm/model_executor/layers/fla/ops/kda.py, vllm/model_executor/layers/mamba/mamba_utils.py
- [KV Connector][BugFix] scheduler: Delay freeing blocks of aborted async loads (#32255) [`8e32690`](https://github.com/vllm-project/vllm/commit/8e3269086916d81b010a2d6209784a106ac2994a)
- 受影响文件: tests/v1/core/test_scheduler.py, tests/v1/kv_connector/unit/test_offloading_connector.py, vllm/v1/core/sched/scheduler.py
- [compile] Remove runner type from ignored caching factor list. (#33712) [`a208439`](https://github.com/vllm-project/vllm/commit/a208439537a9071668b99dc8089db0dd8995034a)
- 受影响文件: vllm/config/model.py
Issues：
- [Bug][Docker]: Issues with 0.15.0 and newer docker image when running Qwen3-Next with VLLM_BLOCKSCALE_FP8_GEMM_FLASHINFER=1 (https://github.com/vllm-project/vllm/issues/33833)
- Issue 内容已截断。
- [Bug]: Deepseek V3.2 Benchmark failure "TypeError: argument 'tokens': 'NoneType' object" (https://github.com/vllm-project/vllm/issues/33831)
- Issue 内容已截断。
- [Bug]: mistral3 offline multimodal inference example failing with prompt placeholder error (https://github.com/vllm-project/vllm/issues/33828)
- Issue 内容已截断。
- [Bug]: Step3p5ForCausalLM fails with pipeline parallelism (https://github.com/vllm-project/vllm/issues/33823)
- Issue 内容已截断。
- [CI Failure]: Quantized Models Test in `tests/models/quantization/test_gptq_marlin.py::test_models[5-32-bfloat16-model2]` (https://github.com/vllm-project/vllm/issues/33816)
- Issue 内容已截断。
- [Bug]: llm.score() fails on batched multimodal input for qwen3-vl-reranker (https://github.com/vllm-project/vllm/issues/33813)
- Issue 内容已截断。
- [CI Failure]:  mi325_4: LM Eval Large Models (H100) (https://github.com/vllm-project/vllm/issues/33812)
- Issue 内容已截断。
- [CI Failure]:  Kernels MoE Test %N (https://github.com/vllm-project/vllm/issues/33809)
- Issue 内容已截断。
### NVIDIA/cutile-python
昨日无更新。

## 总结
- Diff 内容已截断以满足 prompt 预算。
- Issue 内容已截断以满足 prompt 预算。
- OpenRouter repo summarize failed for NVIDIA/cutlass: OpenRouter 402 Payment Required (qwen/qwen3-coder:free): {"error":{"message":"Provider returned error","code":402,"metadata":{"raw":"{\"error\":\"API key USD spend limit exceeded. Your account may still have USD balance, but this API key has reached its configured USD spending limit.\"}","provider_name":"Venice","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
- OpenRouter repo summarize failed for vllm-project/vllm: OpenRouter 404 Not Found (moonshotai/kimi-k2:free): {"error":{"message":"No endpoints found for moonshotai/kimi-k2:free.","code":404},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
