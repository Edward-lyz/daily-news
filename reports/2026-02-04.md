今日的 AI Infra 的新闻如下。

## 摘要
```json
{
  "summary": "NVIDIA发布了CUTLASS 4.4，增强了CuTe DSL功能并添加了对GB300架构的支持，同时引入了无片段编程等高级特性。FlashInfer新增了Fused MoE支持并优化了Hopper GDN预填充内核性能，但mxfp4量化内核性能仍需改进。SGLang优化了不同TP大小下的MHA模型KV偏移计算，并添加了对interns1-pro模型的支持。VLLM添加了ColBERT模型支持和Qwen3-Omni转录功能，同时优化了聊天完成流式性能和规范解码异步调度，实现了1.5%的吞吐量提升。",
  "conclusion": "随着Blackwell架构的普及，针对新硬件的优化将成为重点，但需注意性能回归问题。多模态模型支持持续扩展，但音频处理和容器化部署仍存在挑战。量化技术（如NVFP4、MXFP4）的准确性问题需要持续关注。异步调度和零拷贝技术成为性能优化的关键方向，但可能引入新的兼容性问题。分布式训练环境中的内存管理和同步机制仍是稳定性隐患。"
}
```

## 具体内容分析
### deepseek-ai/DeepGEMM
昨日无更新。
### deepseek-ai/FlashMLA
昨日无更新。
### NVIDIA/cutlass
# NVIDIA CUTLASS 4.4 Release Update

## 版本发布
- **日期**: 2026-02-04
- **仓库**: NVIDIA/cutlass
- **版本**: 4.4
- **提交**: [6b3e607](https://github.com/NVIDIA/cutlass/commit/6b3e607b852f1543dc21323155a2ad71473c8642)

## 主要更新

### CuTe DSL 增强
- 支持 CUDA toolkit 13.1
- 新增 GB300 (Blackwell) 架构支持
- 引入 `cute.experimental` 高级可组合层
  - 无片段编程模型
  - 自动 TMA 描述符生成
  - SIMT 复制的自动向量化和谓词
  - 新流水线抽象
  - 新分区操作
  - 设备端 TMA 描述符管理

### 新增示例
- 添加 SM103 批处理 3xFP4 块缩放 GEMM 内核示例
- 新增多个 Blackwell 实验性示例：
  - `dense_gemm.py`
  - `dense_block_scaled_gemm.py`
  - `dense_gemm_2sm.py`
  - `dense_gemm_cute_pipeline.py`
  - `dense_gemm_ptr_array.py`

### 代码改进
- 修复内核调度器中的内存依赖问题
- 增强 `wait_on_dependent_grids()` 网格同步
- 优化内核启动性能

## 问题报告

1. **性能问题**: Blackwell GEMM 循环中频繁调用 `cuTensorMapEncodeTiled` 和 `cudaGetDriverEntryPoint` 导致显著开销
2. **Bug**: 在 fp8 共享内存张量上调用 `cute.print_tensor` 时出现段错误
3. **Bug**: nvidia-cutlass-dsl v4.4.0.dev1 导入错误
4. **问题**: 是否支持 NVIDIA Tesla V100 GPU

## 统计数据
- 总变更: 14,702 行
- 新增: 13,228 行
- 删除: 1,474 行
### flashinfer-ai/flashinfer
## FlashInfer 更新摘要 (2026-02-04)

### 提交

1. **修复 CI 权限问题** ([cdbb2c3](https://github.com/flashinfer-ai/flashinfer/commit/cdbb2c3c73598bbb04a0e2dc2d7cf6ee9e5aeda5))
   - 修复了在 ci-infra runner 上的发布工作流权限错误
   - 移除了 Docker 运行中的 chown 命令和 --user 标志

2. **新增 Fused MoE 支持** ([e284274](https://github.com/flashinfer-ai/flashinfer/commit/e284274e2eb67538bb9c884f2ed9c0143772d3ac))
   - 支持 Fused MoE 非门控 Relu2 NVFP4 & FP8
   - 增加对 Nemotron 的支持
   - 修改了多个核心文件，包括 benchmarks、csrc、flashinfer、include 和 tests

3. **多节点/多 GPU 测试脚本** ([9bf007d](https://github.com/flashinfer-ai/flashinfer/commit/9bf007d7403f0a394fda44abd4170b354cac3f05))
   - 添加/更新了多节点和多 GPU 测试脚本
   - 移除了旧的 Blackwell 内核测试脚本
   - 新增了通用测试工具脚本

4. **修复 CI 测试发现** ([567ded1](https://github.com/flashinfer-ai/flashinfer/commit/567ded1f612c38bd6a921a4a7ce4ecc6db2b9a9e))
   - 将 tests/mamba/test_utils.py 重命名为 tests/mamba/utils.py
   - 修复了 CI 测试发现问题

5. **修复 MLA 基准测试内存带宽计算** ([f84ac1c](https://github.com/flashinfer-ai/flashinfer/commit/f84ac1c97e2e1a4391150ca73971b973ee569e5a))
   - 修正了 benchmarks/bench_trtllm_gen_mla.py 中的内存带宽计算
   - 更新了 benchmarks/routines/attention.py 中的相关代码

6. **优化 Hopper GDN 预填充编译时间** ([6ae5bfe](https://github.com/flashinfer-ai/flashinfer/commit/6ae5bfe6a48c591a171f32888e5de28b9ca0207b))
   - 重构了 Hopper 的 GDN 预填充内核，减少编译时间
   - 修复了文档字符串
   - 添加了新的内核实例文件

### 问题

1. **mxfp4 量化内核性能问题** ([#2496](https://github.com/flashinfer-ai/flashinfer/issues/2496))
   - mxfp4 量化内核性能显著低于 nvfp4 (0.442 TFLOPs/sec vs 5.597 TFLOPs/sec)
   - 建议通过 CuTe DSL 更新 mxfp4 量化内核

2. **XQA 多 GPU 环境问题** ([#2494](https://github.com/flashinfer-ai/flashinfer/issues/2494))
   - XQA 总是在 rank 0 上设置动态共享内存大小
   - 在多 GPU 环境中会导致 CUDA 参数错误

3. **GDN 预填充内核产生 NaN** ([#2490](https://github.com/flashinfer-ai/flashinfer/issues/2490))
   - GDN 预填充内核在 vLLM 中使用时产生 NaN 值
   - 已提供测试用例复现问题

4. **GLM-4.7 NVFP4 准确性问题** ([#2485](https://github.com/flashinfer-ai/flashinfer/issues/2485))
   - 使用 FlashInfer TRTLLM MoE 后端时，GLM-4.7 ModelOpt NVFP4 产生无意义输出
   - 影响多个推理框架 (vLLM 和 SGLang)

5. **RadixTopKKernel_Unified 非法内存访问** ([#2486](https://github.com/flashinfer-ai/flashinfer/issues/2486))
   - top_k_page_table_transform 函数中存在非法内存访问问题
   - 已提供最小测试用例
### Dao-AILab/flash-attention
# Flash-Attention 仓库更新 (2026-02-04)

## 提交

1. **Markus Hoehnerbach** 添加了 flex flash 的简短 README
   - PR: #2231
   - 文件: `flash_attn/cute/README.md`
   - 变更: +26
   - [链接](https://github.com/Dao-AILab/flash-attention/commit/24445c0c177f0455c076b32c41b26eee81c4e7a7)

2. **Jane (Yuan) Xu** 更新了 Torch 版本配置
   - PR: #2155
   - 文件: `hopper/setup.py`
   - 变更: +1 -1
   - [链接](https://github.com/Dao-AILab/flash-attention/commit/ef9e6a644192eb2b90155abe0372542f6d9a27b6)

3. **Driss Guessous** 修复了共享内存竞争问题
   - PR: #2229
   - 文件: 
     - `flash_attn/cute/block_sparse_utils.py` (+1 -1)
     - `flash_attn/cute/flash_fwd_sm100.py` (+3 -3)
   - 变更: +4 -4
   - [链接](https://github.com/Dao-AILab/flash-attention/commit/188643b82d5b06679662028558d802bcd9acfe6c)

## 问题

无新问题报告。
### sgl-project/sglang
# SGLang Newsletter - 2026-02-04

## 主要更新

### 性能优化
- 改进了不同TP大小下MHA模型的KV偏移计算 ([f730c18](https://github.com/sgl-project/sglang/commit/f730c186799d966a62531269ce46178364c85dc3))
- 优化了get_topk_ragged，通过融合get k和k_scale triton内核 ([760ae93](https://github.com/sgl-project/sglang/commit/760ae933bb3878a6897e7e552a746929c29e9d90))
- 将qkvbfg线性融合为一个gemm，f_b g_b融合为批处理gemm ([37c33cc](https://github.com/sgl-project/sglang/commit/37c33cc0aa6213fd4abcfb40c3e1d71dde484295))

### 模型支持
- 新增支持interns1-pro模型 ([3e7ecb7](https://github.com/sgl-project/sglang/commit/3e7ecb78a60f8e1d889cfe25c88006577783d903))
- 为Qwen3-Coder-Next-FP8添加了H100 TP=2的MoE融合配置 ([efbf395](https://github.com/sgl-project/sglang/commit/efbf39583e7a716e0204b071db687145392e41b2))

### 文档更新
- 记录了SGLANG_MOONCAKE_CUSTOM_MEM_POOL和支持的值 ([c8212b9](https://github.com/sgl-project/sglang/commit/c8212b9fac11d7ad3a2aa088946e1a815a618a97))
- 支持Markdown/Notebook友好的文档导出 ([669a9bd](https://github.com/sgl-project/sglang/commit/669a9bd1809a344fae5b9d962d2cd6f842cb50c1))

### 多模态/扩散模型增强
- 添加了门控残差layernorm scale shift和layernorm scale shift内核融合 ([4739f2e](https://github.com/sgl-project/sglang/commit/4739f2e8d5732f7464d1af75d31b4d44c61783b6))
- 重构了model_stages到stages文件夹 ([36a3e78](https://github.com/sgl-project/sglang/commit/36a3e78af93b93bfa8e6a7c654c935e5f90a63f1))
- 清理了MOVA代码 ([0c9a0ad](https://github.com/sgl-project/sglang/commit/0c9a0adc5329d393435097337fa113174363299e))

## 问题跟踪

### 文档请求
- 更新推测解码文档 ([#18268](https://github.com/sgl-project/sglang/issues/18268))
- 添加分段CUDA图文档 ([#18267](https://github.com/sgl-project/sglang/issues/18267))
- 解释如何使用nvidia modelopt检查点 ([#18261](https://github.com/sgl-project/sglang/issues/18261))

### 功能请求
- 添加π/π-FAST VLA模型推理支持 ([#18266](https://github.com/sgl-project/sglang/issues/18266))
- 集成FA4 SM90解码 ([#18265](https://github.com/sgl-project/sglang/issues/18265))
- [ModelOpt] 加载器不支持MXFP8 ([#18258](https://github.com/sgl-project/sglang/issues/18258))

### Bug报告
- [AITER attention] OOM错误，因为已解决的max_total_num_tokens未考虑注意力后端的内存需求 ([#18262](https://github.com/sgl-project/sglang/issues/18262))
### vllm-project/vllm
## VLLM Updates - 2026-02-04

### 新功能
- **ColBERT late interaction model support** ([#33686](https://github.com/vllm-project/vllm/pull/33686)): 添加了 ColBERT 模型支持，涉及 977 次代码添加，包括新的模型实现、测试和文档。
- **Add transcription support for Qwen3-Omni** ([#29828](https://github.com/vllm-project/vllm/pull/29828)): 为 Qwen3-Omni 模型添加了转录功能。

### 性能优化
- **Optimize chat completion streaming performance** ([#33782](https://github.com/vllm-project/vllm/pull/33782)): 优化了聊天补全流式性能，修改了 `vllm/entrypoints/openai/chat_completion/serving.py`。
- **Optimize spec decoding + async scheduling** ([#33612](https://github.com/vllm-project/vllm/pull/33612)): 通过优化 spec decoding 和异步调度实现了 1.5% 的吞吐量提升。
- **Change GDN Attention State Layout** ([#33291](https://github.com/vllm-project/vllm/pull/33291)): 将注意力状态布局从 [N, HV, K, V] 优化为 [N, HV, V, K] 以提高性能。
- **Implement zero-copy GQA for multimodal and CPU** ([#33732](https://github.com/vllm-project/vllm/pull/33732)): 为多模态模型和 CPU 执行添加了零拷贝 GQA 支持。

### Bug 修复
- **Fix DeepSeek R1 with CUTLASS MLA** ([#33637](https://github.com/vllm-project/vllm/pull/33637)): 修复了 DeepSeek R1 模型在 B200 上使用 CUTLASS MLA 的兼容性问题。
- **Disable TRTLLM attention when KV transfer is enabled** ([#33192](https://github.com/vllm-project/vllm/pull/33192)): 修复了与 KV transfer 的兼容性问题。
- **Fix torchrun PP broadcast deadlock with async scheduling** ([#33701](https://github.com/vllm-project/vllm/pull/33701)): 解决了异步调度中管道并行性的死锁问题。
- **Fix `normalize` still being passed to `PoolerConfig`** ([#33794](https://github.com/vllm-project/vllm/pull/33794)): 修正了池化配置中的参数传递。

### 模型支持
- **Support RotaryEmbedding CustomOp for gpt-oss** ([#33800](https://github.com/vllm-project/vllm/pull/33800)): 为 gpt-oss 模型添加了 RotaryEmbedding CustomOp 支持。
- **Fix audio-in-video support for Qwen2.5-Omni and Qwen3-Omni** ([#33605](https://github.com/vllm-project/vllm/pull/33605)): 修复了多模态模型中的音频处理问题。

### 基础设施
- **Reduce e2e fusion test time** ([#33293](https://github.com/vllm-project/vllm/pull/33293)): 大幅减少了 torch.compile 融合测试的执行时间。
- **Unify Ray device visibility handling across CUDA and ROCm** ([#33308](https://github.com/vllm-project/vllm/pull/33308)): 改进了 Ray 分布式设置中的设备可见性处理。

### 问题
- **Docker issues with Qwen3-Next** ([#33833](https://github.com/vllm-project/vllm/issues/33833)): 在运行特定配置的 Qwen3-Next 时，较新的 Docker 映像存在问题。问题正文被截断。
- **Deepseek V3.2 Benchmark failure** ([#33831](https://github.com/vllm-project/vllm/issues/33831)): 基准测试失败，错误为 "TypeError: argument 'tokens': 'NoneType' object"。问题正文被截断。
- **Mistral3 offline multimodal inference example failing** ([#33828](https://github.com/vllm-project/vllm/issues/33828)): 示例因提示占位符错误而失败。问题正文被截断。
- **Step3p5ForCausalLM fails with pipeline parallelism** ([#33823](https://github.com/vllm-project/vllm/issues/33823)): 使用管道并行性时模型失败。问题正文被截断。
- **Quantized Models Test CI failure** ([#33816](https://github.com/vllm-project/vllm/issues/33816)): 量化模型的 CI 测试失败。问题正文被截断。
- **llm.score() fails on batched multimodal input** ([#33813](https://github.com/vllm-project/vllm/issues/33813)): 处理批量多模态输入时 API 失败。问题正文被截断。
### NVIDIA/cutile-python
昨日无更新。

## 总结
- Diff 内容已截断以满足 prompt 预算。
- Issue 内容已截断以满足 prompt 预算。
- OpenRouter repo summarize failed for NVIDIA/cutlass with openai/gpt-oss-120b:free: OpenRouter 429 Too Many Requests: {"error":{"message":"Provider returned error","code":429,"metadata":{"raw":"openai/gpt-oss-120b:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations","provider_name":"OpenInference","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
- OpenRouter repo summarize failed for flashinfer-ai/flashinfer with openai/gpt-oss-120b:free: OpenRouter 429 Too Many Requests: {"error":{"message":"Provider returned error","code":429,"metadata":{"raw":"openai/gpt-oss-120b:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations","provider_name":"OpenInference","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
- OpenRouter repo summarize failed for Dao-AILab/flash-attention with openai/gpt-oss-120b:free: OpenRouter 429 Too Many Requests: {"error":{"message":"Provider returned error","code":429,"metadata":{"raw":"openai/gpt-oss-120b:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations","provider_name":"OpenInference","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
- OpenRouter repo summarize failed for sgl-project/sglang with openai/gpt-oss-120b:free: OpenRouter 429 Too Many Requests: {"error":{"message":"Provider returned error","code":429,"metadata":{"raw":"openai/gpt-oss-120b:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations","provider_name":"OpenInference","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
- OpenRouter repo summarize failed for vllm-project/vllm with openai/gpt-oss-120b:free: OpenRouter 429 Too Many Requests: {"error":{"message":"Provider returned error","code":429,"metadata":{"raw":"openai/gpt-oss-120b:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations","provider_name":"OpenInference","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
- OpenRouter global summarize failed for openai/gpt-oss-120b:free: OpenRouter 429 Too Many Requests: {"error":{"message":"Provider returned error","code":429,"metadata":{"raw":"openai/gpt-oss-120b:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations","provider_name":"OpenInference","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
