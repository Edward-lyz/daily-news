今日的 AI Infra 的新闻如下。

## 摘要
本期各项目聚焦于性能优化与兼容性提升。**NVIDIA/cutlass** 仍缺少近期提交记录，仅报告了 GPU 示例崩溃和全局内存张量创建的疑问。**flashinfer** 通过版本升级至 0.6.3、扩展非 DeepSeekV3 路由检查、强化 NO_SMEM 对齐以及加入 RoPE 与采样基准，显著提升了多路复用与数值稳定性。**flash‑attention** 将主线标记为 v3.0.0 稳定版并修复了 head_dim 限制的 KV 阶段逻辑。**sglang** 本周提交超过二十项，包括跨容器 HF 下载 race 条件修复、NPU 量化最佳实践、GPTQ‑Marlin JIT 内核迁移、RoPE JIT 实现、XPU MoE 支持以及文档与 CI 改进。**vllm** 则持续扩展模型支持（Voyage‑4‑Nano、MXFP4 MoE、s390x BF16）、优化 Docker XPU 镜像、加入 OpenTelemetry 跟踪、完善 FP8、NVFP4 与 LoRA 的量化路径，并实现统一并行草稿框架和 NCCL 权重同步。**cutile‑python** 仍未有代码提交，仅报告了 B200 卡死、分支类型推断回归以及 CUDA/PTX 注入的需求。

## 具体内容分析
### deepseek-ai/DeepGEMM
昨日无更新。
### deepseek-ai/FlashMLA
昨日无更新。
### NVIDIA/cutlass
提交  
- 暂无近期提交记录  

Issues  

- **[BUG] Cute example crashes**  
  - 作者: **bikshand**，创建于: **2026‑02‑05 20:31 UTC**  
  - 模块/文件: `examples/cute/tutorial/blackwell/cute_tutorial_02_mma_tma_sm100.cpp`  
  - 简要描述: 在 B200 (SM100) GPU 上运行示例 `cute_tutorial_02_mma_tma_sm100` 时抛出 `cudaErrorNoKernelImageForDevice`，导致程序异常终止。  
  - 关联 PR/Commit: 暂无对应 PR 或 commit  

- **[QST] Is there a way to create a gmem cute.tensor in @cute.jit ?**  
  - 作者: **whatdhack**，创建于: **2026‑02‑05 04:17 UTC**  
  - 模块/文件: 未指定（涉及 `cute.jit` 与全局内存分配器）  
  - 简要描述: 询问是否存在类似 `utils.SmemAllocator()` 的全局内存分配器，用于在 `cute.jit` 中创建 `gmem` tensor。  
  - 关联 PR/Commit: 暂无对应 PR 或 commit
### flashinfer-ai/flashinfer
**提交**  

- `d0886ce` – [PR #2497](https://github.com/flashinfer-ai/flashinfer/pull/2497)  
  - **变更**：`version.txt`  
    ```diff
    -0.6.2
    +0.6.3
    ```
  - **说明**：版本号从 0.6.2 升级至 0.6.3。

- `1e9b237` – [PR #2502](https://github.com/flashinfer-ai/flashinfer/pull/2502)  
  - **变更**：`csrc/trtllm_fused_moe_kernel_launcher.cu`（共 19 行改动，patch 被截断）  
    ```cpp
    // 新增对非 DeepSeekV3 路由的检查
    if (static_cast<RoutingMethodType>(routing_method_type) != RoutingMethodType::DeepSeekV3) {
      TVM_FFI_ICHECK(args->n_group <= 1) << "Current routing kernel (no groups) only supports n_group <= 1";
      TVM_FFI_ICHECK(args->topk_group <= 1) << "Current routing kernel (no groups) only supports topk_group <= 1";
    }
    // DeepSeekV3 路由的额外校验
    if (static_cast<RoutingMethodType>(routing_method_type) == RoutingMethodType::DeepSeekV3) {
      TVM_FFI_ICHECK(args->n_group != 0) << "n_group should not be zero for DeepSeekV3 routing";
      TVM_FFI_ICHECK(args->topk_group != 0) << "if n_group is given, topk_group must be given";
      TVM_FFI_ICHECK_EQ(args->num_experts % args->n_group, 0) << "num_experts must be divisible by n_group";
    }
    ```
  - **说明**：放宽了对非 DeepSeekV3 路由的限制，同时为 DeepSeekV3 添加了必需的参数检查。

- `8655234` – [PR #2495](https://github.com/flashinfer-ai/flashinfer/pull/2495)  
  - **变更**：`csrc/nv_internal/tensorrt_llm/kernels/cutlass_kernels/moe_gemm/moe_gemm_template_dispatch.h`（共 18 行改动，patch 被截断）  
    ```cpp
    // 对 NO_SMEM epilogue schedule 添加对输出 N 对齐的检查
    if (inputs.gemm_config.epilogue_schedule == cutlass_extensions::EpilogueScheduleType::NO_SMEM) {
      TLLM_CHECK_WITH_INFO(
          inputs.n % (256 / cutlass::sizeof_bits<OutputType>::value) == 0,
          "Output N %ld does not meet minimum alignment requirements for NO_SMEM epilogue %d",
          (long)inputs.n, (int)(256 / cutlass::sizeof_bits<OutputType>::value));
      // 进一步检查...
    }
    ```
  - **说明**：在自动调优阶段确保 NO_SMEM 调度的输出维度满足 256‑bit 对齐要求，防止非法 tactic 被选中。

- `1748eb5` – [PR #2484](https://github.com/flashinfer-ai/flashinfer/pull/2484)  
  - **变更**：`benchmarks/README.md`（90 行改动）  
    ```diff
    - Benchmark FlashInfer's Attention, GEMM, MOE, Norm, and Quantization API ...
    + Benchmark FlashInfer's Attention, GEMM, MOE, Norm, Quantization, Sampling, and RoPE API ...
    ```
  - **变更**：`benchmarks/flashinfer_benchmark.py`（20 行改动）  
    ```python
    elif args.routine in benchmark_apis["sampling"]:
        from routines.sampling import run_sampling_test
        res = run_sampling_test(args)
    elif args.routine in benchmark_apis["rope"]:
        from routines.rope import run_rope_test
        res = run_rope_test(args)
    ```
  - **变更**：`benchmarks/routines/flashinfer_benchmark_utils.py`（304 行改动）  
    - 新增 `sampling`、`rope` 参数列表；删除/合并部分旧参数。  
  - **新增**：`benchmarks/routines/rope.py`（1710 行）  
    - 实现 RoPE（旋转位置编码）基准测试，包括 `apply_rope`、`apply_rotary_emb` 等 API。  
  - **新增**：`benchmarks/routines/sampling.py`（2025 行）  
    - 实现采样基准测试，支持 `top_k`、`top_p`、`temperature` 等采样策略。  
  - **说明**：扩展基准套件，加入采样与 RoPE 两大新模块，并相应更新 CLI 与文档。

- `cdbb2c3` – 无对应 PR（仅提交）  
  - **变更**：`.github/workflows/nightly-release.yml` 与 `.github/workflows/release.yml`（各 8 行删除）  
    ```diff
    -          chown -R $(id -u):$(id -g) ${{ github.workspace }}
    -          chown -R $(id -u):$(id -g) ${{ github.workspace }}/ci-cache
    -            --user $(id -u):$(id -g) \
    ```
  - **说明**：移除在 CI 容器中手动更改文件所有者的步骤，避免权限错误。

---

**Issues**  

- [#2504](https://github.com/flashinfer-ai/flashinfer/issues/2504) – Autotune 在小 batch size 下为 `trtllm_fp4_block_scale_moe` 选取次优 tactic。  
- [#2501](https://github.com/flashinfer-ai/flashinfer/issues/2501) – Autotune 在 `cutlass_fused_moe` 的 W4A8 配置下失败，仅在开启 autotune 时出现。
### Dao-AILab/flash-attention
提交  

- **hdim 192 smem fix** – [f1284cf](https://github.com/Dao-AILab/flash-attention/commit/f1284cff5d2b2ad4160ceefaf096a800502d16fd) – 修正 `kv_stage` 条件，加入 `head_dim` 限制。  
  ```python
  self.kv_stage = (
      4
      if (self.q_dtype.width == 8 or self.q_stage == 1)
      and self.head_dim_padded <= 128
      and self.head_dim_v_padded <= 128
      else 3
  )
  ```

- **Mark current main version as v3.0.0 stable** – [e2743ab](https://github.com/Dao-AILab/flash-attention/commit/e2743ab5b3803bb672b16437ba98a3b1d4576c50) – 将 `__version__` 从 `3.0.0.b1` 更新为正式版 `3.0.0`。

- **short readme for flex flash** – [24445c0](https://github.com/Dao-AILab/flash-attention/commit/24445c0c177f0455c076b32c41b26eee81c4e7a7) – 为 `flash_attn/cute` 添加开发安装、测试、Lint 指南的 README。  
  ```markdown
  # Flash Attention CUTE
  ## Development Installation
  1. git clone ...
  2. pip install -e "./cute[dev]"
  ## Running Tests
  pytest tests/cute/
  ## Linting
  ruff check flash_attn/cute/
  ```

Issues  

- **FA4 Incompatible with Cutlass 4.3.5** – 作者: S1ro1 – FA4 升级到 Cutlass 4.3.5 后缺少 `cutlass.utils.ampere_utils`，导致导入错误。建议降级至 4.2.1。  
  链接: https://github.com/Dao-AILab/flash-attention/issues/2234  

- **Does FA4 support MLA?** – 作者: cyk2018 – 在 MLA 场景下，prefill 正常但 decode 失败，询问 FA4 是否支持不同的 q/k 与 v 维度组合。  
  链接: https://github.com/Dao-AILab/flash-attention/issues/2233
### sgl-project/sglang
## 本期提交概览  

| 短 SHA | 作者 | 日期 | 提交信息 | 关联 PR（Commit URL） | 关键文件/模块 | 代码片段（若有） |
|--------|------|------|----------|----------------------|--------------|----------------|
| `d0c39bc` | Alison Shao | 2026‑02‑06 | Fix cross‑container HF download race condition in CI (#18328) | https://github.com/sgl-project/sglang/commit/d0c39bc2193c3e28eee94018340d32029541f0a3 | `python/sglang/srt/model_loader/ci_weight_validation.py` | 新增 `import time` 与 `_cleanup_incomplete_blobs` 辅助函数，用于清理 `.incomplete` 文件（diff 已截断）。 |
| `92b8bd6` | amote‑i | 2026‑02‑06 | fix npu best practice (#18330) | https://github.com/sgl-project/sglang/commit/92b8bd68333ef9bca081201cca474755e5e42a98 | `docs/platforms/ascend_npu_best_practice.md` | 将 Qwen3‑32B 的量化方式从 **W8A8** 改为 **BF16**（表格行更新）。 |
| `aa390d2` | Linyu Wu | 2026‑02‑06 | [Kernel] Migrate GPTQ‑Marlin GEMM kernel to JIT (#18067) | https://github.com/sgl-project/sglang/commit/aa390d276279360899047567fe6892e28abed7cc | 多个新增文件（`jit_kernel/benchmark/bench_gptq_marlin.py`、`jit_kernel/csrc/gemm/marlin/*`、`jit_kernel/gptq_marlin.py`、`jit_kernel/tests/test_gptq_marlin.py` 等） | 添加 JIT 版 GPTQ‑Marlin GEMM 实现及基准测试脚本，首次引入 `scalar_type.hpp` 与 `marlin_template.h`（diff 已截断）。 |
| `ef1d0ea` | shuwenn | 2026‑02‑05 | [Doc] add a summary section for spec decode document (#18323) | https://github.com/sgl-project/sglang/commit/ef1d0ea8854155983796ba200addd4c6db0e2e41 | `docs/advanced_features/speculative_decoding.ipynb` | 在 notebook 开头新增 “Summary” 小节（diff 已截断）。 |
| `8b21dd4` | shuwenn | 2026‑02‑05 | [Doc] refine spec decode docs for SpecV2/STANDALONE/NGRAM (#18321) | https://github.com/sgl-project/sglang/commit/8b21dd4b774a4fd79662d11cfa3f0478bfb33f3f | 同上 | 扩充 SpecV2、Standalone、N‑gram 的使用说明（diff 已截断）。 |
| `6a4b81e` | aaaandychen | 2026‑02‑05 | Refactor(qwen3‑vl) optimize position encoding interpolation (#16781) | https://github.com/sgl-project/sglang/commit/6a4b81e2d9fc0da998d296b86fbacc7e1594988d | `python/sglang/srt/models/qwen3_vl.py`、`test/srt/test_embed_interpolate_unittest.py` | 改进位置编码插值逻辑，新增单元测试（diff 已截断）。 |
| `d22163e` | Alison Shao | 2026‑02‑05 | Fix flaky test_frequency_penalty_reduces_word_repetition by using deterministic seeds (#18285) | https://github.com/sgl-project/sglang/commit/d22163eb8c02169f5e39de1f5f3d09023c1e3c25 | `test/registered/sampling/test_penalty.py` | 为随机种子加入固定值，确保测试可复现（diff 已截断）。 |
| `498d8d0` | ovidiusm | 2026‑02‑05 | NixlKVManager optimizations (#17654) | https://github.com/sgl-project/sglang/commit/498d8d068096dca5f17f28d54c5464706ec22d7c | `python/sglang/srt/disaggregation/nixl/conn.py` | 大幅删减冗余代码，提升 KV 管理器吞吐（+80 / -60 行）。 |
| `b639779` | wxy | 2026‑02‑05 | [diffusion] feat: allow T5's TP Group to reuse the transformer’s SP Group (#17818) | https://github.com/sgl-project/sglang/commit/b639779dd89aea042d4a274f79190126cf841330 | 多文件涉及 `multimodal_gen/configs/*`、`runtime/*`、`layers/*` | 统一 TP 与 SP 分组配置，简化分布式部署（diff 已截断）。 |
| `3f32a58` | Glen Liu | 2026‑02‑05 | throw error if got adapter with added_tokens (#18046) | https://github.com/sgl-project/sglang/commit/3f32a5831d329320972403931819ca037b071830 | `python/sglang/srt/lora/lora_manager.py` | 在加载 LoRA 适配器时检测 `added_tokens`，若存在则抛异常（+4 行）。 |
| `2eb4359` | pansicheng | 2026‑02‑05 | [Kernel] Add JIT apply_rope_with_cos_sin_cache_inplace (#18155) | https://github.com/sgl-project/sglang/commit/2eb4359ada96d94a9f4047afec0fae76722766f7 | `jit_kernel/csrc/elementwise/rope.cuh`、`jit_kernel/rope.py`、`jit_kernel/tests/test_rope.py`、`srt/layers/rotary_embedding.py` | 实现 **rope** 的原位计算 JIT 版本，提升旋转嵌入效率（diff 已截断）。 |
| `4aa03d9` | 陈一沫 | 2026‑02‑05 | [diffusion] fix: fix accuracy bug caused by #14717 (#18296) | https://github.com/sgl-project/sglang/commit/4aa03d91fd86da1dbd0272e7e2cee7f66d093383 | `jit_kernel/diffusion/cutedsl/common/reduce.py`、`jit_kernel/tests/test_fused_norm_scale_shift.py`、`multimodal_gen/runtime/layers/layernorm.py` | 修正归约算子导致的精度回退（+3 / -3 行）。 |
| `8f8c172` | ishandhanani | 2026‑02‑05 | docker: add patch to increase GPU deepep timeout (#18298) | https://github.com/sgl-project/sglang/commit/8f8c1724ae2f4dd4ba40b18d246766da3545e4eb | `docker/Dockerfile` | 将 `DEEP_EP_TIMEOUT` 参数提升 3 秒（+3 行）。 |
| `afae4c7` | Shangming Cai | 2026‑02‑05 | [PD] Minor code cleanup for mooncake backend (#18279) | https://github.com/sgl-project/sglang/commit/afae4c7178f57a60f45c0685f41e1ac42ae4b607 | `srt/disaggregation/mooncake/conn.py` | 代码格式化与冗余删除（+27 / -26 行）。 |
| `079fc8f` | zhangheng | 2026‑02‑05 | [piecewise graph]: support MiniMax‑M2 (#18217) | https://github.com/sgl-project/sglang/commit/079fc8f3c591a43316d98fae6f108ce03d0eeeb3 | `srt/layers/quantization/fp8_kernel.py`、`srt/models/minimax_m2.py` | 为 MiniMax‑M2 添加 FP8 支持（+5 行）。 |
| `3f1df32` | danielafrimi | 2026‑02‑05 | [FIX] Always support TP > 4 for FP4 Gemm (#17300) | https://github.com/sgl-project/sglang/commit/3f1df322f9df60e228a567bbd2fa3064d4b5f269 | `srt/layers/quantization/modelopt_quant.py` | 扩展 FP4 GEMM 的 TP 上限，兼容 >4 的并行度（+154 / -6 行）。 |
| `368936a` | Meng, Hengyu | 2026‑02‑05 | [XPU] Integrate MoE and minor improvements in XPU attention backend (#13561) | https://github.com/sgl-project/sglang/commit/368936a62bdd533cdfe919d8a055c73cc2e34712 | 多文件涉及 `layers/moe/fused_moe_triton/*`、`layers/quantization/unquant.py`、`utils/common.py`、`test/srt/xpu/test_deepseek_ocr.py` | XPU 后端加入 MoE 支持，优化注意力实现（diff 已截断）。 |
| `dff3ba2` | Xiaoyu Zhang | 2026‑02‑05 | [Diffusion] Support layerwise offload for mova (#18272) | https://github.com/sgl-project/sglang/commit/dff3ba202ad034630a8faa53ae6550a06b981e90 | `multimodal_gen/runtime/server_args.py` | 新增 `--layerwise-offload` 参数，支持分层卸载（+4 / -3 行）。 |
| `c910829` | Alison Shao | 2026‑02‑05 | Fix test_return_routed_experts to use response‑level sglext (#18274) | https://github.com/sgl-project/sglang/commit/c910829708c7b71f82e646393c6503b17501e396 | `test/registered/rl/test_return_routed_experts.py` | 调整测试使用响应级别的 `sglext`，确保路由专家返回正确（+6 / -8 行）。 |
| `e616d35` | Kun Lin | 2026‑02‑05 | Support Markdown/Notebook‑Friendly Documentation Export for Downstream Integration (#18278) | https://github.com/sgl-project/sglang/commit/e616d3584737686f6d221ce3f21c67b98a936827 | `docs/Makefile` | 新增 `make md` 目标，将 `.ipynb` 转为 Markdown（+9 / -3 行）。 |
| `de6a032` | rinbaro | 2026‑02‑05 | [docs] fix misspellings & typos (#18276) | https://github.com/sgl-project/sglang/commit/de6a03260f59fd33a9eeb8f67e7e6e2cf235a70f | 多个文档文件（如 `docs/advanced_features/dp_for_multi_modal_encoder.md` 等） | 统一拼写错误，提升文档可读性（共 46 处修改）。 |
| `c8212b9` | Teng Ma | 2026‑02‑05 | [PD] doc: Document SGLANG_MOONCAKE_CUSTOM_MEM_POOL and supported values (#18259) | https://github.com/sgl-project/sglang/commit/c8212b9fac11d7ad3a2aa088946e1a815a618a97 | `docs/advanced_features/pd_disaggregation.md`、`docs/references/environment_variables.md` | 新增环境变量说明，列举可选内存池实现。 |
| `f730c18` | Ch3ngY1 | 2026‑02‑05 | [PD] improve kv offset calculation for MHA model with different tp size (#18163) | https://github.com/sgl-project/sglang/commit/f730c186799d966a62531269ce46178364c85dc3 | `srt/disaggregation/mooncake/conn.py` | 重写 KV 偏移计算逻辑，兼容不同 TP 大小（+31 / -75 行）。 |
| `f218234` | Mick | 2026‑02‑05 | [diffusion] chore: prohibit Chinese characters usage (#18249) | https://github.com/sgl-project/sglang/commit/f218234e4f19323d09f10c03505a89a82d52ebd4 | `.pre-commit-config.yaml`、`multimodal_gen/runtime/models/encoders/t5.py` | 在 CI 中加入中文字符检测，防止代码库出现非 ASCII 文本（+8 行）。 |
| `599c5f4` | yinghui | 2026‑02‑05 | fix kimi k2.5's moe gemm config init (#18064) | https://github.com/sgl-project/sglang/commit/599c5f4922579742a0c65a4c2fb4503dd63f7ae3 | `srt/managers/scheduler.py` | 修正 Kimi‑K2.5 MoE GEMM 配置初始化的默认值（+6 / -1 行）。 |

> **说明**：部分提交的 `patch_truncated` 为 `true`，表示完整 diff 未展示，仅提供关键改动概览。

---

## 本期 Issue 关注  

| Issue 编号 | 标题 | 链接 | 简要概述 |
|------------|------|------|----------|
| #18313 | **Feature**: Change new allreduce_fusion API from Flashinfer | https://github.com/sgl-project/sglang/issues/18313 | 计划将 `flashinfer.comm.trtllm_allreduce_fusion` 替换为 `flashinfer.comm.allreduce_fusion`，需同步更新调用点。 |
| #18305 | **Feedback**: Great project! Could use browser automation for easier deployment | https://github.com/sgl-project/sglang/issues/18305 | 用户建议加入浏览器自动化（如 Selenium）以简化部署与测试流程。 |
| #18300 | **Bug**: `ValueError: token_to_kv_pool_allocator` memory leak detected during batch inference with Qwen3‑Coder‑Next‑FP8 | https://github.com/sgl-project/sglang/issues/18300 | 在双 RTX 4090 环境下运行 FP8 模型时触发 KV 池内存泄漏检测，导致进程异常退出。 |
| #18299 | **Bug**: qwen3 omni failed to load tokenizer | https://github.com/sgl-project/sglang/issues/18299 | 下载的 Qwen3‑Omni‑30B‑A3B‑Instruct 缺失 `tokenizer.json` 等必备文件，导致加载失败。 |
| #18290 | **Bug**: RuntimeError in `flash_attn_with_kvcache`: query and key must have the same dtype with FP8 models | https://github.com/sgl-project/sglang/issues/18290 | FP8 模型在开启 CUDA Graph 时出现 Q/K dtype 不匹配错误，影响多模型部署。 |
| #18286 | **Roadmap**: SGLang‑Diffusion (Q1 2026) | https://github.com/sgl-project/sglang/issues/18286 | 规划包括 CUDA Graph 小模型加速、并行 VAE 解码、Norm/ROPE 融合、量化路线（E4M3/E5M2/INT8/MXFP8 等）以及稀疏注意力后端等。 |
| #18277 | **Bug**: [MXFP8 Online] AssertionError: n=64 must be divisible by 128 | https://github.com/sgl-project/sglang/issues/18277 | 在 MXFP8 在线推理时触发断言，根因是内部块大小不匹配导致的维度错误。 |
| #18276 | **Docs**: fix misspellings & typos | https://github.com/sgl-project/sglang/issues/18276 | 文档拼写与排版错误的集中修复，提升阅读体验。 |
| #18259 | **Docs**: Document `SGLANG_MOONCAKE_CUSTOM_MEM_POOL` and supported values | https://github.com/sgl-project/sglang/issues/18259
### vllm-project/vllm
**提交**  

- **[9655256] Onboard voyage-4-nano**  
  <https://github.com/vllm-project/vllm/commit/965525667b70dc23463d57295dce792eba1ac452>  
  将 Voyage‑4‑Nano 模型加入模型注册表并补充测试。  
  - `docs/models/supported_models.md`（+1 行）  
  - `tests/models/language/pooling_mteb_test/test_voyage.py`（新增 56 行）  
  - `vllm/model_executor/models/voyage.py`（新增 130 行）  
  - 其他配置文件 (`vllm/config/model.py`, `vllm/model_executor/models/config.py` 等) 进行小幅调整。  

- **[6550815] Replace pip in docker.xpu with uv pip**  
  <https://github.com/vllm-project/vllm/commit/6550815c3ad5fc20e6944483dd8a7a47c18b7c7d>  
  Dockerfile.xpu 改用 `uv pip` 加速依赖安装。  
  - `docker/Dockerfile.xpu`（+46 / -28 行）  

- **[7439e4f] Add mxfp4 moe model support**  
  <https://github.com/vllm-project/vllm/commit/7439e4f41b1f877e088d1b8c9ce4f2847c59423f>  
  为 MoE 引入 MXFP4 量化实现。  
  - `vllm/model_executor/layers/quantization/mxfp4.py`（+53 / -31 行）  

- **[ac04dd3] Add BF16 Kernel type for s390x**  
  <https://github.com/vllm-project/vllm/commit/ac04dd374f996f8df960933fa076bb5ea53c0c2a>  
  在 s390x 平台新增 BF16 解码内核。  
  - `csrc/cpu/mla_decode.cpp`（+9 行）  

- **[035a6cb] Update code for encoder‑decoder models**  
  <https://github.com/vllm-project/vllm/commit/035a6cb09a3f7076d2e79db1af3070325230bb59>  
  修复 encoder‑decoder 模型的输入/调度兼容性。  
  - `vllm/multimodal/inputs.py`（+1 / -1 行）  
  - `vllm/v1/core/sched/scheduler.py`（+8 / -2 行）  

- **[a32cb49] Early‑fail tokenization guard for user requests**  
  <https://github.com/vllm-project/vllm/commit/a32cb49b60688fb64a6d3d7f86378b4d2fad06e6>  
  在前端加入 tokenization 预检查，提升错误响应速度。  
  - `vllm/renderers/params.py`（+53 / -21 行）  
  - 多个 tokenizer 实现文件均有细微改动（`deepseek_v32.py`, `grok2.py`, `hf.py`, `mistral.py`, `protocol.py`）  
  - `tests/renderers/test_completions.py`（+230 / -181 行）  

- **[20d7454] Make flash_attn import optional in MLA attention**  
  <https://github.com/vllm-project/vllm/commit/20d7454c9bb0c2de7f59863f5030e5f494cab178>  
  在 MLA 注意力实现中将 flash_attn 设为可选依赖。  
  - `vllm/model_executor/layers/attention/mla_attention.py`（+19 / -3 行）  

- **[5819ca8] Add reo analytics**  
  <https://github.com/vllm-project/vllm/commit/5819ca8944af4f7dcbac3c6b73179f760e05910d>  
  文档站点加入 REO 分析脚本。  
  - `docs/mkdocs/javascript/reo.js`（+3 行）  
  - `mkdocs.yaml`（+1 行）  

- **[79028d4] Disable clean_logits in deepgemm fp8_mqa_logits kernel**  
  <https://github.com/vllm-project/vllm/commit/79028d438859162841d35bbf2a91eaa8236733fa>  
  关闭 deepgemm FP8 MQA logits 的 clean 操作以提升性能。  
  - `vllm/utils/deep_gemm.py`（+8 / -2 行）  
  - 相关测试文件均作相应修改。  

- **[325ab6b] OTEL tracing during loading**  
  <https://github.com/vllm-project/vllm/commit/325ab6b0a896f4b483b26d99afdfc6d75ea61074>  
  为模型加载过程加入 OpenTelemetry 跟踪。  
  - 新增 `tests/tracing/*` 系列测试  
  - `vllm/config/observability.py`、`vllm/compilation/backends.py` 等文件小幅改动  
  - 移除旧的 `vllm/tracing.py`（-135 行），新增 `vllm/tracing/__init__.py`（+157 行）  

- **[91a07ff] Fix DeepSeek v3.2 tokenizer outputting None**  
  <https://github.com/vllm-project/vllm/commit/91a07ff6187e7308794b2a4863ab9e1f821ed464>  
  修复 DeepSeek‑v3.2 tokenizer 在特定情况下返回 `None` 的 bug。  
  - `vllm/tokenizers/detokenizer_utils.py`（+4 行）  

- **[d5c4800] Adds padding and perf improvements to wvSplitK_fp8**  
  <https://github.com/vllm-project/vllm/commit/d5c4800112c12bbcd4955858ef1b415c16ae16e7>  
  优化 ROCm 上的 FP8 skinny GEMM，实现 padding 与性能提升。  
  - `csrc/rocm/skinny_gemms.cu`（+126 / -174 行）  
  - 相关测试 `tests/kernels/quantization/test_rocm_skinny_gemms.py`（+40 / -52 行）  

- **[42d5d70] Sort safetensors files to ensure deterministic loading order**  
  <https://github.com/vllm-project/vllm/commit/42d5d705f93b254179e062003e8504fbe04f1b30>  
  加载 safetensors 时进行文件排序，保证确定性。  
  - `vllm/model_executor/model_loader/weight_utils.py`（+11 / -2 行）  

- **[116880a] Make MM batching more robust**  
  <https://github.com/vllm-project/vllm/commit/116880a5a0af3a226e29f4716c484a4cc8422fc1>  
  改进多模态模型的批处理鲁棒性。  
  - 大量多模态测试文件新增/修改（`tests/multimodal/*`）  
  - `vllm/model_executor/models/terratorch.py`、`vllm/multimodal/*` 等核心文件均有细微改动。  

- **[4145e50] Fix DSV3.2 NVFP4**  
  <https://github.com/vllm-project/vllm/commit/4145e50d854e3182c22bad99ec011c283b9c493f>  
  修复 DSV3.2 NVFP4 权重加载错误。  
  - `vllm/model_executor/layers/attention/mla_attention.py`（+4 / -2 行）  

- **[20f5d18] Rename `translations` to `speech_to_text`**  
  <https://github.com/vllm-project/vllm/commit/20f5d185a6f570b74ab403577cd4eabe44d14496>  
  将 OpenAI 接口中的 `translations` 重命名为 `speech_to_text`。  
  - 相关 API 路由文件均完成重命名（`vllm/entrypoints/openai/speech_to_text/*`）  
  - `vllm/entrypoints/openai/api_server.py` 进行对应引用更新。  

- **[1887acc] Fix tokenizer test for renamed attr on Transformers v5**  
  <https://github.com/vllm-project/vllm/commit/1887acca9e2ceacea8f7b1770bd0a0fd9b6a3b02>  
  更新测试以适配 Transformers 5.x 的属性改名。  
  - `tests/entrypoints/openai/test_serving_tokens.py`（+9 / -1 行）  

- **[92e7562] Suppress non‑TTY color output on the process name part of the log**  
  <https://github.com/vllm-project/vllm/commit/92e7562a994038c904fea859d90462c7e84a3246>  
  日志中进程名的颜色输出在非 TTY 环境下被抑制。  
  - `vllm/utils/system_utils.py`（+6 / -1 行）  

- **[87d0d17] Consolidate Deepseek‑OCR2 processor**  
  <https://github.com/vllm-project/vllm/commit/87d0d17ab583740bce777f334a6281edf9822e78>  
  合并 Deepseek‑OCR2 处理器，实现代码复用。  
  - `vllm/model_executor/models/deepseek_ocr2.py`（+12 / -4 行）  
  - 删除旧文件 `vllm/transformers_utils/processors/deepseek_ocr2.py`（-320 行）  

- **[a57c822] Make Inplace Flag for FusedMoEModularKernel part of the constructor**  
  <https://github.com/vllm-project/vllm/commit/a57c8228ffb3ff82b983b32b71ff62a837255129>  
  将 `inplace` 参数搬入 `FusedMoEModularKernel` 构造函数。  
  - 相关 MoE 测试文件大量更新（`tests/kernels/moe/*`）  
  - `vllm/model_executor/layers/fused_moe/config.py`（+6 / -3 行）  

- **[1ee9584] Fix swapped engine_ids in NIXL Llama 4 local attention path**  
  <https://github.com/vllm-project/vllm/commit/1ee95841bd251f9081c3a317984c4dcaa003b3c0>  
  修正 NIXL 连接器中 `engine_ids` 的顺序错误。  
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`（+3 / -3 行）  

- **[7d8c680] Add debug logs**  
  <https://github.com/vllm-project/vllm/commit/7d8c6804e2654873cb25d0b23fef178fa5f37237>  
  在 KV 连接器中加入调试日志。  
  - `vllm/distributed/kv_transfer/kv_connector/utils.py`（+2 行）  
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`（+3 行）  

- **[af3162d] Unified Parallel Drafting**  
  <https://github.com/vllm-project/vllm/commit/af3162d3aaa559a738396baf5b5134c1ab0742f5>  
  实现统一的并行草稿（Speculative Decoding）框架。  
  - 关键文件 `vllm/v1/spec_decode/*` 大幅改动（新增 1085 行代码，删除 392 行）  
  - 相关测试 `tests/v1/spec_decode/*` 进行同步更新。  

- **[5b2a942] Fix LoRA Fp8**  
  <https://github.com/vllm-project/vllm/commit/5b2a9422f0f4cbdd69a9fed1dc1605838314ff81>  
  修复 LoRA 在 FP8 量化下的错误。  
  - `vllm/lora/layers/fused_moe.py`（+14 / -8 行）  

- **[c1858b7] Native Weight Syncing API: NCCL**  
  <https://github.com/vllm-project/vllm/commit/c1858b7ec8aa571dc0c0e00aded01019cca6a7e6>  
  引入 NCCL 原生权重同步 API，支持 RLHF 场景。  
  - 新增示例 `examples/offline_inference/new_weight_syncing/rlhf*.py`、`examples/online_serving/rlhf_http.py`  
  - 新增分布式权重同步模块 `vllm/distributed/weight_transfer/*`（共约 300 行新增）  
  - 大量测试 `tests/distributed/*` 覆盖新特性。  

- **[82914d2] Fix step3p5 parser when using mtp**  
  <https://github.com/vllm-project/vllm/commit/82914d2ae8d0362be06700222f4cd4c5f6b0dc36>  
  修复 step3p5 解析器在 MTP 场景下的错误。  
  - 新增完整测试 `tests/tool_parsers/test_step3p5_tool_parser.py`（+1435 行）  
  - `vllm/tool_parsers/step3p5_tool_parser.py` 小幅改动（+20 / -5 行）  

- **[81a90e5] Add bart‑plugin to docs**  
  <https://github.com/vllm-project/vllm/commit/81a90e52776503c6cbdccd30fbe53f61c9179bdf>  
  文档中加入 BART 插件说明。  
  - `docs/design/plugin_system.md`、`docs/models/supported_models.md`、`docs/usage/v1_guide.md` 均有少量增删。  

- **[1c3a221] Fix corner case of sparse embedding**  
  <https://github.com/vllm-project/vllm/commit/1c3a221d3b0f7a82cd9a6d56e10ea360e2435a1c>  
  处理稀疏嵌入的边缘情况。  
  - `vllm/model_executor/layers/pooler/special.py`（+1 / -1 行）  

- **[7bd42e6] Clean up input preprocessing**  
  <https://github.com/vllm-project/vllm/commit/7bd42e609d24501f59a8b405229ed91f4ca8037c>  
  重构输入预处理逻辑，删除冗余代码。  
  - `vllm/inputs/preprocess.py`（-203 行，+68 行）  
  - 相关模块 `vllm/inputs/*`、`vllm/multimodal/inputs.py` 进行同步更新。  

- **[a252283] Fix Kimi‑K2.5 NVFP4 checkpoints weight loading**  
  <https://github.com/vllm-project/vllm/commit/a2522839d87d2b81b57458dfdbbcb27afb8191ae>  
  修复 Kimi‑K2.5 NVFP4 权重加载错误。  
  - `vllm/model_executor/models/kimi_k25.py`（+14 / -4 行）  

- **[59a5cb3] Integrate flashinfer concat_mla_k**  
  <https://github.com/vllm-project/vllm/commit/59a5cb387ae4c11c73855d505adb0b2c7cd3861d>  
  将 flashinfer 的 `concat_mla_k` 接口集成到 MLA 注意力实现中。  
  - `vllm/model_executor/layers/attention/mla_attention.py`（+17 / -3 行）  
  - `vllm/utils/flashinfer.py`
### NVIDIA/cutile-python
提交  
- 暂无提交记录  

Issues  
1. **[BUG]: Hang forever on B200 but not GB10**  
   - URL: https://github.com/NVIDIA/cutile-python/issues/71  
   - 作者: SchrodingerZhu  
   - 创建时间: 2026-02-05 19:05:39 UTC  
   - 说明: issue 内容缺失（body_truncated 为 true），请补充复现步骤和日志。  

2. **[BUG]: Regression of branch type inference**  
   - URL: https://github.com/NVIDIA/cutile-python/issues/70  
   - 作者: SchrodingerZhu  
   - 创建时间: 2026-02-05 18:57:55 UTC  
   - 说明: issue 内容缺失（body_truncated 为 true），请提供具体表现和回退版本。  

3. **[FEA]: CUDA C or PTX Injection**  
   - URL: https://github.com/NVIDIA/cutile-python/issues/69  
   - 作者: neur1n  
   - 创建时间: 2026-02-05 08:50:15 UTC  
   - 说明: issue 内容缺失（body_truncated 为 true），需要阐明需求场景和期望接口。

## 总结
总体来看，核心推理库正加速向多硬件、多量化格式的统一兼容迈进，尤其是对 MoE、FP8/FP4 以及 FlashAttention 的深度集成。建议关注 **flashinfer** 与 **vllm** 在 XPU 与 NCCL 同步方面的进展，它们将直接影响大规模分布式推理的可用性；同时留意 **sglang** 的 JIT 迁移和文档完善，以降低上手门槛。
