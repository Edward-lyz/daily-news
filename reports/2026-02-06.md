ä»Šæ—¥çš„ AI Infra çš„æ–°é—»å¦‚ä¸‹ã€‚

## æ‘˜è¦
### æœ¬å‘¨å…³é”®æ›´æ–°æ¦‚è§ˆ
- **FlashMLA**: å¼•å…¥ `CUDAGuard` ä¸åŠ¨æ€è®¾å¤‡ ID åˆ†é…ï¼Œæå‡ CUDA èµ„æºç®¡ç†å®‰å…¨æ€§ï¼Œä»£ç æ”¹åŠ¨è¡Œæ•°ä¸º +12 -2ã€‚\
- **CUTLASS**: ç¤¾åŒºæŠ¥å‘Š GEMM+SiLU ç»„åˆåœ¨ç‰¹å®š GPU ä¸Šæ˜¾è‘—æ…¢äºå•ç‹¬ GEMMï¼Œæä¾›äº†æ€§èƒ½å¯¹æ¯”æˆªå›¾ï¼Œæç¤ºåç»­éœ€è¦ä¼˜åŒ– epilogue å®ç°ã€‚\
- **FlashInfer**: å®Œæˆä¸¤å¤§åŠŸèƒ½å‡çº§ï¼šâ‘  å°† `grouped_gemm_nt_masked` ä» `cute_dsl` ç§»è‡³ `gemm`ï¼Œå®ç°å‘åå…¼å®¹å¹¶ç»Ÿä¸€ APIï¼›â‘¡ ä¸º SM120/SM121 æ·»åŠ  FP4 GEMM æ–° tile é…ç½®åŠå¯é€‰ Streamâ€‘K è°ƒåº¦å™¨ï¼Œæ˜¾è‘—æå‡å¤§çŸ©é˜µä¹˜æ³•ååã€‚\
- **SGLang**: å¤§è§„æ¨¡ CI ä¸æ–‡æ¡£æ”¹åŠ¨ï¼ŒåŒ…æ‹¬åˆå¹¶ Stageâ€‘C å¤§è§„æ¨¡ GPU æµ‹è¯•å¥—ä»¶ã€ä¸º NIXL è¿æ¥å™¨åŠ å…¥æ··åˆæ¨¡å‹ PDã€æ‰©å±• Diffusion æ–‡æ¡£ã€è¿ç§» GPTQâ€‘Marlin GEMM åˆ° JITã€ä»¥åŠå¤šé¡¹ bug ä¿®å¤ï¼ˆå¦‚ HF ä¸‹è½½ç«äº‰ã€torch.compile å›¾ä¸­æ–­ç­‰ï¼‰ã€‚\
- **vLLM**: å¤šé¡¹åº•å±‚ä¸åŠŸèƒ½å¢å¼ºï¼šâ‘  ROCm å¹³å°æ£€æµ‹ä¸­é˜²æ­¢æå‰ CUDA åˆå§‹åŒ–ï¼›â‘¡ ä¿®å¤è·¯ç”±å™¨ç±»å‹åˆ¤å®šä¸ `logprobs=0` è¿”å›ç©ºåˆ—è¡¨é—®é¢˜ï¼›â‘¢ æ”¯æŒ SM120/SM121 çš„ FP4 GEMM tile ä¸ Streamâ€‘Kï¼›â‘£ å¼•å…¥ XPU wna16 kernelã€CPU NEON BFMMLA BF16 åŠ é€Ÿã€MiniCPMâ€‘o 4.5ã€Voyageâ€‘4â€‘nano ç­‰æ–°æ¨¡å‹ï¼›â‘¤ å®Œå–„æ–‡æ¡£ã€CIã€preâ€‘commit æ£€æŸ¥ä»¥åŠ KV Connector æ–¹æ³•è¦†ç›–ã€‚\
- **Flashâ€‘Attention**: è®¨è®ºåœ¨ `flashâ€‘attentionâ€‘4` ä¸­å°† `e2e_res` è®¾ä¸ºè°ƒä¼˜å˜é‡çš„å¯è¡Œæ€§ï¼Œæå‡ºç›´æ¥è°ƒä¼˜è¯¥å‚æ•°çš„å»ºè®®ã€‚

## å…·ä½“å†…å®¹åˆ†æ
### deepseek-ai/DeepGEMM
æ˜¨æ—¥æ— æ›´æ–°ã€‚
### deepseek-ai/FlashMLA
## deepseek-ai/FlashMLA

**2026-02-06** - Zeyu WANG æ·»åŠ äº† CUDAGuard å’ŒåŠ¨æ€è®¾å¤‡ ID åˆ†é…åŠŸèƒ½ ([#160](https://github.com/deepseek-ai/FlashMLA/pull/160))

ä¿®æ”¹æ–‡ä»¶:
- `csrc/sm100/prefill/dense/fmha_cutlass_bwd_sm100.cuh`: æ·»åŠ  CUDAGuard å¤´æ–‡ä»¶å¹¶å®ç°åŠ¨æ€è®¾å¤‡ ID åˆ†é…
- `csrc/sm100/prefill/dense/fmha_cutlass_fwd_sm100.cuh`: æ·»åŠ  CUDAGuard å¤´æ–‡ä»¶å¹¶å®ç°åŠ¨æ€è®¾å¤‡ ID åˆ†é…

å˜æ›´ç»Ÿè®¡: +12 -2
### NVIDIA/cutlass
## æäº¤

æœ¬å‘¨ NVIDIA/cutlass ä»“åº“æ— ä»£ç æäº¤ã€‚

## Issues

### [QST] Why does gemm + epilogue(silu) take so much time?

æœ‰ç”¨æˆ·æŠ¥å‘Šåœ¨ä½¿ç”¨ CUTLASS è¿›è¡Œ GEMM + SiLU epilogue æ“ä½œæ—¶ï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼ŒGEMM + SiLU çš„å»¶è¿Ÿè¿œé«˜äºå•ç‹¬çš„ GEMM æ“ä½œï¼Œè€Œ GEMM + Bias åˆ™æ²¡æœ‰æ˜æ˜¾å¢åŠ å»¶è¿Ÿã€‚

è¯¥é—®é¢˜åœ¨ thor-u GPU ä¸Šå¤ç°ï¼Œç¯å¢ƒé…ç½®ä¸º CUTLASS v4.3.5ã€CUDA Toolkit 12.8 å’Œ Linux OSã€‚ç”¨æˆ·é™„ä¸Šäº†è¯¦ç»†çš„æ€§èƒ½å¯¹æ¯”æˆªå›¾ã€‚

Issue é“¾æ¥ï¼š[https://github.com/NVIDIA/cutlass/issues/3007](https://github.com/NVIDIA/cutlass/issues/3007)
### flashinfer-ai/flashinfer
## æœ¬æ¬¡æäº¤æ¦‚è§ˆ  

| PR | ç®€è¦è¯´æ˜ | å…³é”®æ–‡ä»¶/æ¨¡å— | ä»£ç ç‰‡æ®µ |
|---|---|---|---|
| **#2503** â€“  refactor: Port upstream CUTLASS fixes and refactor grouped_gemm_nt_masked GEMM module location | å°† CUTLASS ä¸Šæ¸¸ä¿®å¤è¿ç§»è‡³æœ¬ä»“åº“ï¼Œå¹¶æŠŠ `grouped_gemm_nt_masked` ä» `flashinfer.cute_dsl` ç§»åŠ¨åˆ° `flashinfer.gemm`ï¼ŒåŒæ—¶åŠ å…¥å‘åå…¼å®¹çš„åºŸå¼ƒæç¤ºã€‚ | `flashinfer/cute_dsl/__init__.py`ã€`flashinfer/gemm/__init__.py`ã€`flashinfer/gemm/kernels/__init__.py`ã€`flashinfer/gemm/kernels/grouped_gemm_masked_blackwell.py`ã€`tests/gemm/test_cute_dsl_blockscaled_gemm.py` | ```python\n# flashinfer/gemm/__init__.py\ntry:\n    from .kernels.grouped_gemm_masked_blackwell import (\n        grouped_gemm_nt_masked,\n        Sm100BlockScaledPersistentDenseGemmKernel,\n        create_scale_factor_tensor,\n    )\n    _cute_dsl_kernels = [\"grouped_gemm_nt_masked\", \"Sm100BlockScaledPersistentDenseGemmKernel\", \"create_scale_factor_tensor\"]\nexcept ImportError:\n    pass\n``` |
| **#2460** â€“  perf: add fp4 GEMM tile configs and streamK scheduler for SM120 | ä¸º SM120/SM121 æ·»åŠ äº† `(128,128,256)`ã€`(256,128,128)` ç­‰æ–° tile é…ç½®ï¼Œå¹¶å¼•å…¥å¯é€‰çš„ Streamâ€‘K è°ƒåº¦å™¨ã€‚ | `flashinfer/jit/gemm/core.py`ã€`include/flashinfer/gemm/cutlass_gemm_configs.h`ã€`include/flashinfer/gemm/fp4_gemm_cutlass_template_sm120.h`ã€`include/flashinfer/gemm/fp4_gemm_template_sm120.h` | ```c++\n// cutlass_gemm_configs.h\nstruct CutlassGemmConfig {\n    ...\n    bool use_stream_k = false;  // SM120: false = DP scheduler, true = StreamK scheduler\n};\n\n// core.py (JIT)\ncta_m_n_k_list = [\n    (128, 128, 128),\n    (128, 128, 256),\n    (256, 128, 128),\n]\n``` |

### ä»£ç è¿ç§»è¦ç‚¹  

- **åºŸå¼ƒè·¯å¾„**ï¼š`flashinfer.cute_dsl` ä¸­çš„ GEMM ç¬¦å·ç°åœ¨ä»…ä½œå‘åå…¼å®¹ï¼Œæ–‡æ¡£å·²æ ‡è®° `.. deprecated::`ã€‚  
- **æ–°å…¬å…± API**ï¼šæ‰€æœ‰ç”¨æˆ·åº”æ”¹ä¸º `from flashinfer.gemm import grouped_gemm_nt_masked, create_scale_factor_tensor, Sm100BlockScaledPersistentDenseGemmKernel`ã€‚  
- **å†…éƒ¨ç»„ç»‡**ï¼šæ–°å¢ `flashinfer/gemm/kernels` åŒ…ï¼Œç”¨äºå­˜æ”¾ CuTeâ€‘DSL å®ç°çš„å†…éƒ¨ kernelï¼Œå¤–éƒ¨ä»…é€šè¿‡ `flashinfer.gemm` æš´éœ²ã€‚  

### æ€§èƒ½æ”¹è¿›æ¦‚è§ˆ  

- **SM120/SM121 æ–° tile**ï¼šåœ¨ FP4 GEMM ä¸­åŠ å…¥æ›´å¤§ K ç»´åº¦çš„ tileï¼Œæå‡å¤§çŸ©é˜µä¹˜æ³•ååã€‚  
- **Streamâ€‘K è°ƒåº¦**ï¼šé€šè¿‡ `use_stream_k` æ ‡å¿—å¯åœ¨ SM120 ä¸Šå¯ç”¨ Streamâ€‘K è°ƒåº¦å™¨ï¼Œé’ˆå¯¹æ·±åº¦å­¦ä¹ å·¥ä½œè´Ÿè½½æä¾›æ›´å¥½çš„å¹¶è¡Œåº¦ã€‚  

## æœ¬æœŸ Issue æ±‡æ€»  

| Issue | ç®€è¦æè¿° |
|---|---|
| [#2514](https://github.com/flashinfer-ai/flashinfer/issues/2514) â€“ **test_groupwise_scaled_gemm_mxfp4.py åœ¨ B300 ä¸Šå¶å‘ NaN** | åœ¨ SM103ï¼ˆB300ï¼‰ä¸Šè¿è¡Œ `test_groupwise_scaled_gemm_mxfp4.py` æ—¶å‡ºç°ä¸ç¡®å®šæ€§ NaNï¼Œéœ€æ’æŸ¥æ•°å€¼ç¨³å®šæ€§ã€‚ |
| [#2513](https://github.com/flashinfer-ai/flashinfer/issues/2513) â€“ **ä¸º Mamba Selective State Update æ·»åŠ åŸºå‡†** | æ–°å¢çš„ Mamba é€‰æ‹©æ€§çŠ¶æ€æ›´æ–° kernel ç¼ºå°‘å¾®åŸºå‡†ï¼Œéœ€è¦è¡¥å…… benchmark è„šæœ¬ã€‚ |
| [#2511](https://github.com/flashinfer-ai/flashinfer/issues/2511) â€“ **TRTLLMâ€‘GEN åç«¯ CUDA Graph ç»“æœä¸ä¸€è‡´** | åœ¨ B200 ä¸Šä½¿ç”¨ `BatchDecodeWithPagedKVCacheWrapper` ä¸ TRTLLMâ€‘GEN æ—¶ï¼Œå¼€å¯ CUDA Graph ä¼šå¯¼è‡´ç»“æœåå·®ã€‚ |
| [#2507](https://github.com/flashinfer-ai/flashinfer/issues/2507) â€“ **trtllm_fp8_per_tensor_scale_moe æŠ¥ AttributeError** | `trtllm_fp8_per_tensor_scale_moe` åœ¨ 0.6.3 ç‰ˆæœ¬ä¸­å›  `activation_type` å¤„ç†ä¸å½“æŠ›å‡º `'int' object has no attribute 'value'`ã€‚ |
| [#2506](https://github.com/flashinfer-ai/flashinfer/issues/2506) â€“ **Jetson Orin NX æ¨¡å—æ³¨å†Œå¤±è´¥** | åœ¨ Jetson Orin NXï¼ˆCUDA 12.6ï¼‰ä¸Šæ‰§è¡Œ `flashinfer show-config` æ—¶æç¤º `No module named 'nvidia'`ï¼Œéœ€è¦æ£€æŸ¥ Jetson ç¯å¢ƒçš„ä¾èµ–è·¯å¾„ã€‚ |

---  

**æ€»ä½“è¯„ä¼°**ï¼šæœ¬æ¬¡ PR ä¸»è¦å®Œæˆäº† API é‡æ„ã€å‘åå…¼å®¹ä¸æ€§èƒ½å‡çº§ä¸‰å¤§ä»»åŠ¡ã€‚ä»£ç è¿ç§»æå‡äº†åº“çš„å¯ç»´æŠ¤æ€§ï¼Œæ–°å¢çš„ SM120 tile ä¸ Streamâ€‘K è°ƒåº¦ä¸ºé«˜ç«¯ GPUï¼ˆå¦‚ H100ã€H200ï¼‰æä¾›äº†æ˜¾è‘—åŠ é€Ÿæ½œåŠ›ã€‚ä¸æ­¤åŒæ—¶ï¼Œè‹¥å¹²å·²çŸ¥ Bugï¼ˆå°¤å…¶æ˜¯ B300 ä¸Šçš„æ•°å€¼ä¸ç¨³å®šï¼‰ä»å¾…è¿›ä¸€æ­¥è°ƒè¯•ã€‚åç»­å»ºè®®ä¼˜å…ˆè·Ÿè¿› Issue #2514 ä¸ #2507ï¼Œä»¥ç¡®ä¿æ–°åŠŸèƒ½åœ¨æ‰€æœ‰å—æ”¯æŒç¡¬ä»¶ä¸Šä¿æŒæ­£ç¡®æ€§ã€‚
### Dao-AILab/flash-attention
**2026-02-06**

**Dao-AILab/flash-attention**

**Issue:**
- [Why not choose e2e_res as our tune variable](https://github.com/Dao-AILab/flash-attention/issues/2237) - cyk2018 æå‡ºå…³äºåœ¨ flash-attention-4 ä¸­è°ƒæ•´ e2e_res å’Œ e2e_freq å‚æ•°çš„é—®é¢˜ï¼Œè®¨è®ºäº†å½“å‰é€‰æ‹© e2e_freq ä½œä¸ºè°ƒä¼˜å˜é‡çš„åˆç†æ€§ï¼Œå»ºè®®è€ƒè™‘ç›´æ¥è°ƒæ•´ e2e_res å‚æ•°ã€‚
### sgl-project/sglang
æäº¤  
- **bedade1** â€“ Merge stageâ€‘câ€‘testâ€‘largeâ€‘4â€‘gpu suites into partitioned suites (#18325)  
  - ä½œè€…ï¼šAlison Shaoï¼Œ2026â€‘02â€‘06 23:32:33 UTC  
  - PR é“¾æ¥ï¼š<https://github.com/sgl-project/sglang/commit/bedade1ef050eaae4cd2232e229b1d267efb66fd>  
  - å…³é”®æ”¹åŠ¨ï¼š  
    - `.github/workflows/pr-test.yml` ä¸­åˆ é™¤äº† `stage-c-test-large-4-gpu` ç›¸å…³ä½œä¸šï¼ˆ87 è¡Œåˆ é™¤ï¼‰ã€‚  
    - `scripts/ci/utils/slash_command_handler.py` ç§»é™¤å¯¹ `stage-c-test-large-4-gpu` ä¸ `stage-c-test-large-4-gpu-b200` çš„æ”¯æŒã€‚  
    - `test/README.md` å°† Stageâ€¯C åˆ’åˆ†ä¸º 4â€‘GPU ä¸ 8â€‘GPU ä¸¤å¥—æ–°å¥—ä»¶ï¼Œåˆ é™¤æ—§å¥—ä»¶åç§°ã€‚  
    - å¤šä¸ªæµ‹è¯•æ–‡ä»¶ï¼ˆ`test_local_attn.py`ã€`test_dp_attention_large.py`ã€`test_cutedsl_moe.py`ã€`test_return_routed_experts.py`ã€`test_eagle_dp_attention.py`ã€`test_eagle_infer_beta_dp_attention.py`ï¼‰çš„ `register_cuda_ci` è°ƒç”¨ä» `stage-c-test-large-4-gpu` æ›¿æ¢ä¸ºå¯¹åº”çš„ 4â€‘GPU æˆ– 4â€‘GPUâ€‘b200 å¥—ä»¶ã€‚  
    - `test/run_suite.py` ç§»é™¤æ—§å¥—ä»¶åç§°ï¼ŒåŠ å…¥æ–°å¥—ä»¶ `stage-c-test-4-gpu-h100`ã€`stage-c-test-4-gpu-b200`ã€`stage-c-test-4-gpu-gb200`ã€‚  
  - **è¯´æ˜**ï¼š`patch_truncated` ä¸º trueï¼Œå®Œæ•´ diff æœªå±•ç¤ºï¼Œä»¥ä¸Šä¸ºå…³é”®æ‘˜è¦ã€‚

- **f1ff697** â€“ add hybrid model PD to NIXL connector (#16229)  
  - ä½œè€…ï¼šNeal Vaidyaï¼Œ2026â€‘02â€‘06 23:05:36 UTC  
  - PR é“¾æ¥ï¼š<https://github.com/sgl-project/sglang/commit/f1ff697494c59bd0a4d298d5a1411f9eed563982>  
  - å…³é”®æ”¹åŠ¨ï¼š`python/sglang/srt/disaggregation/nixl/conn.py`  
    - ä¸º `TransferInfo` å¢åŠ  `dst_state_indices: List[int]` å­—æ®µã€‚  
    - `from_zmq` è§£æç¬¬ 8 æ¡æ¶ˆæ¯ä¸º `dst_state_indices`ï¼ˆè‹¥å­˜åœ¨ï¼‰ã€‚  
    - `KVArgsRegisterInfo` æ–°å¢ `dst_state_data_ptrs: list[int]`ã€‚  
  - **è¯´æ˜**ï¼šå®Œæ•´ diff å·²æä¾›ï¼ˆ140 è¡Œæ”¹åŠ¨ï¼‰ã€‚

- **c850a8a** â€“ [Docs] Add Falcon H1, Hunyuanâ€‘Large, Qwen3â€‘Omni support and update Diffusion usage (#17888)  
  - ä½œè€…ï¼šRishit Shivamï¼Œ2026â€‘02â€‘06 21:17:51 UTC  
  - PR é“¾æ¥ï¼š<https://github.com/sgl-project/sglang/commit/c850a8a41af2d2f973a7cb033eef9ba9cacc0ca3>  
  - å…³é”®æ”¹åŠ¨ï¼šæ–°å¢æ–‡æ¡£ `docs/basic_usage/diffusion.md`ï¼ˆä»‹ç»å›¾åƒ/è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼‰å’Œ `docs/basic_usage/diffusion_llms.md`ï¼ˆä»‹ç»æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼‰ã€‚  
  - å…¶ä»–æ–‡æ¡£è·¯å¾„æ›´æ–°ï¼š`openai_api_embeddings.ipynb`ã€`openai_api_vision.ipynb` ä¸­çš„æ¨¡å‹åˆ—è¡¨é“¾æ¥æŒ‡å‘æ–°çš„ç›®å½•ï¼›`docs/index.rst` æ·»åŠ  diffusion æ–‡æ¡£å…¥å£ï¼›`docs/index.rst` ä¸­çš„ â€œSupported Modelsâ€ ç« èŠ‚å±‚çº§è°ƒæ•´ã€‚  
  - **è¯´æ˜**ï¼šéƒ¨åˆ†æ–‡ä»¶è¢«é‡å‘½åæˆ–ç§»åŠ¨ï¼Œ`patch_truncated` ä¸º trueï¼Œå…·ä½“æ”¹åŠ¨è¯·å‚è€ƒæäº¤ã€‚

- **f2e0048** â€“ Add CI permission for Shunkangz, dongjiyingdjy, samuellees (#18377)  
  - ä½œè€…ï¼šBaizhou Zhangï¼Œ2026â€‘02â€‘06 17:19:02 UTC  
  - PR é“¾æ¥ï¼š<https://github.com/sgl-project/sglang/commit/f2e0048d0688e929803f109bb633b327a9fb3d2c>  
  - å…³é”®æ”¹åŠ¨ï¼š`.github/CI_PERMISSIONS.json` æ–°å¢ 3 ä½ç”¨æˆ·çš„ CI æƒé™ï¼ˆ21 è¡Œæ–°å¢ï¼‰ã€‚  

- **e13b727** â€“ [diffusion] CI: update perf baseline (#17512)  
  - ä½œè€…ï¼šProzac614ï¼Œ2026â€‘02â€‘06 16:28:44 UTC  
  - PR é“¾æ¥ï¼š<https://github.com/sgl-project/sglang/commit/e13b727e928354e53b1e5ba3f613d8bb447a5594>  
  - å…³é”®æ”¹åŠ¨ï¼š`python/sglang/multimodal_gen/test/server/perf_baselines.json` æ›´æ–°åŸºå‡†æ•°æ®ï¼ˆ+398 è¡Œï¼Œâ€‘331 è¡Œï¼‰ã€‚  

- **c6aa186** â€“ Add Nemotron 3 Nano tests (#18119)  
  - ä½œè€…ï¼šshaharmor98ï¼Œ2026â€‘02â€‘06 15:55:42 UTC  
  - PR é“¾æ¥ï¼š<https://github.com/sgl-project/sglang/commit/c6aa1863be84c77bf423ee811e03e9129401c539>  
  - å…³é”®æ”¹åŠ¨ï¼š  
    - `python/pyproject.toml` æ–°å¢ä¾èµ–è¡Œï¼ˆ+1ï¼‰ã€‚  
    - æ–°å¢ `python/sglang/test/kits/lm_eval_kit.py`ï¼ˆ109 è¡Œï¼‰ç”¨äº LMâ€‘Eval æµ‹è¯•ã€‚  
    - æ–°å¢ä¸¤ä»½ Nemotronâ€‘3â€‘Nano é…ç½® YAMLï¼ˆ`test/lm_eval_configs/...`ï¼‰ï¼Œä»¥åŠå¯¹åº”çš„æ³¨å†Œæµ‹è¯• `test/registered/models/test_nvidia_nemotron_3_nano.py`ï¼ˆ41 è¡Œï¼‰ã€‚  

- **79d409f** â€“ [diffusion] fix: offload text encoder model in image encoding stage (#18317)  
  - ä½œè€…ï¼šxiaoyeï¼Œ2026â€‘02â€‘06 14:55:56 UTC  
  - PR é“¾æ¥ï¼š<https://github.com/sgl-project/sglang/commit/79d409f21019686d2ac462ad6287c2cc92751120>  
  - å…³é”®æ”¹åŠ¨ï¼š`python/sglang/multimodal_gen/runtime/pipelines_core/stages/image_encoding.py` æ–°å¢ 1 è¡Œä»£ç ï¼Œå®ç°æ–‡æœ¬ç¼–ç å™¨çš„ offloadã€‚  

- **3d68bd9** â€“ add hicache jit test (#17847)  
  - ä½œè€…ï¼šXuchun Shangï¼Œ2026â€‘02â€‘06 08:54:33 UTC  
  - PR é“¾æ¥ï¼š<https://github.com/sgl-project/sglang/commit/3d68bd9d9b6ed994a2de1f7d9d2bd2956f8f1ac9>  
  - å…³é”®æ”¹åŠ¨ï¼š  
    - æ–°å¢ `python/sglang/jit_kernel/benchmark/bench_hicache.py`ï¼ˆ406 è¡Œï¼‰ç”¨äº HiCache åŸºå‡†ã€‚  
    - å¤šä¸ª JIT kernel åŸºå‡†æ–‡ä»¶ï¼ˆ`bench_per_tensor_quant_fp8.py`ã€`bench_qknorm.py`ã€`bench_rmsnorm.py`ã€`bench_store_cache.py`ã€`utils.py`ï¼‰åˆ†åˆ«åšäº†ç»†å¾®å¢åˆ ï¼ˆæ€»è®¡ +523 / â€‘74 è¡Œï¼‰ã€‚  

- **f798ab9** â€“ [diffusion] fix: fix torch.compile graph break caused by torch._dynamo.disable (#18336)  
  - ä½œè€…ï¼šéƒ‘ä¸€å¸†ï¼Œ2026â€‘02â€‘06 06:48:09 UTC  
  - PR é“¾æ¥ï¼š<https://github.com/sgl-project/sglang/commit/f798ab9775ba16b3b64e65f5c33a222ad5a64829>  
  - å…³é”®æ”¹åŠ¨ï¼š  
    - `python/sglang/jit_kernel/diffusion/cutedsl/scale_residual_norm_scale_shift.py` è°ƒæ•´å®ç°ï¼ˆ+32 / â€‘20 è¡Œï¼‰ã€‚  
    - `python/sglang/jit_kernel/tests/test_fused_norm_scale_shift.py` åˆ é™¤å†—ä½™æ–­è¨€ï¼ˆâ€‘3 è¡Œï¼‰ã€‚  
    - `python/sglang/multimodal_gen/runtime/layers/layernorm.py` å°å¹…ä¼˜åŒ–ï¼ˆ+5 è¡Œï¼‰ã€‚  

- **d0c39bc** â€“ Fix crossâ€‘container HF download race condition in CI (#18328)  
  - ä½œè€…ï¼šAlison Shaoï¼Œ2026â€‘02â€‘06 05:01:41 UTC  
  - PR é“¾æ¥ï¼š<https://github.com/sgl-project/sglang/commit/d0c39bc2193c3e28eee94018340d32029541f0a3>  
  - å…³é”®æ”¹åŠ¨ï¼š`python/sglang/srt/model_loader/ci_weight_validation.py` å¢åŠ æ–‡ä»¶é”ä¸é‡è¯•é€»è¾‘ï¼ˆ+99 è¡Œï¼Œâ€‘13 è¡Œï¼‰ï¼Œé˜²æ­¢å¤šå®¹å™¨å¹¶å‘ä¸‹è½½åŒä¸€æ¨¡å‹å¯¼è‡´çš„å†²çªã€‚  

- **92b8bd6** â€“ fix npu best practice (#18330)  
  - ä½œè€…ï¼šamote-iï¼Œ2026â€‘02â€‘06 02:14:46 UTC  
  - PR é“¾æ¥ï¼š<https://github.com/sgl-project/sglang/commit/92b8bd68333ef9bca081201cca474755e5e42a98>  
  - å…³é”®æ”¹åŠ¨ï¼š`docs/platforms/ascend_npu_best_practice.md` ä¸­çš„ 8 è¡Œåˆ é™¤ä¸ 7 è¡Œæ–°å¢ï¼Œæ›´æ–°äº† NPU ä½¿ç”¨å»ºè®®ã€‚  

- **aa390d2** â€“ [Kernel] Migrate GPTQâ€‘Marlin GEMM kernel to JIT (#18067)  
  - ä½œè€…ï¼šLinyu Wuï¼Œ2026â€‘02â€‘06 00:31:42 UTC  
  - PR é“¾æ¥ï¼š<https://github.com/sgl-project/sglang/commit/aa390d276279360899047567fe6892e28abed7cc>  
  - å…³é”®æ”¹åŠ¨ï¼šå¤§é‡æ–°å¢ JIT kernel ä»£ç ï¼ˆå…±è®¡çº¦ 4â€¯k è¡Œï¼‰ï¼ŒåŒ…æ‹¬ï¼š  
    - `bench_gptq_marlin.py`ï¼ˆ118 è¡Œï¼‰åŸºå‡†è„šæœ¬ã€‚  
    - `csrc/gemm/marlin/` ç›®å½•ä¸‹çš„å®ç°æ–‡ä»¶ï¼ˆ`dequant.h`ã€`gptq_marlin.cuh`ã€`kernel.h`ã€`marlin.cuh`ã€`marlin_dtypes.cuh`ã€`marlin_template.h`ï¼‰å…±è®¡çº¦ 2.5â€¯k è¡Œã€‚  
    - Python åŒ…è£…å±‚ `gptq_marlin.py`ï¼ˆ115 è¡Œï¼‰ä¸æµ‹è¯• `test_gptq_marlin.py`ï¼ˆ125 è¡Œï¼‰ã€‚  
    - ç›¸å…³é‡åŒ–å·¥å…· `marlin_utils.py`ã€`marlin_utils_fp8.py` å°å¹…æ”¹åŠ¨ï¼ˆÂ±2 è¡Œï¼‰ã€‚  

---

Issues  
- **[Feature] Return token ids under /chat/completion endpoint (Prompt + Response)**  
  - é“¾æ¥ï¼š<https://github.com/sgl-project/sglang/issues/18378>  
  - ç®€è¿°ï¼šç”¨æˆ·å¸Œæœ›åœ¨ `/chat/completion` æ¥å£è¿”å›ç”Ÿæˆ token çš„ ID ä¸ prompt token å…ƒä¿¡æ¯ï¼Œä»¥ä¿è¯ RL è®­ç»ƒçš„ token ä¸€è‡´æ€§ã€‚  

- **Empty responses when streaming LLM outputs on large models (KeyError: prompt_tokens)**  
  - é“¾æ¥ï¼š<https://github.com/sgl-project/sglang/issues/18371>  
  - ç®€è¿°ï¼šåœ¨ 128k/332k ä¸Šä¸‹æ–‡é•¿åº¦ä¸‹è¿›è¡Œæµå¼è¾“å‡ºæ—¶ï¼ŒæœåŠ¡å™¨æŠ›å‡º `KeyError: 'prompt_tokens'`ï¼Œå¯¼è‡´å®¢æˆ·ç«¯æ”¶åˆ°ç©ºå“åº”ã€‚  

- **Error in SGLang multiâ€‘machine PP (Pipeline Parallelism) mode deployment for inference**  
  - é“¾æ¥ï¼š<https://github.com/sgl-project/sglang/issues/18366>  
  - ç®€è¿°ï¼šç”¨æˆ·åœ¨å¤šæœº PP éƒ¨ç½²æ—¶é‡åˆ°ç¯å¢ƒå˜é‡é…ç½®ä¸å¯åŠ¨è„šæœ¬é”™è¯¯ï¼Œå¯¼è‡´æ¨ç†å¤±è´¥ã€‚  

- **ğŸš€ [RFC] Decoupling Framework Logic from Hardware Kernels: Offloading Triton Kernels to sglâ€‘kernel**  
  - é“¾æ¥ï¼š<https://github.com/sgl-project/sglang/issues/18365>  
  - ç®€è¿°ï¼šæè®®å°†æ¡†æ¶å±‚é€»è¾‘ä¸ç¡¬ä»¶ Triton kernel è§£è€¦ï¼Œè¿ç§»è‡³ç‹¬ç«‹çš„ `sglâ€‘kernel` é¡¹ç›®ï¼Œä»¥æå‡å¯ç»´æŠ¤æ€§ä¸è·¨å¹³å°å…¼å®¹ã€‚  

- **[Diffuser] Why connection failed on EOFError ?**  
  - é“¾æ¥ï¼š<https://github.com/sgl-project/sglang/issues/18363>  
  - ç®€è¿°ï¼šç”¨æˆ·åœ¨ä½¿ç”¨ Diffuser è¿›è¡Œè§†é¢‘ç”Ÿæˆæ—¶ï¼Œå¯åŠ¨è„šæœ¬æŠ¥ EOFErrorï¼Œæ— æ³•åŠ è½½æ¨¡å‹æƒé‡ã€‚  

- **[Bug] DeepSeek OCR 2 Accuracy Issue**  
  - é“¾æ¥ï¼š<https://github.com/sgl-project/sglang/issues/18358>  
  - ç®€è¿°ï¼šDeepSeek OCRâ€‘2 åœ¨ SGLang ä¸­çš„è¯†åˆ«å‡†ç¡®ç‡ä½äº Transformers åŸºå‡†ï¼Œæ€€ç–‘æ¨¡å‹åŠ è½½æˆ–åå¤„ç†å·®å¼‚ã€‚  

- **Sanitize exception messages in HTTP entrypoints to prevent information exposure**  
  - é“¾æ¥ï¼š<https://github.com/sgl-project/sglang/issues/18348>  
  - ç®€è¿°ï¼šå»ºè®®åœ¨ HTTP API å±‚ç»Ÿä¸€æ•è·å¼‚å¸¸ï¼Œè¿”å›é€šç”¨é”™è¯¯ä¿¡æ¯å¹¶åœ¨æœåŠ¡å™¨æ—¥å¿—ä¸­è®°å½•å®Œæ•´å †æ ˆï¼Œä»¥é¿å…æ³„éœ²å†…éƒ¨å®ç°ç»†èŠ‚ã€‚  

- **Potential ReDoS in Pythonic tool call detector**  
  - é“¾æ¥ï¼š<https://github.com/sgl-project/sglang/issues/18347>  
  - ç®€è¿°ï¼š`sglang/srt/function_call/pythonic_detector.py` ä¸­çš„æ­£åˆ™è¡¨è¾¾å¼å­˜åœ¨åµŒå¥—é‡å¤ï¼Œå¯èƒ½è¢«æ¶æ„è¾“å…¥è§¦å‘æ­£åˆ™æ‹’ç»æœåŠ¡ï¼ˆReDoSï¼‰ã€‚å»ºè®®é‡æ„ä¸ºæ›´å®‰å…¨çš„è§£ææ–¹å¼ã€‚
### vllm-project/vllm
- **[bugfix] [ROCm] Fix premature CUDA initialization in platform detection**  
  PR: <https://github.com/vllm-project/vllm/pull/33941> (SHA `4a2d00e`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/platforms/rocm.py`ï¼ˆæ–°å¢ 40 è¡Œï¼Œåˆ é™¤ 6 è¡Œï¼‰ï¼Œåœ¨å¹³å°æ£€æµ‹é˜¶æ®µå»¶è¿Ÿ CUDA åˆå§‹åŒ–ï¼Œé˜²æ­¢åœ¨ ROCm ç¯å¢ƒä¸‹æå‰åŠ è½½ CUDAã€‚  
  å…¶ä»–æ”¹åŠ¨ï¼š`.buildkite/test-amd.yaml`ã€`.buildkite/test_areas/cuda.yaml` å„åŠ  1 è¡Œï¼›æ–°å¢ CUDA æµ‹è¯•è„šæœ¬ `tests/cuda/scripts/check_device_count_respects_env.py`ã€`tests/cuda/scripts/check_platform_no_cuda_init.py` ä¸å¯¹åº”æµ‹è¯• `tests/cuda/test_platform_no_cuda_init.py`ï¼ˆå…± 111 è¡Œæ–°å¢ï¼‰ã€‚

- **Fix RoutingMethodType logic**  
  PR: <https://github.com/vllm-project/vllm/pull/33919> (SHA `207c3a0`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/model_executor/layers/fused_moe/router/router_factory.py`ï¼ˆæ–°å¢ 14 è¡Œï¼Œåˆ é™¤ 1 è¡Œï¼‰ï¼Œä¿®æ­£è·¯ç”±æ–¹æ³•ç±»å‹åˆ¤æ–­ï¼›`fused_topk_router.py` ä¸ `fused_topk_bias_router.py` å„è°ƒæ•´ 13 è¡Œå®ç°ï¼›`config.py`ã€`flashinfer_trtllm_moe.py` ä¹Ÿç›¸åº”æ›´æ–°ã€‚

- **Fix `logprobs=0` handling for `/inference/v1/generate` endpoint**  
  PR: <https://github.com/vllm-project/vllm/pull/34010> (SHA `ae2e93f`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/entrypoints/serve/disagg/serving.py`ï¼ˆä¿®æ”¹ 5 è¡Œï¼‰ï¼Œç¡®ä¿ `logprobs=0` æ—¶è¿”å›ç©ºåˆ—è¡¨è€Œéé”™è¯¯ï¼›å¯¹åº”æµ‹è¯• `tests/entrypoints/openai/test_serving_tokens.py` å¢åŠ  26 è¡Œæ–­è¨€ã€‚

- **Fix no attribute error of SharedFusedMoE (DeepSeekâ€‘V3.1 as test model)**  
  PR: <https://github.com/vllm-project/vllm/pull/33993> (SHA `9e9acce`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/model_executor/layers/fused_moe/shared_fused_moe.py`ï¼ˆæ–°å¢ 4 è¡Œï¼‰ï¼Œä¸º `SharedFusedMoE` æ·»åŠ ç¼ºå¤±å±æ€§ï¼›`layer.py` å°å¹…è°ƒæ•´ï¼ˆ+2/-1 è¡Œï¼‰ã€‚

- **[Rocm][Bugfix] Fix dtype not same for gemm_a4w4 op**  
  PR: <https://github.com/vllm-project/vllm/pull/33734> (SHA `fe54382`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx.py`ï¼ˆæ–°å¢ 6 è¡Œï¼Œåˆ é™¤ 1 è¡Œï¼‰ï¼Œç»Ÿä¸€ GEMM A4W4 æ“ä½œçš„ dtypeã€‚

- **[Refactor] Remove align block size logic in `moe_permute`**  
  PR: <https://github.com/vllm-project/vllm/pull/33449> (SHA `77c09e1`)  
  å…³é”®æ–‡ä»¶ï¼š`csrc/moe/moe_permute_unpermute_op.cu`ï¼ˆåˆ é™¤ 39 è¡Œï¼Œæ–°å¢ 5 è¡Œï¼‰ï¼Œ`csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.cu`ï¼ˆåˆ é™¤ 60 è¡Œï¼‰ï¼Œä»¥åŠå¯¹åº”çš„å¤´æ–‡ä»¶å’Œæµ‹è¯• `tests/kernels/moe/test_moe_permute_unpermute.py`ï¼ˆåˆ é™¤ 118 è¡Œï¼Œæ–°å¢ 15 è¡Œï¼‰ï¼Œç®€åŒ– permute é€»è¾‘å¹¶æå‡ç¼–è¯‘é€Ÿåº¦ã€‚

- **[Model Runner V2] support apply penalty for spec decode**  
  PR: <https://github.com/vllm-project/vllm/pull/33251> (SHA `16786da`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/v1/worker/gpu/sample/penalties.py`ï¼ˆæ–°å¢ 37 è¡Œï¼Œåˆ é™¤ 7 è¡Œï¼‰ï¼Œåœ¨ Specâ€‘Decode ä¸­åŠ å…¥æƒ©ç½šé¡¹ï¼›`model_runner.py`ã€`input_batch.py`ã€`sampler.py` ä¹ŸåŒæ­¥æ›´æ–°ä»¥æ”¯æŒæ–°æƒ©ç½šã€‚

- **[DOC] [ROCm] Update docker deployment doc**  
  PR: <https://github.com/vllm-project/vllm/pull/33971> (SHA `aaa2efb`)  
  å…³é”®æ–‡ä»¶ï¼š`docs/deployment/docker.md`ï¼ˆåˆ é™¤ 156 è¡Œï¼Œæ–°å¢ 8 è¡Œï¼‰ï¼Œé‡æ–°ç»„ç»‡ ROCm Docker éƒ¨ç½²è¯´æ˜ï¼›`gpu.cuda.inc.md`ã€`gpu.rocm.inc.md`ã€`gpu.md` ä¹Ÿç›¸åº”æ›´æ–°ã€‚

- **[KV Connector] Add missing method overrides to MultiConnector**  
  PR: <https://github.com/vllm-project/vllm/pull/33292> (SHA `aca5967`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py`ï¼ˆæ–°å¢ 43 è¡Œï¼Œåˆ é™¤ 5 è¡Œï¼‰ï¼Œå®ç°ç¼ºå¤±çš„æŠ½è±¡æ–¹æ³•ï¼›å¯¹åº”å•å…ƒæµ‹è¯• `tests/v1/kv_connector/unit/test_multi_connector.py` å¢åŠ  134 è¡Œè¦†ç›–ã€‚

- **[Log] Optimize duplicate startup log**  
  PR: <https://github.com/vllm-project/vllm/pull/33944> (SHA `67a746e`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/compilation/backends.py`ã€`vllm/utils/deep_gemm.py`ã€`vllm/v1/worker/gpu_worker.py` å„åšå°‘é‡è¡Œæ•°çš„å»é‡å¤„ç†ï¼Œå‡å°‘å¯åŠ¨æ—¶çš„é‡å¤æ—¥å¿—ã€‚

- **[Bugfix] Fix tool calling with fast detokenization (dsv32)**  
  PR: <https://github.com/vllm-project/vllm/pull/33964> (SHA `7bec435`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/tool_parsers/deepseekv32_tool_parser.py`ï¼ˆæ–°å¢ 12 è¡Œï¼‰ï¼Œä¿®å¤åœ¨ä½¿ç”¨ dsv32 å¿«é€Ÿå»æ ‡è®°æ—¶å·¥å…·è°ƒç”¨å¤±æ•ˆçš„é—®é¢˜ã€‚

- **[Docs] Update link to Benchmark CLI documentation**  
  PR: <https://github.com/vllm-project/vllm/pull/33254> (SHA `5c52644`)  
  å…³é”®æ–‡ä»¶ï¼š`benchmarks/README.md`ï¼ˆä¿®æ”¹ 1 è¡Œï¼‰ï¼Œæ›´æ–° Benchmark CLI çš„é“¾æ¥ã€‚

- **[XPU][5/N] add wna16 xpu kernel**  
  PR: <https://github.com/vllm-project/vllm/pull/33973> (SHA `2ce9fe4`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/model_executor/layers/quantization/kernels/mixed_precision/xpu.py`ï¼ˆæ–°å¢ 57 è¡Œï¼Œåˆ é™¤ 66 è¡Œï¼‰ï¼Œå®ç° wna16 ç²¾åº¦çš„ XPU kernelï¼›CI è„šæœ¬ `.buildkite/scripts/hardware_ci/run-xpu-test.sh` ä¹Ÿä½œç›¸åº”è°ƒç”¨ã€‚

- **[Refactor] Consolidate sequence normalization and encâ€‘dec parsing**  
  PR: <https://github.com/vllm-project/vllm/pull/33928> (SHA `cd8b405`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/engine/protocol.py`ï¼ˆæ–°å¢ 7 è¡Œï¼Œåˆ é™¤ 2 è¡Œï¼‰ï¼Œç»Ÿä¸€åºåˆ—å½’ä¸€åŒ–ä¸ç¼–ç â€‘è§£ç è§£æï¼›`vllm/entrypoints/llm.py` å¤§å¹…æ”¹åŠ¨ï¼ˆ+187/-149 è¡Œï¼‰ï¼Œæå‡å…¥å£å‚æ•°å¤„ç†çš„å¯ç»´æŠ¤æ€§ã€‚

- **[Model] Support MiniCPMâ€‘o 4.5**  
  PR: <https://github.com/vllm-project/vllm/pull/33431> (SHA `4707f7e`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/model_executor/models/minicpmo.py`ï¼ˆæ–°å¢ 86 è¡Œï¼Œåˆ é™¤ 7 è¡Œï¼‰ï¼ŒåŠ å…¥ MiniCPMâ€‘o 4.5 çš„æ¨¡å‹é€‚é…ã€‚

- **[Docs] Add sections on process architecture and minimum CPU resources**  
  PR: <https://github.com/vllm-project/vllm/pull/33940> (SHA `c39ee9e`)  
  å…³é”®æ–‡ä»¶ï¼š`docs/design/arch_overview.md`ï¼ˆæ–°å¢ 67 è¡Œï¼‰ï¼Œ`docs/configuration/optimization.md`ï¼ˆæ–°å¢ 46 è¡Œï¼‰ï¼Œå¹¶æ–°å¢ä¸¤å¼ æ¶æ„ç¤ºæ„å›¾ã€‚

- **[ROCm][AITER] Fix AITER import regression for explicit backend selection**  
  PR: <https://github.com/vllm-project/vllm/pull/33749> (SHA `350ca72`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/_aiter_ops.py`ï¼ˆæ–°å¢ 114 è¡Œï¼Œåˆ é™¤ 34 è¡Œï¼‰ï¼Œ`vllm/v1/attention/backends/rocm_aiter_fa.py`ï¼ˆå¾®è°ƒ 9 è¡Œï¼‰ï¼Œç¡®ä¿åœ¨æ˜¾å¼æŒ‡å®šåç«¯æ—¶ AITER æ­£å¸¸å¯¼å…¥ã€‚

- **[FIX] guidance: use max(vocab_size, len(tokenizer)) for n_vocab**  
  PR: <https://github.com/vllm-project/vllm/pull/33509> (SHA `1fb0495`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/v1/structured_output/backend_guidance.py`ï¼ˆ+1/-1 è¡Œï¼‰ï¼Œé˜²æ­¢ vocab å¤§å°ä½äº tokenizer é•¿åº¦å¯¼è‡´é”™è¯¯ã€‚

- **[Bugfix] Fix models and tests for transformers v5**  
  PR: <https://github.com/vllm-project/vllm/pull/33977> (SHA `85ee1d9`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/model_executor/models/audioflamingo3.py`ã€`hunyuan_vision.py`ã€`isaac.py` ç­‰å‡åšå¾®è°ƒï¼ˆ+/-1 è¡Œï¼‰ï¼Œå¹¶æ›´æ–°å¤šæ¨¡æ€æµ‹è¯•ä»¥å…¼å®¹ Transformers 5.0ã€‚

- **Update `WeightTransferConfig` to be more standard**  
  PR: <https://github.com/vllm-project/vllm/pull/33989> (SHA `51a7bda`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/config/weight_transfer.py`ï¼ˆåˆ é™¤ 2 è¡Œï¼‰ï¼Œ`vllm/engine/arg_utils.py`ï¼ˆ+4/-4 è¡Œï¼‰ï¼Œç»Ÿä¸€é…ç½®ç±»çš„å‘½åä¸ç»“æ„ã€‚

- **[Docs] Improve documentation**  
  PR: <https://github.com/vllm-project/vllm/pull/33799> (SHA `6e7b1c4`)  
  å…³é”®æ–‡ä»¶ï¼š`docs/models/supported_models.md`ï¼ˆå¢åˆ å„ 8 è¡Œï¼‰ï¼Œ`vllm/config/model.py`ã€`vllm/entrypoints/llm.py` å„å¾®è°ƒ 2 è¡Œï¼Œæå‡æ–‡æ¡£å¯è¯»æ€§ã€‚

- **[Bugfix][Model] Support LoRA on Qwen3 Output Embedding**  
  PR: <https://github.com/vllm-project/vllm/pull/29816> (SHA `2991dd3`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/model_executor/models/qwen3.py`ã€`qwen3_moe.py`ï¼ˆå„ +5 è¡Œï¼‰ï¼Œ`vllm/lora/layers/logits_processor.py`ï¼ˆ+5/-2 è¡Œï¼‰ï¼Œå¹¶æ–°å¢ LoRA å•å…ƒæµ‹è¯• `tests/lora/test_qwen3_unembed.py`ï¼ˆ100 è¡Œï¼‰ã€‚

- **[torch.compile] Reorganize compilation tests**  
  PR: <https://github.com/vllm-project/vllm/pull/33731> (SHA `ac32e66`)  
  å…³é”®æ–‡ä»¶ï¼š`tests/compile/correctness_e2e/test_async_tp.py`ï¼ˆæ–°å¢ 79 è¡Œï¼‰ï¼Œ`tests/compile/passes/distributed/test_async_tp.py`ï¼ˆé‡å‘½åå¹¶æ”¹åŠ¨ 80 è¡Œï¼‰ï¼Œæ•´ä½“é‡æ„ `vllm/compilation` ç›®å½•ç»“æ„ä»¥æ”¯æŒ vLLM IRã€‚

- **[CPU][BugFix] Fix loading of w8a8int models with bias**  
  PR: <https://github.com/vllm-project/vllm/pull/33582> (SHA `f79d9dc`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit.py`ï¼ˆ+8/-3 è¡Œï¼‰ï¼Œä¿®å¤å¸¦ bias çš„ 8â€‘bit æ¨¡å‹åŠ è½½é”™è¯¯ã€‚

- **Bump HF Hub client to get bug fix**  
  PR: <https://github.com/vllm-project/vllm/pull/33984> (SHA `ba5cbbf`)  
  å…³é”®æ–‡ä»¶ï¼š`requirements/rocm-test.txt`ã€`requirements/test.txt`ï¼ˆå„ +1/-1 è¡Œï¼‰ï¼Œå‡çº§ `huggingface_hub` ç‰ˆæœ¬ã€‚

- **[PaddleOCRâ€‘VL] Add BC for transformers 5.0 config**  
  PR: <https://github.com/vllm-project/vllm/pull/33976> (SHA `233b26a`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/model_executor/models/paddleocr_vl.py`ï¼ˆæ–°å¢ 7 è¡Œï¼‰ï¼Œå…¼å®¹ Transformers 5.0 çš„é…ç½®æ ¼å¼ã€‚

- **Consolidate and fix forbidden import `preâ€‘commit` checks**  
  PR: <https://github.com/vllm-project/vllm/pull/33982> (SHA `791a94b`)  
  å…³é”®æ–‡ä»¶ï¼š`.pre-commit-config.yaml`ï¼ˆä¿®æ”¹ 21 è¡Œï¼‰ï¼Œæ–°å¢ `tools/pre_commit/check_forbidden_imports.py`ï¼ˆ142 è¡Œï¼‰ï¼Œåˆ é™¤æ—§çš„æ£€æŸ¥è„šæœ¬ï¼Œæå‡ CI å¯¹ç¦ç”¨å¯¼å…¥çš„æ£€æµ‹ã€‚

- **support view_from_cpu_tensor on XPU**  
  PR: <https://github.com/vllm-project/vllm/pull/33868> (SHA `e969a16`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/utils/torch_utils.py`ï¼ˆ+6/-2 è¡Œï¼‰ï¼Œ`vllm/v1/worker/gpu/buffer_utils.py`ï¼ˆ+2/-2 è¡Œï¼‰ï¼Œä¸º XPU æ·»åŠ  `view_from_cpu_tensor` æ”¯æŒã€‚

- **Fix `main` preâ€‘commit**  
  PR: <https://github.com/vllm-project/vllm/pull/33975> (SHA `6d8d34b`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/model_executor/models/voyage.py`ï¼ˆ+1/-1 è¡Œï¼‰ï¼Œä¿®æ­£ preâ€‘commit é’©å­ä¸­çš„ `main` å¼•ç”¨ã€‚

- **[cpu][performance] CPU Paged Attention NEON BFMMLA BF16 Implementation**  
  PR: <https://github.com/vllm-project/vllm/pull/32263> (SHA `1363e3d`)  
  å…³é”®æ–‡ä»¶ï¼š`csrc/cpu/cpu_attn_neon_bfmmla.hpp`ï¼ˆæ–°å¢ 682 è¡Œï¼‰ï¼Œå®ç°åŸºäº NEON BFMMLA çš„ BF16 æ³¨æ„åŠ›åŠ é€Ÿï¼›`cpu_attn_impl.hpp`ã€`cpu_attn_neon.hpp` ä¹Ÿç›¸åº”æ›´æ–°ã€‚

- **Onboard voyageâ€‘4â€‘nano**  
  PR: <https://github.com/vllm-project/vllm/pull/33720> (SHA `9655256`)  
  å…³é”®æ–‡ä»¶ï¼š`vllm/model_executor/models/voyage.py`ï¼ˆæ–°å¢ 130 è¡Œï¼‰ï¼ŒåŠ å…¥ Voyageâ€‘4â€‘Nano æ¨¡å‹æ”¯æŒï¼›æ–‡æ¡£ `docs/models/supported_models.md` æ›´æ–°ã€‚

- **[XPU] Replace pip in docker.xpu with uv pip**  
  PR: <https://github.com/vllm-project/vllm/pull/31112> (SHA `6550815`)  
  å…³é”®æ–‡ä»¶ï¼š`docker/Dockerfile.xpu`ï¼ˆæ–°å¢ 46 è¡Œï¼Œåˆ é™¤ 28 è¡Œï¼‰ï¼Œæ”¹ç”¨ `uv` åŠ é€Ÿä¾èµ–å®‰è£…ã€‚

---

### Issuesï¼ˆæˆªè‡³ 2026â€‘02â€‘06ï¼‰

- **[Usage] How to load FP8 Quantised Model - Value error, Found unknown quantization**  
  <https://github.com/vllm-project/vllm/issues/34028> â€“ ä½œè€… *gabinguo*ã€‚  
  Issue æè¿°ç¼ºå°‘å®Œæ•´çš„ `collect_env.py` è¾“å‡ºï¼Œæ­£æ–‡è¢«æˆªæ–­ï¼Œéœ€è¡¥å…¨ç¯å¢ƒä¿¡æ¯ä»¥å®šä½ FP8 é‡åŒ–åŠ è½½é”™è¯¯ã€‚

- **[RFC] Helix (Context + Tensor) Parallelism for Efficient Longâ€‘Context Decoding**  
  <https://github.com/vllm-project/vllm/issues/34018> â€“ ä½œè€… *sungsooha*ã€‚  
  æè®®å®ç° Helix å¹¶è¡Œæ–¹æ¡ˆï¼Œç»“åˆ Context Parallelism ä¸ Tensor Parallelismï¼Œå‡å°‘ KV ç¼“å­˜å¤åˆ¶ï¼Œæå‡é•¿ä¸Šä¸‹æ–‡è§£ç æ•ˆç‡ã€‚

- **[Bug] vLLM crashes when trying to load Devstralâ€‘2â€‘123Bâ€‘Instructâ€‘2512 directly from S3**  
  <https://github.com/vllm-project/vllm/issues/34016> â€“ ä½œè€… *moorglade*ã€‚  
  Issue å†…å®¹ä¸ºç©ºï¼Œä»…æŠ¥å‘Šå´©æºƒï¼Œéœ€è¦æä¾›å †æ ˆä¿¡æ¯å’Œ S3 è·¯å¾„ä»¥å¤ç°ã€‚

- **[Feature] MXFP8 GEMM / Grouped GEMM Kernels for AMD**  
  <https://github.com/vllm-project/vllm/issues/34012> â€“ ä½œè€… *EdalatiAli*ï¼ˆ1 æ¡
### NVIDIA/cutile-python
æ˜¨æ—¥æ— æ›´æ–°ã€‚

## æ€»ç»“
### å±•æœ›ä¸å…³æ³¨ç‚¹
- **ç¡¬ä»¶é€‚é…**ï¼šFlashInfer ä¸ vLLM å¯¹ SM120/SM121 çš„ FP4 tile ä¸ Streamâ€‘K æ”¯æŒï¼Œä»¥åŠ XPU wna16 kernel çš„è½åœ°ï¼Œé¢„ç¤ºç€æ–°ä¸€ä»£ GPU/åŠ é€Ÿå¡çš„ç®—å­åº“æ­£å¿«é€Ÿè·Ÿè¿›ï¼Œåç»­éœ€å…³æ³¨è¿™äº›ç‰¹æ€§åœ¨å®é™…æ¨ç†å·¥ä½œè´Ÿè½½ä¸­çš„ç¨³å®šæ€§ä¸å…¼å®¹æ€§ã€‚\
- **æ€§èƒ½ç“¶é¢ˆ**ï¼šCUTLASS ç¤¾åŒºæŠ¥å‘Šçš„ GEMM+SiLU å»¶è¿Ÿé—®é¢˜æé†’æˆ‘ä»¬ï¼Œå¤åˆç®—å­ï¼ˆå°¤å…¶æ˜¯å¸¦æœ‰éçº¿æ€§ epilogueï¼‰çš„å®ç°ä»æ˜¯æ€§èƒ½ä¼˜åŒ–çš„çƒ­ç‚¹ï¼ŒæœŸå¾…åç»­ upstream ä¿®å¤ã€‚\
- **CI ä¸å¯ç»´æŠ¤æ€§**ï¼šSGLang ä¸ vLLM å¤§å¹…åº¦æ¸…ç† CIã€æ–‡æ¡£ä¸ preâ€‘commit è§„åˆ™ï¼Œæå‡äº†é¡¹ç›®çš„å¯ç»´æŠ¤æ€§å’Œè·¨å¹³å°éƒ¨ç½²ä½“éªŒï¼Œå€¼å¾—å…¶ä»–å¼€æº AI åŸºç¡€è®¾æ–½é¡¹ç›®å€Ÿé‰´ã€‚\
- **æ¨¡å‹ç”Ÿæ€**ï¼švLLM æ–°å¢ MiniCPMâ€‘oã€Voyageâ€‘4â€‘nano ç­‰æ¨¡å‹æ”¯æŒï¼Œé…åˆ FlashInfer çš„ API é‡æ„ï¼Œä¸ºå¤šæ¨¡å‹éƒ¨ç½²æä¾›äº†æ›´ç»Ÿä¸€çš„æ¥å£ï¼Œå»ºè®®å…³æ³¨è¿™äº›æ¨¡å‹åœ¨å®é™…ä¸šåŠ¡åœºæ™¯ä¸­çš„è¡¨ç°ä¸è°ƒä¼˜æŒ‡å—ã€‚\
- **ä¸‹ä¸€æ­¥**ï¼šå…³æ³¨ FlashInfer çš„ SM120/SM121 æ€§èƒ½åŸºå‡†ã€vLLM å¯¹ ROCm/CPU æ–°ç®—å­çš„å®é™…åŠ é€Ÿæ•ˆæœï¼Œä»¥åŠ CUTLASS å¯¹ GEMM+SiLU çš„åç»­ä¼˜åŒ–è¿›å±•ï¼Œä»¥ä¾¿åŠæ—¶è¯„ä¼°å¯¹è‡ªç ”æ¨ç†æ¡†æ¶çš„è¿ç§»ä»·å€¼ã€‚
