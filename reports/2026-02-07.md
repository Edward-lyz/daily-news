今日的 AI Infra 的新闻如下。

## 摘要
本周 **flashinfer** 引入基于 CuTe‑DSL 的 NVFP4 MoE 实现，针对 Blackwell GPU（SM100）完成深度优化，包括全套 benchmark、C++/CUDA 绑定以及 Python API，显著提升 DSR1 场景下的吞吐与延迟；同时修复了 SGLang 使用 DeepSeek‑V3.2 时的 FP4 GEMM 崩溃问题，提供回退路径。**flash‑attention** 新增环境变量 `FLASH_ATTENTION_FWD_TRITON_AMD_CONFIG_JSON`，实现对 AMD Triton 前向配置的即时覆盖，提升了在非自动调优模式下的性能可控性。**sgl‑project** 继续完善 FP4 GEMM 后端文档、优化分布式超时参数、升级 Mooncake 版本并加入 Ascend NPU 支持，提升跨硬件兼容性与部署体验。**vllm** 侧重功能扩展与性能调优：加入 FlashInfer KernelConfig 控制、支持 Mistral3 的 Eagle3 推测解码、扩展 run_batch 支持语音转文字/翻译、优化 ROCm skinny GEMM 与 QK‑RoPE 融合、以及多实例 Ray 目录冲突修复，整体提升了模型兼容性和推理效率。

## 具体内容分析
### deepseek-ai/DeepGEMM
昨日无更新。
### deepseek-ai/FlashMLA
昨日无更新。
### NVIDIA/cutlass
昨日无更新。
### flashinfer-ai/flashinfer
**提交**  
- **PR #2398** – feat: cuteDSL fp4 moe for better DSR1 performance.  
  - **Commit**: `99562e5` – <https://github.com/flashinfer-ai/flashinfer/commit/99562e5eeaa26b08ba5e514bfe22d84404390ca6>  

**主要改动**  

| 模块 / 文件 | 类型 | 关键代码片段 |
|------------|------|--------------|
| `benchmarks/bench_moe_deepseek.py` | 新增 | ```python\n#!/usr/bin/env python3\n\"\"\"DeepSeek‑V3 MoE Performance Benchmark - CuteDSL vs CUTLASS vs TRTLLM.\"\"\"\n``` |
| `csrc/moe_utils_binding.cu` | 新增 | ```cpp\n#include \"flashinfer/trtllm/fused_moe/RoutingKernel.h\"\n#include \"tensorrt_llm/kernels/cuteDslKernels/moeUtils.h\"\n``` |
| `csrc/nv_internal/include/tensorrt_llm/common/config.h` | 新增 | ```cpp\n#ifndef TRTLLM_ABI_NAMESPACE\n#define TRTLLM_ABI_NAMESPACE _v1\n#endif\n``` |
| `csrc/nv_internal/tensorrt_llm/kernels/cuteDslKernels/moeUtils.cu` | 新增 | ```cpp\ntemplate <typename T>\nauto constexpr bitsPerElement = sizeof(T) * 8;\n``` |
| `csrc/nv_internal/tensorrt_llm/kernels/cuteDslKernels/moeUtils.h` | 新增 | ```cpp\nenum class MoeActivationType { Gelu=0, Relu=1, Silu=2, Swiglu=3, Geglu=4, Identity=5 };\n``` |
| `csrc/nv_internal/tensorrt_llm/kernels/cutlass_kernels/moe_gemm/moe_kernels.cuh` | 新增 | ```cpp\ntemplate <template <class> class ActFn>\nstruct IdentityAdaptor { ... };\n``` |
| `flashinfer/__init__.py` | 修改 | ```python\nimport contextlib\nwith contextlib.suppress(ImportError):\n    from .fused_moe import cute_dsl_fused_moe_nvfp4, CuteDslMoEWrapper\n``` |
| `flashinfer/cute_dsl/__init__.py` | 修改 | ```python\nfrom .utils import (\n    is_cute_dsl_available,\n    make_ptr,\n    get_cutlass_dtype,\n    get_num_sm,\n    convert_sf_to_mma_layout,\n    convert_sf_from_mma_layout,\n    get_mma_sf_shape,\n)\n``` |
| `flashinfer/cute_dsl/utils.py` | 修改 | ```python\ndef ceil_div(a: int, b: int) -> int:\n    return (a + b - 1) // b\n``` |
| `flashinfer/fused_moe/__init__.py` | 修改 | ```python\ntry:\n    from .cute_dsl import cute_dsl_fused_moe_nvfp4, CuteDslMoEWrapper\n    _cute_dsl_available = True\nexcept ImportError:\n    _cute_dsl_available = False\n``` |
| `flashinfer/fused_moe/cute_dsl/__init__.py` | 新增 | ```python\nif is_cute_dsl_available():\n    from .fused_moe import cute_dsl_fused_moe_nvfp4, CuteDslMoEWrapper\n``` |
| `flashinfer/fused_moe/cute_dsl/blackwell/__init__.py` | 新增 | ```python\nfrom .blockscaled_contiguous_grouped_gemm import Sm100BlockScaledContiguousGroupedGemmKernel\n``` |
| `flashinfer/fused_moe/cute_dsl/blackwell/blockscaled_contiguous_grouped_gemm.py` | 新增（约 2.4k 行） | 实现 Blackwell GPU 上的 FP4 MoE Block‑scaled GEMM，使用 CuTe‑DSL 生成的 kernel。 |
| `flashinfer/fused_moe/cute_dsl/blackwell/blockscaled_contiguous_gather_grouped_gemm_swiglu_fusion.py` | 新增（约 3.6k 行） | SwigLU 融合实现，支持 FP4 量化的专家并行。 |
| `flashinfer/fused_moe/cute_dsl/blackwell/blockscaled_contiguous_grouped_gemm_swiglu_fusion.py` | 新增（约 2.8k 行） | SwigLU 融合的调度与后处理逻辑。 |
| `flashinfer/fused_moe/cute_dsl/blackwell/blockscaled_contiguous_grouped_gemm_finalize_fusion.py` | 新增（约 2.6k 行） | 完成阶段的结果归约与量化恢复。 |

> **概览**：本次 PR 为 FlashInfer 引入了基于 CuTe‑DSL 的 NVFP4 MoE 实现，针对 Blackwell（SM100）架构做了深度优化，新增了完整的 benchmark、C++/CUDA 绑定以及 Python API，提升了 DSR1 场景下的吞吐与延迟表现。

---

**Issues**  

- **#2516** – *SGLang Deepseek v3.2 NVFP4 GEMM Kernel Crash*  
  - 链接：<https://github.com/flashinfer-ai/flashinfer/issues/2516>  
  - 摘要：在使用 SGLang（v0.5.8.post1‑cu130）并开启 SPECV2 时，DeepSeek‑V3.2‑NVFP4 的 FP4 GEMM 在 1k 输入/1k 输出、并发 10 的情况下触发 `RuntimeError: Error in function 'run'`，导致服务崩溃。该问题与新加入的 CuteDSL FP4 MoE 实现的兼容性有关，后续将通过回退到 Cutlass 实现或修复 kernel 参数传递来解决。
### Dao-AILab/flash-attention
**提交**  
- 912c6c4 – Add `FLASH_ATTENTION_TRITON_AMD_CONFIG_JSON` env var support (PR [#2239](https://github.com/Dao-AILab/flash-attention/pull/2239))  
  - Commit URL: <https://github.com/Dao-AILab/flash-attention/commit/912c6c451863403414323be39eecb0d95e124d4c>

**Issues**  
- 暂无。

**改动要点**  

- **README.md**  
  - 新增环境变量 `FLASH_ATTENTION_FWD_TRITON_AMD_CONFIG_JSON` 的使用说明。  
  - 示例代码片段：  
    ```sh
    FLASH_ATTENTION_FWD_TRITON_AMD_CONFIG_JSON='{"BLOCK_M":128,"BLOCK_N":64,"waves_per_eu":1,"PRE_LOAD_V":false,"num_stages":1,"num_warps":8}'
    ```

- **flash_attn/flash_attn_triton_amd/utils.py**  
  - 引入 `json`、`logging`，新增 `logger`。  
  - 实现 `FLASH_ATTENTION_FWD_TRITON_AMD_CONFIG_JSON` 的解析，生成 `triton.Config` 并保存至 `FWD_CONF_OVERRIDE`。  
  - 解析错误时记录警告日志。

- **flash_attn/flash_attn_triton_amd/fwd_prefill.py**  
  - 调整导入顺序，加入 `FWD_CONF_OVERRIDE`。  
  - 在 `get_fwd_prefill_configs` 中，若未开启 autotune 且 `FWD_CONF_OVERRIDE` 存在，直接返回覆盖配置。

- **整体行为**  
  - 当 `FLASH_ATTENTION_TRITON_AMD_AUTOTUNE="TRUE"` 时，覆盖配置被忽略，保持原有自动调优逻辑。  
  - 未开启 autotune 时，可通过环境变量一次性指定前向预填充的 Triton 配置，省去硬编码默认值的限制。
### sgl-project/sglang
- **64950d8** – 新增 `return_file_paths_only` 参数，支持服务器直接保存视频文件以降低张量传输开销。相关文件：`sampling_params.py`（新增字段与 CLI 参数），`diffusion_generator.py`、`http_server.py`、`openai/utils.py`（调整输出处理逻辑），`utils.py`（引入 `save_outputs`），以及 GPU worker 与调度批处理的细节修改。  
  PR/Commit: https://github.com/sgl-project/sglang/commit/64950d8f9735e0e0f652092b695d21e3ca34f310  

- **31d4cd2** – 修复分布式运行时 `dist_timeout` 参数的生效问题，确保超时设置被正确传递。涉及文件：`parallel_state.py`、`gpu_worker.py`、`server_args.py`。  
  PR/Commit: https://github.com/sgl-project/sglang/commit/31d4cd2ffd9662568765efbc6a4a3939c613ad9a  

- **fddef76** – 更新文档中已废弃的 `--fp4-gemm-backend` 参数说明，保持文档与代码同步。涉及文件：`docs/advanced_features/server_arguments.md`、`modelopt_quant.py`、`server_args.py`。  
  PR/Commit: https://github.com/sgl-project/sglang/commit/fddef766199e85c8200e6b150419e2643ac46a2c  

- **d792aa7** – 移除 GLM‑Image DIT 中不必要的 `norm_type` 参数，简化模型调用。涉及文件：`glm_image.py`。  
  PR/Commit: https://github.com/sgl-project/sglang/commit/d792aa761873c92b867c311b6e4dd2c060103171  

- **eb4cf1d** – CI 中跳过 `test_multi_lora_backend.py` 的部分不稳定子测试，提高 CI 稳定性。涉及文件：`lora_utils.py`。  
  PR/Commit: https://github.com/sgl-project/sglang/commit/eb4cf1dfc46bdc7a90bfe3954044dd35bc7d1637  

- **c47c2f9** – 更新 CUDA 13 安装指南，先安装 PyTorch 再安装其他依赖，避免冲突。涉及文件：`install.md`。  
  PR/Commit: https://github.com/sgl-project/sglang/commit/c47c2f9466dfcbec8256d28b8b6d300ebee22d5f  

- **52401be** – 将 Mooncake 版本升级至 0.3.9，同步 Dockerfile、CI 脚本及对应测试。涉及文件：`Dockerfile`、`ci_install_dependency.sh`、`test_mooncake_ep_small.py`。  
  PR/Commit: https://github.com/sgl-project/sglang/commit/52401bec1d1c624977d6f9d7a9022e0c51880aad  

- **baec650** – 为 LTX2/MOVA 模型在 DIT 中应用 `fused_norm_scale_shift`，提升计算效率。涉及文件：`mova_audio.py`、`mova_video.py`、`mova_video_dit.py`。  
  PR/Commit: https://github.com/sgl-project/sglang/commit/baec650462e351bc338098efdc21726e66328651  

- **e834b85** – 新增 Ascend NPU 支持文档，详细列出特性与兼容模型。涉及文件：`ascend_npu_support_features.md`（新增 487 行），`ascend_npu_support_models.md`（更新 13 行）。  
  PR/Commit: https://github.com/sgl-project/sglang/commit/e834b85ab677971ffcdf60edb373f4472fe37224  

- **1552aab** – 为 `execute_shell_command` 添加环境变量支持，提升工作流灵活性。涉及文件：`.github/workflows/execute-notebook.yml`、`utils.py`。  
  PR/Commit: https://github.com/sgl-project/sglang/commit/1552aab741bf89c241b10e06297ee657597bd758  

- **4637970** – 优化 Qwen3Next 中的 `fused_sigmoid_gating_delta_rule_update_kernel` 实现，提升注意力层性能。涉及文件：`fused_sigmoid_gating_recurrent.py`。  
  PR/Commit: https://github.com/sgl-project/sglang/commit/4637970dfb369d900936136c98cb5a6663385884  

- **9fbec79** – 回滚 “在 aarch64 wheel 中启用完整 kernel” 的改动，恢复原有构建流程。涉及文件：`.github/workflows/release-whl-kernel.yml`、`sgl-kernel/Dockerfile`、`sgl-kernel/build.sh`。  
  PR/Commit: https://github.com/sgl-project/sglang/commit/9fbec799063b23f9d75a7d81006d0612b8bbd547  

---

- **#18430** – Notebook CI 在仅修改 `docs/**/*.ipynb` 时未被触发，导致 PR 检查缺失。  
  作者：alphabetc1，创建于 2026‑02‑07T21:10:24Z  

- **#18427** – 建议清理 SGLang 文档与 Cookbook 中的重复内容，并将模型使用示例统一迁移至 Cookbook。  
  作者：zhaochenyang20，创建于 2026‑02‑07T19:54:48Z  

- **#18421** – Scheduler 子进程因 NCCL 超时崩溃后，主 HTTP 服务仍保持运行但无法处理请求，健康检查检测延迟较大。  
  作者：Simon‑Li，创建于 2026‑02‑07T15:45:21Z  

- **#18414** – PD 分片在使用 nixl 传输后端时出现间歇性失败，导致性能不稳定。  
  作者：ChowXu，创建于 2026‑02‑07T10:39:52Z  

- **#18410** – Kimi 2.5 在 TP4 DP2 配置下偶发崩溃，复现于特定错误数据集。  
  作者：haowen‑han，创建于 2026‑02‑07T09:20:08Z  

- **#18393** – speculative_decoding.ipynb 执行时间远超其他文档 notebook，建议优化以缩短 Document CI 总时长。  
  作者：zhaochenyang20，创建于 2026‑02‑07T05:07:27Z  

- **#18392** – sgl‑kernel 在 Blackwell CUDA‑13.0 环境下构建失败，影响源码编译与二进制发布。  
  作者：AlbertZhangHIT，创建于 2026‑02‑07T05:05:04Z
### vllm-project/vllm
提交  
- **[7fcb705]** – *Skip GCS test*  
  - PR: <https://github.com/vllm-project/vllm/commit/7fcb705b80176fbdb92dd94f27f566c876d8b8a9>  
  - 变更文件: `tests/model_executor/model_loader/runai_streamer_loader/test_runai_model_streamer_loader.py`（+4 行）  
  - 简述: CI 中跳过 GCS 相关测试，提升构建速度。  

- **[b956cdf]** – *Fix run_batch docs*  
  - PR: <https://github.com/vllm-project/vllm/commit/b956cdf818dbb05f4dc327cc76b46c18a1e37cf8>  
  - 变更文件: `vllm/entrypoints/openai/run_batch.py`（+5 / -2 行）  
  - 简述: 更新 `run_batch` 接口文档，纠正参数说明。  

- **[ed17f54]** – *Perf tuning & case expansion for wvSplitKrc*  
  - PR: <https://github.com/vllm-project/vllm/commit/ed17f54c8bccc2f1a19f83f65b36d837698b8792>  
  - 变更文件:  
    - `csrc/rocm/skinny_gemms.cu`（+143 / -184 行）  
    - `tests/kernels/quantization/test_rocm_skinny_gemms.py`（+52 / -31 行）  
    - `vllm/model_executor/layers/utils.py`（+19 / -8 行）  
  - 简述: 大幅优化 ROCm skinny GEMM 实现并补充测试，提升量化性能。  

- **[860981d]** – *Make directory exist ok for Ray multiple replicas*  
  - PR: <https://github.com/vllm-project/vllm/commit/860981d8d85bbf810d8bdf14d799407adad67b1d>  
  - 变更文件: `vllm/transformers_utils/runai_utils.py`（+1 / -3 行）  
  - 简述: 修复 Ray 在单实例上启动多个副本时目录冲突的问题。  

- **[52181ba]** – *Update DeepGEMM version pin in Dockerfile*  
  - PR: <https://github.com/vllm-project/vllm/commit/52181baaea01f8cc64e2ffde7761a3ff3b7e4d3e>  
  - 变更文件: `docker/Dockerfile`, `docker/versions.json`, `tools/install_deepgemm.sh`（各 +1 / -1 行）  
  - 简述: 同步 DeepGEMM 版本号，确保镜像构建一致性。  

- **[de3869b]** – *Move checks out of unified_kv_cache_update op*  
  - PR: <https://github.com/vllm-project/vllm/commit/de3869bb4db76658bc4775ba4ba5fa6ef546237a>  
  - 变更文件: 多个注意力实现文件（共 +80 / -101 行）  
  - 简述: 将 KV 缓存更新前的检查迁出自定义算子，提升代码可维护性。  

- **[ce9b3cd]** – *Apply PluggableLayer to mamba layers*  
  - PR: <https://github.com/vllm-project/vllm/commit/ce9b3cd3e91c2f6c901f12b038ee7797aaa5c829>  
  - 变更文件: `vllm/model_executor/layers/mamba/mamba_mixer.py`, `mamba_mixer2.py`, `models/plamo2.py`（共 +13 / -31 行）  
  - 简述: 引入可插拔层机制到 Mamba 相关层，便于自定义实现。  

- **[db4ede9]** – *Enable Step3p5ForCausalLM testing*  
  - PR: <https://github.com/vllm-project/vllm/commit/db4ede97434300e2dea211f7234024dd7fe34bfc>  
  - 变更文件: `docs/models/supported_models.md`, `tests/models/registry.py`, `vllm/model_executor/models/step3p5.py`（共 +28 / -32 行）  
  - 简述: 添加 Step3p5 模型支持并更新文档与测试注册。  

- **[2cb2340]** – *Add transcription/translation support to run_batch*  
  - PR: <https://github.com/vllm-project/vllm/commit/2cb2340f7a91a2f6629aacc45563af20897812c0>  
  - 变更文件: `tests/entrypoints/openai/test_run_batch.py`（+140 行），`vllm/entrypoints/openai/run_batch.py`（+407 / -99 行），`vllm/entrypoints/openai/speech_to_text/serving.py`（+8 / -2 行）  
  - 简述: 扩展 `run_batch` 接口以支持语音转文字和翻译功能。  

- **[4df44c1]** – *Enable Eagle3 speculative decoding for Mistral3*  
  - PR: <https://github.com/vllm-project/vllm/commit/4df44c16ba8c4e44aeb7bf0dd622933c693d7613>  
  - 变更文件: `vllm/model_executor/models/mistral3.py`（+9 / -1 行）  
  - 简述: 为 Mistral3 模型开启 Eagle3 推测解码，提高吞吐。  

- **[81fe69c]** – *Stop compiling identical artifacts*  
  - PR: <https://github.com/vllm-project/vllm/commit/81fe69cae5cb0d3aff23934064c639e9c9815a81>  
  - 变更文件: `tests/compile/test_cold_start.py`（+4 / -4 行），`vllm/compilation/backends.py`（+60 / -7 行）  
  - 简述: 优化 torch.compile 流程，避免重复编译相同工件。  

- **[dd6a6e1]** – *Add KernelConfig flag for FlashInfer autotune*  
  - PR: <https://github.com/vllm-project/vllm/commit/dd6a6e119062027fae8964335c525796f71cdc2b>  
  - 变更文件: `vllm/config/kernel.py`（新增 44 行），以及若干配置/入口文件的引用更新（共 +104 行）  
  - 简述: 引入 `KernelConfig` 开关，允许用户手动控制 FlashInfer 自动调优。  

- **[edb359c]** – *Define render_cmpl and render_chat*  
  - PR: <https://github.com/vllm-project/vllm/commit/edb359cce460de499331abb8f9c202aa40bbb335>  
  - 变更文件: 多个渲染与协议文件（共 +150 / -75 行）  
  - 简述: 新增渲染函数以统一聊天与补全的输出格式。  

- **[6ed5eda]** – *Pin grpcio-tools==1.78.0*  
  - PR: <https://github.com/vllm-project/vllm/commit/6ed5eda30009a25b6ca827064880b20a26c9afc2>  
  - 变更文件: `pyproject.toml`, `requirements/*.txt`（共 +12 / -5 行）  
  - 简述: 固定 gRPC 工具链版本，防止构建不一致。  

- **[11a4c9d]** – *Simplify get_max_tokens*  
  - PR: <https://github.com/vllm-project/vllm/commit/11a4c9d30d205ee4cb8e9a92e34834fc6585fce0>  
  - 变更文件: 多个 OpenAI 接口实现（共 +8 / -30 行）  
  - 简述: 精简 `get_max_tokens` 辅助函数，提升代码可读性。  

- **[15a0b9e]** – *Fix spelling errors*  
  - PR: <https://github.com/vllm-project/vllm/commit/15a0b9e570dc8bfe716a7d76d50716898123dbae>  
  - 变更文件: 多处测试与实现文件（共 +10 / -10 行）  
  - 简述: 修正拼写错误，保持文档与代码一致性。  

- **[c490d8c]** – *Pin lm-eval version for ROCm CI*  
  - PR: <https://github.com/vllm-project/vllm/commit/c490d8cc7389860c1c6f9ae4181c023c7350738f>  
  - 变更文件: `requirements/rocm-test.txt`（+1 / -1 行）  
  - 简述: 解决 ROCm 多模态评估的依赖冲突。  

- **[48312e5]** – *Make PlaceholderRange.get_num_embeds a method*  
  - PR: <https://github.com/vllm-project/vllm/commit/48312e579a6de16b3d15a40796ef165ed141c8dc>  
  - 变更文件: 多个多模态与调度相关文件（共 +11 / -12 行）  
  - 简述: 将 `get_num_embeds` 改为实例方法，提升 API 灵活性。  

- **[bc32444]** – *Add enable_sm120_or_later for SM121 CUTLASS support*  
  - PR: <https://github.com/vllm-project/vllm/commit/bc32444b238d2ec3726f599cf3fc67dbaf51a6c6>  
  - 变更文件: `csrc/cutlass_extensions/common.hpp`, `csrc/quantization/.../scaled_mm_blockwise_sm120_fp8_dispatch.cuh`（共 +13 / -1 行）  
  - 简述: 为 DGX Spark（SM121）添加 CUTLASS 编译开关。  

- **[18e8545]** – *Revert: add handle_deprecated back*  
  - PR: <https://github.com/vllm-project/vllm/commit/18e85452979d2f974f2c193d159816a893fbc253>  
  - 变更文件: `vllm/config/utils.py`（+25 行）  
  - 简述: 恢复 `handle_deprecated` 实用函数，兼容旧配置。  

- **[6f7adc5]** – *Fix description in plugin_system.md*  
  - PR: <https://github.com/vllm-project/vllm/commit/6f7adc533a3ad6b45d8e5a92cfa2e0094daa702b>  
  - 变更文件: `docs/design/plugin_system.md`（+1 / -1 行）  
  - 简述: 微调插件系统文档描述。  

- **[40218a8]** – *Revert token rank comparison diff for ModelRunner V2*  
  - PR: <https://github.com/vllm-project/vllm/commit/40218a82bad0bc772d75551a8009799f1a001db7>  
  - 变更文件: `vllm/v1/worker/gpu/sample/logprob.py`（+1 / -1 行）  
  - 简述: 暂时回滚 token 排序比较的行为差异。  

- **[1c3b220]** – *Add backward-compatible import aliases for translations*  
  - PR: <https://github.com/vllm-project/vllm/commit/1c3b22058f3407c0bf517ef05b72d03a192974b3>  
  - 变更文件: 新增 `vllm/entrypoints/openai/translations/*`（共 69 行）  
  - 简述: 为旧版翻译模块提供兼容别名，防止导入错误。  

- **[3920caf]** – *Fix _fused_moe_lora_expand signature mismatch*  
  - PR: <https://github.com/vllm-project/vllm/commit/3920cafdd6a8b20a76fa4805e010ae5da8f3a45f>  
  - 变更文件: `vllm/lora/ops/triton_ops/fused_moe_lora_op.py`（-1 行）  
  - 简述: 修正 fused MoE LoRA 扩展函数签名，避免运行时错误。  

- **[55aeec0]** – *Fix Whisper tokenization*  
  - PR: <https://github.com/vllm-project/vllm/commit/55aeec04f52a9d347c3299c4f4d0df2683ffac00>  
  - 变更文件: `vllm/model_executor/models/whisper.py`（+8 行）  
  - 简述: 修复 Whisper 模型的分词实现，提升语音转文字准确性。  

- **[9060771]** – *Fix QK Norm+RoPE fusion pattern on B200+FP8*  
  - PR: <https://github.com/vllm-project/vllm/commit/906077181b21128576018604562e55fbc7f70c34>  
  - 变更文件: 多个编译测试与新 Pass `split_coalescing`（共 +154 / -6 行）  
  - 简述: 改进 QK 归一化与 RoPE 融合的模式匹配，新增分块合并 Pass。  

- **[89a385d]** – *Pause/Resume with keep requests for single engine (RL)*  
  - PR: <https://github.com/vllm-project/vllm/commit/89a385d79fb511f439f8ebe9d38d6811b5b24a91>  
  - 变更文件: 示例 `examples/offline_inference/pause_resume.py`（+108 行）以及多处引擎实现（共 +537 行）  
  - 简述: 为单引擎提供暂停/恢复功能，支持保持请求状态，提升 RL 场景可用性。  

---

Issues  
- **[#34067]** – *nightly docker and wheels for ROCm*  
  - <https://github.com/vllm-project/vllm/issues/34067>  
  - 需求：提供 ROCm 的 nightly Docker 镜像与 wheel 包，便于快速实验。  

- **[#34058]** – *Installation failure on Ubuntu*  
  - <https://github.com/vllm-project/vllm/issues/34058>  
  - 描述：用户在 Ubuntu 环境下 `pip install vllm` 失败，需排查依赖冲突。  

- **[#34054]** – *PD disaggregation on same host with nixl connector cannot use NVLink*  
  - <https://github.com/vllm-project/vllm/issues/34054>  
  - 描述：同机多卡部署时未使用 NVLink 进行 KV 缓存传输，导致性能下降。  

- **[#34042]** – *Support for MedGemma Models*  
  - <https://github.com/vllm-project/vllm/issues/34042>  
  - 需求：在 vLLM 中加入对 MedGemma 系列模型的兼容。  

- **[#34041]** – *NumPy version mismatch between modules*  
  - <https://github.com/vllm-project/vllm/issues/34041>  
  - 描述：不同依赖要求的 NumPy 版本冲突，导致导入错误。  

- **[#34040]** – *Encoder cache overflow for multimodal LLMs*  
  - <https://github.com/vllm-project/vllm/issues/34040>  
  - 描述：多模态模型在图像输入过长时触发 encoder cache 限制，需要调大 `--limit-mm-per-p` 参数。  

- **[#34034]** – *vLLM-compile should not run decoder forward pass during compilation*  
  - <https://github.com/vllm-project/vllm/issues/34034>  
  - 建议：在冷启动编译阶段避免执行完整解码前向，以缩短编译时间。
### NVIDIA/cutile-python
昨日无更新。

## 总结
建议关注 FlashInfer 的 CuTe‑DSL MoE 在更多模型上的适配进度，并在 SGLang 中验证回退方案的稳定性；对使用 AMD GPU 的团队，可尝试新环境变量以快速调优 flash‑attention；vLLM 的 KernelConfig 与 Eagle3 方案值得在生产环境中做基准测试，以评估实际加速收益。
