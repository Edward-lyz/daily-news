今日的 AI Infra 的新闻如下。

## 摘要
## 本周关键变更概览
- **FlashMLA** 仍在处理 CUDA/CUTLASS 的 varlen 选项、安全校验、错误信息以及 workspace 管理等核心缺陷，后续计划完善 token‑count 与 batch‑size 逻辑并实现安全的 workspace 分配。  
- **flashinfer** 引入两套 GitHub Actions 工作流（`pr-test.yml` 与 `pr-test-runner.yml`）以隐藏失败的 spot job，并在 Issue 中提出对 Jetson AGX Thor / Spark（SM110）支持的需求，表明对边缘 GPU 的兼容性关注正在上升。  
- **flash‑attention** 大幅重构 Hopper（SM90）和 SM100 代码路径：采用 `setmaxregister_*` 替代旧的 warpgroup 寄存器管理、迁移 DSL 实现至 `quack`、更新布局与数学库调用，并扩展 GPU 轮询测试，显著提升了 Hopper 及后续架构的性能可维护性。  
- **sglang** 近期提交覆盖模型适配（Qwen‑3、Kimi‑K2.5、LFM2、Diffusion）、张量并行、NVFP4 权重映射、平台扩展至 Ascend NPU、CI 稳定性以及后端选择修复，显示出对多模态、跨硬件生态的快速迭代。  
- **vllm** 完成了对 Torch 2.10 的升级、ModelOpt MXFP8 与 NVIDIA B200 FP8‑W8A8 的原生支持、`torch.compile` 下的 MOE 冷启动优化、Marlin MOE 兼容性修复以及 ROCm CI 稳定性提升，同时简化了 DeepSeek‑V32 分词器，实现了更轻量的部署路径。

## 具体内容分析
### deepseek-ai/DeepGEMM
昨日无更新。
### deepseek-ai/FlashMLA
提交  
- 本次 **FlashMLA** 仓库暂无关联的 Pull Request（PR）链接。  

Issues  
- **[CUDA/CUTLASS] Improvements for varlen option derivation, input validation, can_implement errors, and workspace handling**  
  - 作者：red1239109-cmd  
  - 创建时间：2026‑02‑08 09:21:21 UTC  
  - 链接：https://github.com/deepseek-ai/FlashMLA/issues/161  
  - 评论数：0  
  - **摘要**：  
    - 现有 FMHA / attention 的 CUTLASS 包装在四个关键点上存在潜在风险：  
      1. **变长（varlen）选项推导不安全**：`get_options()` 中使用 `options.q = q.size(0) / options.b;` 等除法仅适用于固定序列长度的密集批次，在变长模式下会导致错误的维度计算。  
      2. **输入张量校验过于宽松**，缺少对形状、数据类型以及 batch‑size 一致性的严格检查。  
      3. `op.can_implement()` 失败时的错误信息不够明确，难以定位是硬件不支持还是参数不合法。  
      4. **工作空间（workspace）管理仍标记 TODO**，在实际运行时可能触发未分配或溢出错误。  
  - **后续需求**：  
    - 为变长模式实现安全的 token‑count 与 batch‑size 计算逻辑。  
    - 增强输入张量的 shape 与 dtype 验证，提供统一的异常信息。  
    - 改进 `can_implement` 的错误报告，输出具体不满足的约束。  
    - 完成 workspace 分配与释放的实现，确保运行时安全。  

> **注意**：Issue 内容被截断（`body_truncated: true`），完整描述未提供，建议在后续跟进时补全完整正文以便更精准定位问题。
### NVIDIA/cutlass
昨日无更新。
### flashinfer-ai/flashinfer
**提交**  
- **SHA**: `d5eaa42`  
- **链接**: [https://github.com/flashinfer-ai/flashinfer/commit/d5eaa429b1c2c3cc51fe078028551fef10ca9cc9](https://github.com/flashinfer-ai/flashinfer/commit/d5eaa429b1c2c3cc51fe078028551fef10ca9cc9)  
- **作者**: Yong Wu  
- **日期**: 2026‑02‑08 20:10:43 UTC  
- **提交信息**: `ci: refactor PR tests to hide failed spot jobs from PR status (#2500)`  

**文件变更**  

| 文件 | 状态 | 增删行数 |
|------|------|----------|
| `.github/workflows/pr-test-runner.yml` | 新增 | +674 / 0 |
| `.github/workflows/pr-test.yml` | 修改 | +129 / –556 |

- **`.github/workflows/pr-test-runner.yml`**（新增）  
  ```yaml
  # PR Test Runner - Runs tests and updates check runs.
  # Triggered by pr-test.yml via workflow_dispatch. Not visible on PR status.

  name: PR Test Runner

  on:
    workflow_dispatch:
      inputs:
        pr_head_sha:
          description: 'PR head SHA for check run updates'
          required: true
          type: string
        docker_tag:
          description: 'Docker image tag'
          required: true
          type: string
        # …（后续省略）
  ```
  *注：`patch_truncated` 为 true，未展示完整 diff。*

- **`.github/workflows/pr-test.yml`**（修改）  
  ```yaml
  # PR Test Gateway - Creates check runs and triggers test runner
  name: PR Test

  concurrency:
    # …（后续省略）

  permissions:
    contents: read
    pull-requests: write
    actions: write
  ```
  *注：`patch_truncated` 为 true，未展示完整 diff。*

**统计**  
- 总变更文件数：2  
- 总行数增删：+803 / –556  

---

**Issue**  
- **标题**: Support SM110 (Jetson AGX Thor, Spark)  
- **链接**: [https://github.com/flashinfer-ai/flashinfer/issues/2522](https://github.com/flashinfer-ai/flashinfer/issues/2522)  
- **作者**: jokelord  
- **创建时间**: 2026‑02‑08 14:11:49 UTC  
- **简要描述**: 现有 XQA 仅支持 SM90、SM100、SM120 GPU，用户希望在 SM110（Jetson AGX Thor、Spark）上也能使用，以满足边缘计算和机器人场景的需求。  

---  

*以上信息基于提供的 JSON 数据，若有 `patch_truncated` 或 `body_truncated` 为 true 的情况，已在相应位置说明。*
### Dao-AILab/flash-attention
**提交**  

- `72c7ba4` – Fix Hopper tests (#2242)  
  <https://github.com/Dao-AILab/flash-attention/commit/72c7ba484d33ca43711897de44e5bb8e0589a7f4>  

- `2a8d39c` – **DSL**：`warpgroup_reg_alloc` → `setmaxregister_increase`  
  <https://github.com/Dao-AILab/flash-attention/commit/2a8d39c54075e8dcc0e49a8bcfe54f9c833d7cbd>  

- `17d2943` – **Bwd, Sm90**：`score_mod` 与 `score_mod_bwd` 设为 `partial` 函数  
  <https://github.com/Dao-AILab/flash-attention/commit/17d29436b866c8670eafe742cf7924eb4a9d90f6>  

- `deb1830` – **Bwd, Sm100**：简化 `PipelineTmaUmma.create` 调用  
  <https://github.com/Dao-AILab/flash-attention/commit/deb183092ba7e8d6eb3fc5fdb2e46f9bad63b0da>  

- `90f10fa` – **DSL**：使用 `cute.math.{exp2, log2, log}` 取代手写实现  
  <https://github.com/Dao-AILab/flash-attention/commit/90f10faafd2e85a57595fc26f3b8a9212626e2e1>  

- `8dd8019` – **Layout**：改用 `quack.layout_utils.mma_partition_C_vec`  
  <https://github.com/Dao-AILab/flash-attention/commit/8dd8019cefa4180c7e187d3ad481629739dde819>  

- `b735ef2` – **Layout**：引入 `reshape_acc_to_mn` 与 `reshape_acc_to_frgA`（来自 `quack`）  
  <https://github.com/Dao-AILab/flash-attention/commit/b735ef24c2998848b0fa629456dc1bc38ddd3ddd>  

- `7edcf59` – **DSL**：使用 `cute.arch.warp_reduction_max / sum`  
  <https://github.com/Dao-AILab/flash-attention/commit/7edcf59c9ec652358e84ab222315240de01cd1ea>  

- `81f2c2d` – **Sm90**：迁移至 `quack.sm90_utils` 中的函数  
  <https://github.com/Dao-AILab/flash-attention/commit/81f2c2dcdce01007b5f19f3331a9ea2f311be6d4>  

- `d39b629` – **DSL**：删除 `coord_offset_i64`、`domain_offset_i64`、`elem_pointer_i64`  
  <https://github.com/Dao-AILab/flash-attention/commit/d39b6292bb9d2f3d24fb361466f23d3101f926bc>  

- `5a66f2c` – **DSL**：改用 `cute.arch` 实现的 `fma/mul/add_packed_f32x2`  
  <https://github.com/Dao-AILab/flash-attention/commit/5a66f2cca3e381029cf85f8d076f08aa8ae74908>  

- `a804a5a` – **DSL**：将旧 fence 替换为 `cute.arch.fence_view_async_shared()`  
  <https://github.com/Dao-AILab/flash-attention/commit/a804a5a3ef783af731e9a032b6f2101fddaf0e6b>  

- `48af662` – **测试**：`pytest-dist` 支持 GPU 轮询（#2241）  
  <https://github.com/Dao-AILab/flash-attention/commit/48af662c53b48c3c7ce6e38680a938f6195da75c>  

- `abaa878` – **CUTE**：升级至最新 `cutedsl`（#2216）  
  <https://github.com/Dao-AILab/flash-attention/commit/abaa87875d573e67d7886f2f6c1efa0d0c840c3b>  

---

**Issues**  

暂无关联 Issue。  

---

### 关键文件变更概览（摘录）

| 文件 | 变更类型 | 关键代码片段 |
|------|----------|--------------|
| `flash_attn/cute/interface.py` | 新增变量检查 | ```python\nis_varlen = (\n    cu_seqlens_q is not None\n    or cu_seqlens_k is not None\n    or seqused_q is not None\n    or seqused_k is not None\n)\nassert not is_varlen, "varlen backward is not yet supported on sm90"\n``` |
| `flash_attn/cute/flash_bwd_sm90.py` | 替换寄存器管理 | ```python\ncute.arch.setmaxregister_decrease(self.num_producer_regs)\n...\ncute.arch.setmaxregister_increase(self.num_mma_regs)\n``` |
| `flash_attn/cute/flash_bwd_sm90.py` | 引入 `score_mod`/`score_mod_bwd` | ```python\nscore_mod_fn = partial(self.apply_score_mod, ...)\nscore_mod_bwd_fn = partial(self.apply_score_mod_bwd, ...)\n``` |
| `flash_attn/cute/pipeline.py` | 大幅重构，使用 `cute.arch.cluster_arrive_relaxed` | 删除原 `pipeline_init_wait` 实现，改为直接调用 `cute.arch.cluster_arrive_relaxed`（代码省略） |
| `flash_attn/cute/flash_fwd_sm90.py` | 使用 `setmaxregister_*` 替代 `warpgroup_reg_*` | 同上，统一寄存器增减接口 |
| `flash_attn/cute/flash_bwd.py` | 采用 `layout_utils.transpose_view` | ```python\nsQt, sdOt, sKt, sPt, sdSt = [layout_utils.transpose_view(t) for t in (sQ, sdO, sK, sP, sdS)]\n``` |
| `flash_attn/cute/flash_bwd_postprocess.py` | 使用 `layout_utils.transpose_view` | ```python\nsdQt = layout_utils.transpose_view(sdQ)\n``` |
| `flash_attn/cute/flash_bwd_sm100.py` | 统一布局转置方式 | ```python\nmQ, mdO = [layout_utils.select(t, mode=QO_layout_transpose) for t in (mQ, mdO)]\n``` |
| `flash_attn/cute/flash_fwd_sm100.py` | 删除 `warpgroup_reg_*`，改用 `setmaxregister_*` | 同上 |
| `flash_attn/cute/softmax.py` | 使用 `cute.math` 实现 | 替换 `exp2`, `log2`, `log` 调用为 `cute.math.exp2`, `cute.math.log2`, `cute.math.log` |
| `tests/cute/conftest.py` | 新增 GPU 轮询 fixture | ```python\n# pytest-dist round‑robin to GPUs\n``` |
| `flash_attn/cute/hopper_helpers.py` | 删除（已迁移至 `quack.sm90_utils`） | 文件整体删除，约 101 行 |

> 以上仅列出具有代表性的改动，完整 diff 请参考对应 PR。  

---  

*本期更新聚焦于 SM90/SM100 代码路径的寄存器管理、布局抽象以及 DSL 迁移至 `quack` 与最新 `cutedsl`，提升了代码可维护性并为 Hopper（SM90）提供了更稳健的回归测试。*
### sgl-project/sglang
提交
- **[bf89cc3](https://github.com/sgl-project/sglang/commit/bf89cc3803873a00879c516e247df611ad3247a7)** – *[ModelOPT] Support Qwen 3 Next Coder NVFP4* (Yi Zhong, 2026‑02‑08)  
  - 变更文件：`python/sglang/srt/models/qwen3_next.py`（+35 / ‑6，diff 已截断，未展示具体代码）  

- **[75997eb](https://github.com/sgl-project/sglang/commit/75997ebe8ddf228670824637e67d36bc884ef4ac)** – *Update author information in pyproject.toml* (Lianmin Zheng, 2026‑02‑08)  
  - 变更文件：`sgl-kernel/pyproject.toml`（+1 / ‑1）  

- **[071bf2c](https://github.com/sgl-project/sglang/commit/071bf2ce094c6f45e6039bf5f858f271f8be9f0b)** – *[Kimi‑K2.5] Fix missing `quant_config` in `KimiK25`* (Mohammad Miadh Angkad, 2026‑02‑08)  
  - 变更文件：`python/sglang/srt/models/kimi_k25.py`（+1）  

- **[656a3d7](https://github.com/sgl-project/sglang/commit/656a3d742edb501ac849ead1df10f1c1fb71363a)** – *Add tensor parallelism support to LFM2 ShortConv layers* (Piotr Mazurek, 2026‑02‑08)  
  - 变更文件：`python/sglang/srt/configs/lfm2.py`（±0）  
  - 变更文件：`python/sglang/srt/models/lfm2.py`（+45 / ‑57，diff 已截断）  

- **[6601bc2](https://github.com/sgl-project/sglang/commit/6601bc24da8a3a3b345a429d9315222c2d181dc4)** – *[diffusion] chore: revise process title* (Mick, 2026‑02‑08)  
  - 变更文件：`python/sglang/multimodal_gen/runtime/managers/gpu_worker.py`（+29 / ‑2）  

- **[031a652](https://github.com/sgl-project/sglang/commit/031a652b936ce0a3e468b2de7411293b05d1c189)** – *Fix TRT‑LLM MLA backend applying k_scale to BF16 KV cache in BMM1* (debo3, 2026‑02‑08)  
  - 变更文件：`python/sglang/srt/layers/attention/trtllm_mla_backend.py`（+36 / ‑12，diff 已截断）  

- **[a41aff1](https://github.com/sgl-project/sglang/commit/a41aff12437d2f6bf624118faa17fcc7053f837f)** – *[diffusion] refactor: group component loaders under the component_loaders/ directory* (Mick, 2026‑02‑08)  
  - 变更文件：多文件重命名至 `component_loaders/`（每个文件均有少量行数变动，diff 已截断）  
  - 其他小幅修改：`composed_pipeline_base.py`、`decoding.py`、`denoising.py`（各 ±1 行）  

- **[ca36d88](https://github.com/sgl-project/sglang/commit/ca36d88fa640bcdfc85fec2a267d6a2efa2052a7)** – *[ModelOpt] Fix broken Qwen3‑235B‑A22B‑Instruct‑2507‑NVFP4 launch* (Yi Zhong, 2026‑02‑08)  
  - 变更文件：`python/sglang/srt/models/qwen3_moe.py`（+8）  

- **[43eecd8](https://github.com/sgl-project/sglang/commit/43eecd8265deefe82e84b4225800f3c021e2ad87)** – *[diffusion] feat: support efficient sequence shard* (wxy, 2026‑02‑08)  
  - 变更文件：`pipeline_configs/base.py`（+2）  
  - 变更文件：`sampling_params.py`（+13）  
  - 变更文件：`runtime/models/dits/wanvideo.py`（+88 / ‑16，diff 已截断）  

- **[f600965](https://github.com/sgl-project/sglang/commit/f600965b0ee2251558718c9e559fe6585b7b0e42)** – *[CI] fix: notebook ci may not working* (shuwenn, 2026‑02‑08)  
  - 变更文件：`.github/workflows/execute-notebook.yml`（+13 / ‑2）  

- **[d71ccd8](https://github.com/sgl-project/sglang/commit/d71ccd886096544e216f125eb92177e6b9974bd4)** – *fix: sync server_args.kv_cache_dtype when detecting FP8 KV cache* (Zack Yu, 2026‑02‑08)  
  - 变更文件：`model_runner.py`（+13）  

- **[8e2e835](https://github.com/sgl-project/sglang/commit/8e2e835c2f42da6afc9939a633f4acc07bcc0288)** – *[Fix] Fix backend selection after flashinfer version update* (DarkSharpness, 2026‑02‑08)  
  - 变更文件：`flashinfer_backend.py`（+9 / ‑3）  

- **[00248d8](https://github.com/sgl-project/sglang/commit/00248d85c798c6c4cb84aa5a7ac7685178867219)** – *[diffusion] platform: support WAN/FLUX/Qwen‑Image/Qwen‑Image‑edit on Ascend* (Makcum888e, 2026‑02‑08)  
  - 变更文件众多（共 506 行改动，+476 / ‑30），涉及 NPU CI、分布式调度、层实现等；多数 diff 已截断，未展示具体代码。  

- **[7b83659](https://github.com/sgl-project/sglang/commit/7b836593108559f8db954375f0e64bb38637363e)** – *fix: fix NVFP4 Kimi‑K2.5 weight mapping and exclude list* (Mohammad Miadh Angkad, 2026‑02‑08)  
  - 变更文件：`modelopt_quant.py`（+17）  
  - 变更文件：`kimi_k25.py`（+13 / ‑1）  

Issues
- **[SGLang diffusion GGUF model support](https://github.com/sgl-project/sglang/issues/18445)** – 提议支持 GGUF 格式模型，体积更小且性能相近。Issue 内容完整。  
- **[Crashing on decode side at high load when deepseek MTP is on, in DeepSeek‑V3.2 disaggregated serving](https://github.com/sgl-project/sglang/issues/18443)** – 报告在 DeepSeek‑V3.2 使用 MLP_TP8 + MLA_DP8 的分布式部署下，高负载时解码崩溃。Issue 正文被截断，细节缺失。  
- **[kimik25 not support pp](https://github.com/sgl-project/sglang/issues/18433)** – 反馈 Kimi‑K2.5 在开启 pipeline parallel（pp）时不工作，附带截图。Issue 正文被截断，未提供完整复现步骤。
### vllm-project/vllm
**提交**

| 短 SHA | PR / Commit 链接 | 简要说明 | 关键文件变更 |
|--------|------------------|----------|--------------|
| `f97ca67` | [#30525](https://github.com/vllm-project/vllm/pull/30525) | 更新至 Torch 2.10（最终发布）。 | - `.buildkite/image_build/image_build.yaml`（改动 2 行，删除 1 行）<br>- `CMakeLists.txt`（前后各 5 行）<br>- `cmake/external_projects/triton_kernels.cmake`（前后各 4 行）<br>（diff 已被截断，具体代码片段不可得） |
| `084aa19` | [#33786](https://github.com/vllm-project/vllm/pull/33786) | 新增对 ModelOpt MXFP8 密集模型的支持。 | - `vllm/model_executor/layers/quantization/modelopt.py`（新增 247 行代码）<br>- `vllm/model_executor/layers/quantization/utils/mxfp8_utils.py`（新增 121 行）<br>- 其他相关配置文件少量改动。 |
| `1ecfabe` | [#32958](https://github.com/vllm-project/vllm/pull/32958) | 为 NVIDIA B200 GPU 添加 GLM 4.6 FP8‑W8A8 融合推理配置。 | - 新增 `vllm/model_executor/layers/fused_moe/configs/E=160,N=384,device_name=NVIDIA_B200,dtype=fp8_w8a8.json`（完整 JSON 配置，163 行）。 |
| `4df841f` | [#33735](https://github.com/vllm-project/vllm/pull/33735) | 为 `torch.compile` 增加强制开启 MOE 冷启动优化的选项。 | - `vllm/config/compilation.py`（新增 9 行）<br>- `vllm/config/vllm.py`（新增 8 行）<br>- `vllm/forward_context.py`（删除 9 行，新增 1 行）。 |
| `a263aa6` | [#34088](https://github.com/vllm-project/vllm/pull/34088) | 修复 Marlin MOE 在无 act 与 mul 场景下的兼容性。 | - `vllm/model_executor/layers/fused_moe/fused_marlin_moe.py`（改动 1 行）。 |
| `179ae7d` | [#33771](https://github.com/vllm-project/vllm/pull/33771) | 回滚 GLM‑4.7‑GPTQ 解码性能回退的修复。 | - `vllm/v1/attention/backends/flashinfer.py`（删除 3 行，新增 1 行）。 |
| `c4df59a` | [#32493](https://github.com/vllm-project/vllm/pull/32493) | 为禁用的多模态输入添加嵌入输入支持。 | - 新增 `tests/entrypoints/llm/test_mm_embeds_only.py`（67 行测试）<br>- `vllm/multimodal/budget.py`（改动 39 行）<br>- 其他多模态处理模块少量改动。 |
| `785cf28` | [#34059](https://github.com/vllm-project/vllm/pull/34059) | ROCm CI 中降低两个测试组的资源占用。 | - `.buildkite/test-amd.yaml`（前后各 2 行改动）。 |
| `a96197f` | [#33855](https://github.com/vllm-project/vllm/pull/33855) | 简化 Deepseek‑V32 分词器，实现更快的去分词。 | - `vllm/tokenizers/deepseek_v32.py`（删除 179 行，新增 77 行）<br>- 相关渲染与测试文件少量改动。 |
| `ab10d79` | [#34069](https://github.com/vllm-project/vllm/pull/34069) | 修复 ROCm 上 `act_quant_fusion` 模块的导入错误。 | - `vllm/compilation/passes/fusion/rocm_aiter_fusion.py`（改动 1 行）。 |

> **说明**：部分提交的 `patch` 被标记为 `patch_truncated: true`，因此具体代码片段未展示，仅列出文件路径和改动行数。

**Issues**

| 标题 | 链接 | 作者 | 简要描述 |
|------|------|------|----------|
| `[Usage]: VLLM with finetuned unsloth ministral 3` | https://github.com/vllm-project/vllm/issues/34099 | LeDuySon | 在 RTX A6000 上使用 `vllm serve` 部署 finetuned Ministral‑3 时缺失 `params.json` 等文件，导致启动失败。 |
| `[Bug]: GLM-4.7-Flash requires transformers from git (glm4_moe_lite but Transformers does not recognize this architecture)` | https://github.com/vllm-project/vllm/issues/34098 | brokedba | GLM‑4.7‑Flash 模型使用 `glm4_moe_lite` 架构，仅在 Transformers 主分支可用，导致加载时报 `ValidationError`。 |
| `[Feature]: Support vLLM with Python 3.14` | https://github.com/vllm-project/vllm/issues/34096 | mgoin | 讨论在 PyTorch 2.10 已提供 Python 3.14 支持的情况下，vLLM 是否应跟进并解决依赖库缺少 3.14 wheel 的问题。 |
| `[Usage]: Option to set system_fingerprint?` | https://github.com/vllm-project/vllm/issues/34095 | vrdn-23 | 询问是否可以在 vLLM 的 Chat Completion API 中自定义返回的 `system_fingerprint` 字段，以便区分内部部署的服务器。 |
| `[Bug]: assert num_cache_lines >= batch` | https://github.com/vllm-project/vllm/issues/34094 | vitalik | 在启动日志中出现断言错误，提示缓存行数不足以容纳当前 batch，可能与 KV 缓存分配逻辑有关。 |
| `what am I doing wrong ? libmpi_cxx.so.40: cannot open shared object file: No such file or directory` | https://github.com/vllm-project/vllm/issues/34090 | peter247 | 使用 `uv` 环境安装 vLLM 后运行报缺少 `libmpi_cxx.so.40`，导致启动失败。 |
| `[Bug]: KV Cache Memory Bottleneck Calculation in Pipeline Parallel (_check_enough_kv_cache_memory in get_kv_cache_configs)` | https://github.com/vllm-project/vllm/issues/34076 | chizhiwei | 在两卡（RTX 5070 Ti + RTX 5090 D）使用 pipeline parallel 时，KV 缓存内存检查出现错误，影响长上下文部署。 |
| `[Bug]: #33758 is causing aiohttp/_websocket.c build error` | https://github.com/vllm-project/vllm/issues/34073 | amd-hhashemi | 在 ROCm CI 环境下，因依赖 `aiohttp` 编译 `_websocket.c` 失败，导致构建中断。 |

> **说明**：所有 Issue 的正文均被标记为 `body_truncated: true`，因此仅提供了标题和已知的核心问题描述。
### NVIDIA/cutile-python
昨日无更新。

## 总结
### 展望与关注点
- **兼容性与安全性**：FlashMLA 与 flashinfer 正在填补 varlen 与 SM110 支持的空白，需关注后续实现的错误报告完整性和 workspace 资源管理是否能在生产环境中保持稳定。  
- **架构适配**：flash‑attention 对 Hopper/SM90/SM100 的深度重构为后续更高算力 GPU（如 H100、Ada）奠定了基础，但 DSL 与寄存器抽象的改动可能引入新回归，需要持续回归测试。  
- **跨平台扩展**：sglang 对 Ascend NPU 与 NVFP4 的大规模适配展示了多硬件生态的趋势，建议关注模型量化与张量并行在不同芯片上的性能一致性。  
- **生态协同**：vllm 的 Torch 2.10、ModelOpt MXFP8 与 ROCm 支持提升了整体生态兼容度，但多版本依赖（torch、triton、flashinfer）仍是潜在冲突点，建议在 CI 中加入跨版本兼容性矩阵测试。  
- **总体趋势**：各项目均在强化对新硬件（SM110、B200、Ascend）和新量化格式（MXFP8、FP8‑W8A8）的支持，同时通过 CI 与工作流改进提升发布质量，后续关注这些新特性的实际性能表现以及在大规模部署中的资源调度策略。
