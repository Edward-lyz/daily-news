今日的 AI Infra 的新闻如下。

## 摘要
## 本周关键动态
- **NVIDIA/cutlass**：两条活跃 Issue 报告了 C++20 编译缺失导致的示例构建失败，以及 NCxHWx 卷积权重布局设计疑问，提示在 Hopper GPU 上的兼容性仍需完善。  
- **flashinfer-ai/flashinfer**：提交了缓存 `cudaGetDeviceProperties` 的性能优化、CI 容器用户路径改进以及 FP8 Blockscale GEMM 缺失导致的编译错误，表明对硬件特性和部署便利性的持续关注。  
- **Dao-AILab/flash-attention**：deterministic 模式仍出现梯度不确定性，需进一步排查实现细节以满足严格 reproducibility 要求。  
- **sgl-project/sglang**：累计 13 条 PR，涵盖 CPU 后端崩溃、DeepSpeed‑style KV 事件元数据、Qwen3.5 模型支持、Diffusion 资源清理与 ModelScope 接入等，显示项目在多模态、量化与调度层面的快速迭代。  
- **vllm-project/vllm**：近期合并了 25 条 PR，涉及结构化输出校验、ROCm 兼容、FlashInfer 前缀优化、模型量化、NUMA 绑定、以及对 Qwen3.5、GLM、Gemma3 双向注意力等新模型的原生支持，整体提升了跨平台性能与功能覆盖。

## 具体内容分析
### deepseek-ai/DeepGEMM
昨日无更新。
### deepseek-ai/FlashMLA
昨日无更新。
### NVIDIA/cutlass
## 提交

无提交记录。

## Issues

- **[BUG] Example 63 requires C++20 support**  
  编译时因缺少 C++20 支持导致 NVHPC 25.9 报错。示例 `63_hopper_gemm_with_weight_prefetch` 中使用了需要 C++20 特性的代码，但未明确指定编译器标准。  
  链接: [Issue #3011](https://github.com/NVIDIA/cutlass/issues/3011)

- **[QST] NCxHWx interleaved conv weight layout uses CxRSKx instead of KCxRSx**  
  用户提问为何在 NCxHWx interleaved convolution 中使用 `CxRSKx` 布局而非更直观的 `KCxRSx`。希望了解此设计对性能的影响。  
  链接: [Issue #3010](https://github.com/NVIDIA/cutlass/issues/3010)
### flashinfer-ai/flashinfer
提交  
- **30bf78e** – [perf: cache cudaGetDeviceProperties in gdn_prefill to avoid per-call overhead (#2509)](https://github.com/flashinfer-ai/flashinfer/commit/30bf78e1a05d18aaebd484aa6f3c1db8e4c0b1b4)  
  - **文件**: `csrc/gdn_prefill_launcher.cu`  
    ```cpp
    // 旧实现
    cudaDeviceProp device_properties;
    cudaGetDeviceProperties(&device_properties, dev_id);
    // 新实现
    int device_major;
    cudaDeviceGetAttribute(&device_major, cudaDevAttrComputeCapabilityMajor, dev_id);
    …
    if (device_major == 9) { … }
    ```
- **a2d3b39** – [Enable setting user in CI containers (#2515)](https://github.com/flashinfer-ai/flashinfer/commit/a2d3b39c3e4de00453705223ab60d5fa4bc2e53f)  
  - **文件**: `docker/Dockerfile.cu126`、`docker/Dockerfile.cu128`、`docker/Dockerfile.cu129`、`docker/Dockerfile.cu130`（四个镜像共用同样的改动）  
    ```dockerfile
    # 新增：为任意用户启用 pip --user 安装
    RUN mkdir -p /opt/pip-user && chmod 1777 /opt/pip-user
    ENV PYTHONUSERBASE=/opt/pip-user
    ENV PATH="/opt/pip-user/bin:$PATH"
    ```

Issues  
- **#2527** – *[bug][cubin] Binaries for fp8_blockscale_gemm_sm90 not present in flashinfer-cubin and flashinfer-jit-cache*  
  - 影响：在 vllm 的 cu130 容器中，尽管已安装最新的 `flashinfer-cubin` 与 `flashinfer-jit-cache`（0.6.3/0.6.2），仍会尝试 JIT 编译缺失的 `fp8_blockscale_gemm_sm90` kernel，导致 `cublasLt.h not found` 错误。  
  - 链接: <https://github.com/flashinfer-ai/flashinfer/issues/2527>  

- **#2526** – *silu_and_mul: qwen2.5-vl VIT intermediate_size=3420, but vec_size in flashinfer is always 16*  
  - 影响：在测试 `silu_and_mul` 时，若将模型维度改为 3420（参见 `tests/utils/test_activation.py`），flashinfer 仍强制使用 `vec_size = 16`，导致不匹配。已有社区补丁示例。  
  - 链接: <https://github.com/flashinfer-ai/flashinfer/issues/2526>
### Dao-AILab/flash-attention
## 提交
暂无代码提交。

## Issues
- **FA3 deterministic=True gives non-deterministic gradients in backward (flash_attn_func / flash_attn_qkvpacked_func)**  
  作者: Kingsleyandher 创建时间: 2026‑02‑09 [查看详情](https://github.com/Dao-AILab/flash-attention/issues/2244)  
  简要说明: 在开启 `deterministic=True` 时，前向计算可复现，但反向梯度仍出现位级差异，导致非确定性。测试显示在 30 次运行中，前向全部相同，反向有 29 次不相等，最大绝对差 0.00012207。需要进一步排查实现细节以确保完整的确定性。
### sgl-project/sglang
**提交**  
- **[Bug] CPU Backend fails to runtime‑error** – <https://github.com/sgl-project/sglang/issues/18507>  
  作者：CoffeeVampir3 创建时间：2026‑02‑09 23:22:42Z  
  *Issue 内容缺失（body_truncated 为 true），未提供完整描述。*  

- **[Bug] [sgl‑kernel] CmakeLists.txt Deepgemm commit tag** – <https://github.com/sgl-project/sglang/issues/18505>  
  作者：shivamag125 创建时间：2026‑02‑09 21:59:58Z  
  *Issue 内容缺失（body_truncated 为 true），未提供完整描述。*  

- **[Bug] FlashInfer 0.6.3 trtllm_fp4_block_scale_moe AssertionError during autotune warmup with PP4** – <https://github.com/sgl-project/sglang/issues/18499>  
  作者：YAMY1234 创建时间：2026‑02‑09 18:16:59Z  
  *Issue 内容缺失（body_truncated 为 true），未提供完整描述。*  

- **[Bug] [NPU] use qwen3‑8b‑vl to test accuracy, there is a discrepancy compared to the GPU** – <https://github.com/sgl-project/sglang/issues/18481>  
  作者：shiyuan680 创建时间：2026‑02‑09 09:11:43Z  
  *Issue 内容缺失（body_truncated 为 true），未提供完整描述。*  

- **[Bug] SGLang tools output format miss Function label** – <https://github.com/sgl-project/sglang/issues/18478>  
  作者：attack204 创建时间：2026‑02‑09 08:43:51Z  
  *Issue 内容缺失（body_truncated 为 true），未提供完整描述。*  

- **[diffuser] support for a custom text encoder** – <https://github.com/sgl-project/sglang/issues/18470>  
  作者：JohnHerry 创建时间：2026‑02‑09 06:21:06Z  
  *Issue 内容完整，描述了对自定义文本编码器的需求。*  

---

**总计**  

1. **Tiny fix wrong metric collect key name `forward_prefill` → `forward_extend`** – <https://github.com/sgl-project/sglang/commit/2825f5d8e0aec3191a16ad886ddd0423dab752fe>  
   作者：Liangsheng Yin 日期：2026‑02‑10 00:23:41Z  
   - 关键文件：`test/registered/metrics/test_metrics.py`（修改 7 行）  
   - 代码片段：  
     ```diff
     - {"category": "forward_prefill"},
     + {"category": "forward_extend"},
     ```  
   - 说明：仅更正指标键名，未出现 diff 截断。  

2. **Add cache_config_info metric** – <https://github.com/sgl-project/sglang/commit/26a006e47f0bb764d1bda5e1af6631b12c0e7f4a>  
   作者：Kartik Ramesh 日期：2026‑02‑10 00:09:09Z  
   - 关键文件：`python/sglang/srt/managers/scheduler.py`（新增 5 行），`python/sglang/srt/metrics/collector.py`（新增 13 行）  
   - 代码片段（`collector.py`）  
     ```diff
     + self.cache_config_info = Gauge(
     +     name="sglang:cache_config_info",
     +     documentation="Cache configuration information.",
     +     labelnames=["page_size", "num_pages"],
     +     multiprocess_mode="mostrecent",
     + )
     ```  
   - 说明：实现了一个工作式 Info metric（Prometheus 不直接支持 Info）。  

3. **docs: expand and update modelopt documentation** – <https://github.com/sgl-project/sglang/commit/54589a2f2d0801cfb5e3d6136d0601277ffd2a9a>  
   作者：Zack Yu 日期：2026‑02‑09 23:09:52Z  
   - 关键文件：`docs/advanced_features/quantization.md`（新增 95 行，删除 19 行）  
   - 代码片段（节选）  
     ```diff
     + **Offline vs. Online Quantization:**
     + * **Offline Quantization (pre‑quantized):**
     +   * **Usage:** Download a pre‑quantized model …
     + * **Online Quantization (quant and serve):**
     +   * **Usage:** Load a standard BF16/FP16 model …
     ```  
   - 说明：文档大幅扩展，提供离线/在线量化对比。  

4. **[Auto Sync] Update cache_init_params.py (20260209)** – <https://github.com/sgl-project/sglang/commit/b027c5aca656acf024e3b31f29c3984bd571b554>  
   作者：Lianmin Zheng 日期：2026‑02‑09 22:49:41Z  
   - 关键文件：`python/sglang/srt/disaggregation/kv_events.py`（行首空格修正），`python/sglang/srt/mem_cache/cache_init_params.py`（新增 `cache_ttl_seconds` 字段）  
   - 代码片段（`cache_init_params.py`）  
     ```diff
     + # Time-to-live for cache entries in seconds. If None, TTL is disabled.
     + cache_ttl_seconds: Optional[float] = None
     ```  
   - 说明：为 KV 事件添加统一空格，新增缓存 TTL 配置。  

5. **[Auto Sync] Update logits_processor.py (20260209)** – <https://github.com/sgl-project/sglang/commit/ce95f203b054dae900f60d976fbe2298899b5bd5>  
   作者：Lianmin Zheng 日期：2026‑02‑09 22:33:02Z  
   - 关键文件：`python/sglang/srt/layers/logits_processor.py`（新增 `self.vocab_size`，修改分布式 logits 计算）  
   - 代码片段（关键修改）  
     ```diff
     + self.vocab_size = config.vocab_size
     - if self.config.vocab_size % self.attn_tp_size == 0:
     + if self.vocab_size % self.attn_tp_size == 0:
     ```  
   - 说明：解决在分布式环境下 vocab_size 访问不一致的问题。  

6. **feat(kv‑events): Add medium field to KV event types for storage tier tracking** – <https://github.com/sgl-project/sglang/commit/01e3f4682e2071ae110a593a219685a3ef7e201f>  
   作者：ishandhanani 日期：2026‑02‑09 20:39:15Z  
   - 关键文件：`python/sglang/srt/disaggregation/kv_events.py`（新增 `MEDIUM_GPU`、`MEDIUM_CPU` 常量及 `medium` 字段），`python/sglang/srt/mem_cache/radix_cache.py`（在存储/删除事件中填充 `medium=MEDIUM_GPU`）  
   - 代码片段（`kv_events.py`）  
     ```diff
     + MEDIUM_GPU = "GPU"
     + MEDIUM_CPU = "CPU_PINNED"
     
     class BlockStored(KVCacheEvent):
         ...
         medium: Optional[str] = None
     
     class BlockRemoved(KVCacheEvent):
         ...
         medium: Optional[str] = None
     ```  
   - 说明：为 KV 事件加入存储介质标签，便于监控不同层级缓存。  

7. **[AMD] add amd ci monitor** – <https://github.com/sgl-project/sglang/commit/316f9cbb35840c944bcd1cc0b0722d1bb2b65c78>  
   作者：Bingxu Chen 日期：2026‑02‑09 17:04:54Z  
   - 关键文件：`.github/workflows/amd-ci-job-monitor.yml`（新增 149 行 CI 工作流），`scripts/ci/query_job_status.py`（新增 925 行脚本）  
   - 代码片段（工作流入口）  
     ```yaml
     name: AMD CI Job Monitor
     on:
       schedule:
         - cron: '0 0 * * *'  # Daily at midnight UTC
       workflow_dispatch:
         inputs:
           hours:
             description: 'Time window in hours'
             default: '24'
     ```  
   - 说明：提供每日 AMD CI 运行状态查询工具。  

8. **model: support Qwen3.5** – <https://github.com/sgl-project/sglang/commit/27c447653d9cf0f63aea1c190b931be4875cbf86>  
   作者：Zheng Li 日期：2026‑02‑09 16:27:59Z  
   - 关键文件：`benchmark/kernels/fused_moe_triton/common_utils.py`（新增模型名称），`python/sglang/srt/configs/__init__.py`（导入 Qwen3.5 配置），`python/sglang/srt/configs/qwen3_5.py`（新增完整配置文件），`python/sglang/srt/models/qwen3_5.py`（新增模型实现，约 1300 行）  
   - 代码片段（`common_utils.py`）  
     ```diff
     + "Qwen3_5MoeForConditionalGeneration",
     ```  
   - 说明：首次在 SGLang 中加入 Qwen3.5 系列模型的完整支持。  

9. **Fix MMLU benchmark to auto‑download data and resolve path issue** – <https://github.com/sgl-project/sglang/commit/0b4d4f28388ab324766812ff8c69ca7ed2cfd4da>  
   作者：Xinyuan Tong 日期：2026‑02‑09 15:40:40Z  
   - 关键文件：`benchmark/mmlu/bench_sglang.py`（新增 30 行下载逻辑）  
   - 代码片段（核心）  
     ```python
     if not os.path.exists(data_path):
         download_mmlu(data_path)
     ```  
   - 说明：解决 MMLU 基准数据缺失导致的运行错误。  

10. **Pass `quantize_config` to `_initialize_model`** – <https://github.com/sgl-project/sglang/commit/006da2226864f64726cfe501fdde49c5a11e9c41>  
    作者：Kurt Shuster 日期：2026‑02‑09 15:34:42Z  
    - 关键文件：`python/sglang/srt/model_loader/loader.py`（新增 51 行，删除 39 行）  
    - 代码片段（新增参数传递）  
      ```diff
      + def _initialize_model(self, quantize_config=None):
      +     if quantize_config:
      +         model = apply_quantization(model, quantize_config)
      ```  
    - 说明：让模型加载阶段能够直接接受量化配置。  

11. **feature: support bidirectional attention for Gemma‑3** – <https://github.com/sgl-project/sglang/commit/ddbcfbaaabce5a014da22bb4ae7c320128dc10b9>  
    作者：brimon 日期：2026‑02‑09 15:17:45Z  
    - 关键文件：`python/sglang/srt/models/gemma3_causal.py`（大量删除/添加），`python/sglang/srt/models/gemma3_mm.py`（新增 65 行），以及文档 `docs/supported_models/...md`（新增 20 行）  
    - 代码片段（注意力实现）  
      ```diff
      - # 原单向注意力实现
      + # 新增双向注意力逻辑
      ```  
    - 说明：为 Gemma‑3 引入双向注意力，提升多模态推理能力。  

12. **[diffusion] chore: fix unclean shutdown and resource leaks** – <https://github.com/sgl-project/sglang/commit/4f7da5ad0f5e8215973ca74d9d8b86e73f6d761f>  
    作者：Mick 日期：2026‑02‑09 14:32:08Z  
    - 关键文件：`python/sglang/multimodal_gen/runtime/entrypoints/diffusion_generator.py`、`.../openai/utils.py`、`.../launch_server.py`、`.../gpu_worker.py`、`.../scheduler.py`（累计 33 行修改）  
    - 代码片段（资源清理）  
      ```python
      try:
          await diffusion_task
      finally:
          await gpu_worker.shutdown()
      ```  
    - 说明：确保 Diffusion 任务结束后正确释放 GPU 资源。  

13. **[diffusion] feat: add ModelScope support** – <https://github.com/sgl-project/sglang/commit/76eb1c8406bc4a73cec05f516093ef80d8e56387>  
    作者：yrk111222 日期：2026‑02‑09 11:23:45Z  
    - 关键文件：`python/sglang/cli/utils.py`（新增 12 行），`python/sglang/multimodal_gen/apps/webui/main.py`（新增 12 行），`python/sglang/multim
### vllm-project/vllm
**提交**  
- `1339784` – *[structured output] validate unsupported json features first*  
  PR: https://github.com/vllm-project/vllm/commit/13397841ab469cecf1ed425c3f52a9ffc38139b5  
  影响文件：`vllm/v1/structured_output/backend_xgrammar.py`（仅 5 行增删，具体改动未展示，patch 被截断）。  

- `c60f8e3` – *[Bugfix][ROCm][GPT-OSS] Use old triton_kernels implementation on ROCm if the new API is not available*  
  PR: https://github.com/vllm-project/vllm/commit/c60f8e3b49eced1a17ba0e11da3f8c107b309df9  
  影响文件：`vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py`（新增 52 行，删除 9 行，主要加入对旧实现的回退逻辑）。  

- `5e75a14` – *[Doc] Add DCP support to attention backend doc*  
  PR: https://github.com/vllm-project/vllm/commit/5e75a14a667dccf7f48781568f19f1a6b9c8014a  
  影响文件：`docs/design/attention_backends.md`（文档增删约 25 行）以及 `tools/pre_commit/generate_attention_backend_docs.py`（大幅改动 1362 行，生成脚本更新）。  

- `e7e5278` – *[ModelRunner V2][BugFix] Fix `max_query_len` calculation*  
  PR: https://github.com/vllm-project/vllm/commit/e7e52781ff636bf772301c9282a7601c73b8b905  
  影响文件：`vllm/v1/worker/gpu/attn_utils.py`（1 行改动），`vllm/v1/worker/gpu/cudagraph_utils.py`（新增 1 行），`vllm/v1/worker/gpu/model_runner.py`（新增 3 行），`vllm/v1/worker/gpu/spec_decode/eagle.py`（新增 1 行）。  

- `bb9f973` – *[torch.compile][Fusion] Fix attention fusion pass removing kv_update op*  
  PR: https://github.com/vllm-project/vllm/commit/bb9f97308d0b88c5ad2d64c217b22866e40c79df  
  影响文件：`tests/compile/passes/test_fusion_attn.py`（12 行增，1 行删），`vllm/compilation/passes/fusion/attn_quant_fusion.py`（新增 10 行）。  

- `4d39650` – *[ROCm] update triton branch to support gpt-oss models for gfx11xx devices*  
  PR: https://github.com/vllm-project/vllm/commit/4d3965096164328451538988824d72ab03593c04  
  影响文件：`docker/Dockerfile.rocm_base`（仅 1 行改动，patch 被截断）。  

- `8fd31f6` – *[Bugfix] Voxtral prompt/audio placeholder alignment*  
  PR: https://github.com/vllm-project/vllm/commit/8fd31f62452960efdd6dd7b912c388f487536b3c  
  影响文件：`vllm/model_executor/models/voxtral.py`（新增 21 行，删除 3 行）。  

- `eadb4e8` – *[Bugfix] Avoid duplicate k-proj weight emission in helper*  
  PR: https://github.com/vllm-project/vllm/commit/eadb4e868bae8acd2e9b764f5827c0500ec44c34  
  影响文件：`vllm/model_executor/models/whisper.py`（2 行增删）。  

- `285bab4` – *[Kernel] use flashinfer for gdn prefill*  
  PR: https://github.com/vllm-project/vllm/commit/285bab47526cbc4d4e26c61d831eaeb17b253d0f  
  影响文件：`vllm/model_executor/models/qwen3_next.py`（新增 115 行，删除 2 行）。  

- `995bbf3` – *[Bugfix] Fix shared expert input for latent MoE in EP+DP (Nemotron-H)*  
  PR: https://github.com/vllm-project/vllm/commit/995bbf38f114a0e1bd7e34d6fd92d255ac2efca7  
  影响文件：多处 MoE 相关实现，累计增删 33 行（细节见各文件）。  

- `d4f123c` – *[Kernel] FlashInfer: switch allreduce fusion to unified API*  
  PR: https://github.com/vllm-project/vllm/commit/d4f123cc48c374f7aad48cd808d797c71711ebc7  
  影响文件：`benchmarks/kernels/benchmark_fused_collective.py`（增 54 行，删 69 行），`tests/compile/passes/distributed/test_fusion_all_reduce.py`（增 3 行，删 2 行），`vllm/compilation/passes/fusion/allreduce_rms_fusion.py`（增 23 行，删 43 行）。  

- `cb62e86` – *Add NUMA Core binding in nixl_connector for CPU xPyD*  
  PR: https://github.com/vllm-project/vllm/commit/cb62e86f83bf859fb25936a0c39709a31515fddc  
  影响文件：`vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`（新增 11 行），`vllm/platforms/cpu.py`（新增 61 行），`vllm/v1/worker/cpu_worker.py`（新增 1 行）。  

- `781ddf7` – *[CI][torch.compile] Fix incorrect filtering for E2E fusion tests on B200*  
  PR: https://github.com/vllm-project/vllm/commit/781ddf786861f40de6d94d45d7b149d0f8d58c11  
  影响文件：`.buildkite/test_areas/compile.yaml`（增 8 行，删 10 行）。  

- `64a9c25` – *[UX] Add `--language-model-only` for hybrid models*  
  PR: https://github.com/vllm-project/vllm/commit/64a9c2528b1487fbfefa333cb1b246a57cddd4b2  
  影响文件：`vllm/config/model.py`（新增 3 行），`vllm/config/multimodal.py`（增 11 行，删 3 行），`vllm/engine/arg_utils.py`（新增 5 行）。  

- `d0d97e2` – *[Misc] Fix up attention benchmarks*  
  PR: https://github.com/vllm-project/vllm/commit/d0d97e2974250edb61fbff6964e95a5b6d22d763  
  影响文件：多处基准脚本，累计增 219 行，删 95 行。  

- `9562912` – *[MODEL] Adding Support for Qwen3.5 Models*  
  PR: https://github.com/vllm-project/vllm/commit/9562912cead1f11e8540fb91306c5cbda66f0007  
  影响文件：新增 `vllm/model_executor/models/qwen3_5.py`（993 行）和 `vllm/model_executor/models/qwen3_5_mtp.py`（447 行），以及若干配置与注册文件的少量改动。  

- `9bdb06b` – *[XPU][6/N] add xpu scaled_mm kernel*  
  PR: https://github.com/vllm-project/vllm/commit/9bdb06b4368e304bc5e23c8df2dff8f8b2ccf0f6  
  影响文件：`vllm/model_executor/layers/quantization/kernels/scaled_mm/xpu.py`（新增 59 行），以及相关初始化文件的少量改动。  

- `caad9f1` – *[Fix] [CPU Backend] : Prepack weights for w8a8 oneDNN matmul*  
  PR: https://github.com/vllm-project/vllm/commit/caad9f1e01ee04e4f5912d0287031ea3a850f6dc  
  影响文件：`csrc/cpu/dnnl_helper.cpp`（新增 10 行，删 2 行）。  

- `1d5922f` – *[ASR] Fix audio benchmark and add RTFx metric*  
  PR: https://github.com/vllm-project/vllm/commit/1d5922fadeebc5ec133dc1c88eb1e85605a5510c  
  影响文件：`docs/benchmarking/cli.md`（增 17 行），`vllm/benchmarks/datasets.py`（增 56 行，删 18 行），以及若干脚本的少量改动。  

- `3025b3c` – *[CI] Remove empty image_size_factors for fuyu, glm4_1v, glm_ocr*  
  PR: https://github.com/vllm-project/vllm/commit/3025b3cebb1f019ccd6918cc54da1ca32f53a777  
  影响文件：`tests/models/multimodal/generation/test_common.py`（增 3 行，删 3 行）。  

- `978a37c` – *[Model] GLM adaptation*  
  PR: https://github.com/vllm-project/vllm/commit/978a37c82387ce4a40aaadddcdbaf4a06fc4d590  
  影响文件：`vllm/model_executor/models/deepseek_v2.py`（增 5 行，删 1 行）等，主要加入 GLM 兼容性改动。  

- `5a5c435` – *fix(cpu): fix mla_decode compilation on x86 without AVX512*  
  PR: https://github.com/vllm-project/vllm/commit/5a5c43511ac98299856d0fee6c619fdd8bcdd2ef  
  影响文件：`csrc/cpu/mla_decode.cpp`（删 10 行，增 1 行），用于在缺少 AVX512 的 CPU 上兼容编译。  

- `d9bede0` – *[BugFix] Fix `fastsafetensors` TP all procs using all GPUs*  
  PR: https://github.com/vllm-project/vllm/commit/d9bede0314ba19a3f8336dcaeeeaf9e2c5487053  
  影响文件：`vllm/model_executor/model_loader/weight_utils.py`（增 9 行，删 6 行），修复分布式加载时的文件排序问题。  

- `22b6494` – *[Frontend][last/5] Make pooling entrypoints request schema consensus*  
  PR: https://github.com/vllm-project/vllm/commit/22b64948f6f4381bce7ac8ec0487020f0129e1cb  
  影响文件：大量新增/修改，包括 `examples/pooling/classify/vision_classification_online.py`（新增 110 行）等，完善 pooling 接口并提供示例。  

- `7c233db` – *[Tiny] Rename encoder budget file to more specific name*  
  PR: https://github.com/vllm-project/vllm/commit/7c233dbb36262b5106f46eb21a6324d15e424005  
  影响文件：`vllm/multimodal/encoder_budget.py` 被重命名，相关引用在 `vllm/lora/model_manager.py`、调度器等处做了微调（各 1 行增删）。  

- `a75a5b5` – *[bug-fix] supported_tasks is breaking backward compatibility at init_app_state*  
  PR: https://github.com/vllm-project/vllm/commit/a75a5b54c7f76bc2e15d3025d61e63cc91c7b0d7  
  影响文件：`vllm/entrypoints/openai/api_server.py`（增 26 行），修正任务列表向后兼容性。  

**Issues**  
- **[Bug] LoRA adapters with mismatched module name prefixes silently produce base-model output**  
  URL: https://github.com/vllm-project/vllm/issues/34186  
  作者: Butanium  
  描述：LoRA 适配器在模块名前缀不匹配时未报错，导致模型仍使用基模型权重。  

- **[Bug] fastsafetensors distributed loading crashes due to unsorted file list**  
  URL: https://github.com/vllm-project/vllm/issues/34180  
  作者: jaim12005  
  描述：`fastsafetensors_weights_iterator` 未对文件列表排序，导致多节点 TP 分片不一致，引发 NCCL 广播错误并崩溃。  

- **[CI Failure] Transformers Nightly Models, tests/models/test_initialization.py**  
  URL: https://github.com/vllm-project/vllm/issues/34178  
  作者: bnellnm  
  描述：初始化测试在 nightly 运行中大量失败，具体日志见 CI 链接。  

- **[CI Failure] tests/integration/test_rl.py: RuntimeError: operator torchvision::nms does not exist**  
  URL: https://github.com/vllm-project/vllm/issues/34166  
  作者: bnellnm  
  描述：RL 集成测试因缺失 `torchvision::nms` 算子报错，可能与 PyTorch 2.10 升级有关。  

- **[CI Failure] mi325_4: LM Eval Large Models**  
  URL: https://github.com/vllm-project/vllm/issues/34164  
  作者: AndreasKaratzas  
  描述：大模型评估在 CI 中因磁盘空间不足导致 OSError。  

- **[CI Failure] mi325_1: ROCm GPT-OSS Eval**  
  URL: https://github.com/vllm-project/vllm/issues/34162  
  作者: AndreasKaratzas  
  描述：ROCm 环境下 GPT‑OSS 评估测试失败，涉及 AITER 相关环境变量配置。  

- **[CI Failure] mi325_1: Entrypoints Unit Tests**  
  URL: https://github.com/vllm-project/vllm/issues/34160  
  作者: AndreasKaratzas  
  描述：入口点单元测试在 ROCm CI 中失败，具体细节未提供。  

- **[Bug] AttributeError: 'Glm46VImageProcessorFast' object has no attribute 'get_number_of_image_patches'**  
  URL: https://github.com/vllm-project/vllm/issues/34156  
  作者: regmibijay  
  描述：GLM 视觉处理器缺少 `get_number_of_image_patches` 方法，导致运行时异常。
### NVIDIA/cutile-python
昨日无更新。

## 总结
整体来看，AI 基础设施生态正加速围绕新硬件（如 Hopper、AMD ROCm、XPU）和新模型（Qwen3.5、Gemma3）进行适配与优化。开发者应关注 **deterministic** 计算的回归、FP8/FP16 兼容性以及分布式加载的排序问题，以免在大规模部署时出现隐蔽错误。建议在 CI 中加入更细粒度的硬件特性检测，并及时同步最新的模型配置文档，确保社区用户能够平滑迁移到最新的性能提升上。
