今日的 AI Infra 的新闻如下。

## 摘要
## 本日关键变更概览

- **SGLang**：新增 `PrefillStats` 统计类，完善 prefill 日志与指标；修复 radix 缓存键逻辑，提升多轮对话缓存一致性；引入 Tilelang 稀疏解码 JIT kernel，支持 `topk` 并行块；大量 NPU 文档更新与 CI 条件优化，提升 Ascend 生态兼容性。 
- **vLLM**：实现 `MoERunner` 抽象层，统一 MoE 调度并迁移至 `DefaultMoERunner`；加入 pre‑commit 布尔上下文检查，强化代码质量；在 `torch.compile` 路径关闭递归 `pre_grad_passes`，提升编译稳定性；扩展 Speculative Decode 配置以支持 draft 模式；EPLB 异步调度改为多线程实现，显著提升并行效率。 
- **底层算子**：vLLM 新增 MLA FP8 预填充实验、TopK‑per‑Row kernel 与 DeepSeek‑V3.2 `topKperRow` 加速；FlashAttention 相关 issue 报告了对 CUDA 12 与 Ascend 实现的需求，提示即将到来的硬件适配。 
- **文档与 CI**：两项目均完善平台文档（Ascend NPU、Read‑the‑Docs）并细化 CI 触发条件，减少不必要的全量测试，提升 CI 效率。 
- **模型兼容性**：vLLM 更新 Mamba 状态 dtype 配置、支持 GLM‑4 MoE DSA、兼容 Transformers 5.x 与 DeepSeek‑V3.5，进一步扩展模型生态。

## 具体内容分析
### deepseek-ai/DeepGEMM
昨日无更新。
### deepseek-ai/FlashMLA
昨日无更新。
### NVIDIA/cutlass
昨日无更新。
### flashinfer-ai/flashinfer
昨日无更新。
### Dao-AILab/flash-attention
提交  
- 本次暂无代码提交记录。  

Issues  
- **#2248**  |  *flash_attn-2.8.3+cu12torch2.10cxx11abiTRUE-cp312-cp312-linux_x86_64.whl*  
  作者：wwfcnu，创建于 2026‑02‑10。  
  内容：torch 2.10 已得到支持。  
  未关联 PR，且缺少对应 commit SHA 与链接，暂无法提供。  

- **#2247**  |  *Add Ascend Implementation for FlashAttention*  
  作者：AnyFree813，创建于 2026‑02‑10。  
  内容：希望贡献 Ascend 实现，已提交 PR。  
  关联 PR：[#2246](https://github.com/Dao-AILab/flash-attention/pull/2246)。
### sgl-project/sglang
**本日提交概览（2026‑02‑10）**

| SHA | 标题 | 关键改动模块/文件 | 简要说明 |
|-----|------|-------------------|----------|
| 93fca0b | Fix wrong prefill log. (#18570) | `schedule_batch.py`, `scheduler.py`, `scheduler_metrics_mixin.py`, `scheduler_output_processor_mixin.py` | 新增 `PrefillStats` 数据类用于记录 prefill 统计；在 `ScheduleBatch` 中加入 `prefill_stats` 成员；在调度器生成新 batch 时填充该统计；日志输出改为使用 `prefill_stats`。 |
| 2bfab1b | Fix radix cache key to include generated tokens in multi‑turn (regression) (#16521) | `radix_cache.py` | 修正缓存键生成逻辑，使用 `token_ids` 而非 `req.fill_ids`，避免多轮对话时键缺失。 |
| 4262f52 | Tilelang sparse decode fwd for dsv32 mi355 (#18488) | `tilelang_kernel.py` | 新增 `sparse_mla_fwd_decode_partial` JIT kernel，实现稀疏 MLA 解码的部分前向计算，支持 `topk` 分块并行。 |
| 2d38b8a | Revert "[sgl‑kernel] upgrade deepgemm" (#18562) | `sgl‑kernel/CMakeLists.txt` | 回退 DeepGEMM 依赖的 Git tag；恢复 `deep_gemm_cpp` 的 `USE_SABI` 编译选项。 |
| 573ff55 | [NPU][docs]fix bug about hyperlink for best practice for ascend npu (#18561) | `docs/platforms/ascend_npu_best_practice.md` | 修正文档中超链接错误，确保最佳实践页面指向正确。 |
| 4460376 | fix(config): Support setting Mamba state dtype via config file (#18532) | `configs/falcon_h1.py`, `configs/jet_nemotron.py`, `configs/lfm2.py`, `configs/mamba_utils.py`, `configs/nemotron_h.py`, `configs/qwen3_next.py`, `environ.py`, `server_args.py` | 在多模型配置中加入 `mamba2_state_dtype` 参数，支持通过环境变量或配置文件自定义 Mamba SSM 状态数据类型；`SGLANG_MAMBA_SSM_DTYPE` 环境变量改为可为空。 |
| d0d387d | [NPU] update npu doc (#18474) | `docs/platforms/ascend_npu_support.rst` | 在 NPU 支持列表中新增 `ascend_npu_support_features.md`。 |
| 99101ce | [NPU][docs] improve docs for Best Practice on Ascend NPU (#18360) | `docs/platforms/ascend_npu_best_practice.md` | 大幅优化低延迟表格排版，统一列名并补全配置链接。 |
| bec7fe9 | [sgl‑kernel] upgrade deepgemm (#18362) | `sgl‑kernel/CMakeLists.txt`, `sgl‑kernel/build.sh`, `sgl‑kernel/csrc/elementwise/concat_mla.cu` | 更新 DeepGEMM 依赖 tag；恢复 `_C` 扩展模块名以兼容 Python 包；在构建脚本中恢复 `USE_CCACHE` 参数显示。 |
| efcdda0 | [diffusion] fix: fix fsdp (#18187) | `multimodal_gen/configs/models/dits/zimage.py`, `multimodal_gen/runtime/layers/layernorm.py`, `multimodal_gen/runtime/layers/lora/linear.py`, `multimodal_gen/runtime/layers/triton_ops.py`, `multimodal_gen/runtime/loader/component_loaders/text_encoder_loader.py`, `multimodal_gen/runtime/loader/fsdp_load.py`, `multimodal_gen/runtime/managers/gpu_worker.py`, `multimodal_gen/runtime/models/dits/zimage.py`, `multimodal_gen/test/scripts/gen_perf_baselines.py`, `multimodal_gen/test/server/perf_baselines.json`, `multimodal_gen/test/server/test_server_common.py`, `multimodal_gen/test/server/testcase_configs.py` | 为 Z‑Image 模型添加 `is_zimage_layer` 判定函数，修正 FSDP 分片逻辑；同步更新相关层实现。 |
| 49cbb46 | [NPU] [CI] Enable run multimodal NPU CI when changes only in multimodal_gen (#18523) | `.github/workflows/pr-test-npu.yml` | CI 条件优化，仅在 multimodal_gen 变动时触发 NPU 测试。 |
| 47978ee | [diffusion] feat: support parallel wan‑vae decode (#18179) | `multimodal_gen/configs/models/vaes/wanvae.py`, `multimodal_gen/runtime/models/vaes/parallel/wan_common_utils.py`, `multimodal_gen/runtime/models/vaes/parallel/wan_dist_utils.py`, `multimodal_gen/runtime/models/vaes/wanvae.py` | 引入并行 WAN‑VAE 解码实现，新增分布式工具与公共函数，提升大规模 VAE 推理吞吐。 |
| 26f2b37 | [DLLM] Basic dLLM scheduling strategy and implementation (#17484) | `dllm/mixin/req.py`, `dllm/mixin/scheduler.py`, `managers/schedule_batch.py`, `managers/schedule_policy.py`, `managers/scheduler.py`, `model_executor/forward_batch_info.py`, `server_args.py`, `test/registered/dllm/test_dllm_batching.py` (removed), `test/registered/dllm/test_llada2_mini.py` | 实现 dLLM（decoder‑only LLM）调度框架，新增请求/调度混合策略；对应测试更新。 |
| 8da14ae | [HiCache] fix: StorageMetricsCollector was initialized twice (#18354) | `mem_cache/hiradix_cache.py` | 防止 `StorageMetricsCollector` 重复实例化，修复潜在内存统计冲突。 |
| d94d0af | [AMD] Turn on aiter‑prebuild (#18425) | `docker/rocm.Dockerfile` | 在 ROCm 镜像中启用 `aiter` 预编译，加速 AMD 环境构建。 |
| 398b81f | Support GlmMoeDsaForCausalLM (#18521) | `configs/model_config.py`, `models/glm4_moe.py`, `server_args.py` | 新增 GLM‑4 MoE DSA 模型支持，扩展模型配置与服务器参数。 |
| e8a2c13 | Deepseekv32 compatibility with transformers v5 (#18297) | `configs/model_config.py`, `environ.py`, `layers/attention/nsa/nsa_indexer.py`, `layers/attention/nsa_backend.py`, `models/deepseek_v2.py` | 兼容 Transformers 5.x，更新模型配置与注意力实现细节。 |
| 0b15f19 | [EPD] Add notification mechanism to fix server hang and add timeout env var (#18229) | `disaggregation/encode_receiver.py`, `disaggregation/encode_server.py`, `environ.py`, `managers/scheduler_metrics_mixin.py` | 引入服务器通知机制防止卡死；新增 `SGLANG_EPD_TIMEOUT` 环境变量。 |
| 1d366f1 | Make bench_one_batch_server compatible for more backends (#18512) | `test/bench_one_batch_server_internal.py` | 扩展基准测试兼容性，支持更多后端实现。 |
| 4a1b50b | Fix idle batch predict dtype in spec v2 (#18379) | `speculative/eagle_info_v2.py` | 修正空闲 batch 预测时的数据类型错误。 |
| df733c5 | [HiCache][PP] add test case for compatibility (#16395) | `test/manual/hicache/test_pp_with_hicache.py` | 新增 HiCache 与 Pipeline Parallelism 兼容性测试。 |
| 2825f5d | Tiny fix wrong metric collect key name `forward_prefill` → `forward_extend` (#18506) | `test/registered/metrics/test_metrics.py` | 更正指标收集键名，统一为 `forward_extend`。 |
| 26a006e | Add cache_config_info metric. (#17273) | `managers/scheduler.py`, `metrics/collector.py` | 新增 `cache_config_info` 指标，记录缓存配置细节。 |

> **注意**：部分 PR 的 `patch_truncated` 为 `true`，未展示完整 diff。若需完整改动，请访问对应 commit 链接。

---

**本日热点 Issue（2026‑02‑10）**

| 编号 | 标题 | 简要描述 | 链接 |
|------|------|----------|------|
| 18568 | [Question] Why there are so many routers in SGLang Diffusion that seems can be shared | 讨论 `multimodal_gen/runtime/entrypoints/http_server.py` 中大量 router 是否可以合并，提升代码复用与维护性。 | https://github.com/sgl-project/sglang/issues/18568 |
| 18563 | [Feature] `wait_for_server` optimization | `wait_for_server` 在多 notebook 并发失败时会长时间阻塞，建议加入超时或并行重试机制。 | https://github.com/sgl-project/sglang/issues/18563 |
| 18551 | Support FP4 KV cache parsing with ModelOpt checkpoints | 需求在 ModelOpt 提供的 FP4 KV 缓存模型上实现解析，映射到 `fp4_e2m1`。 | https://github.com/sgl-project/sglang/issues/18551 |
| 18550 | Error instead of silent Triton fallback when `--kv-cache-dtype fp8_e5m2` with `--attention-backend fa3` | 当 KV 缓存 dtype 与 FA3 不兼容时应抛错，而非静默回退到 Triton。 | https://github.com/sgl-project/sglang/issues/18550 |
| 18534 | [diffusion] loading dit model part with RoPE failed | RoPE 组件的 `cos_sin_cache` 在加载时触发异常，需修复权重加载兼容性。 | https://github.com/sgl-project/sglang/issues/18534 |
| 18529 | [Feature] deterministic inference support for A100 | 询问是否已在 A100 上实现批次不变（deterministic）推理支持。 | https://github.com/sgl-project/sglang/issues/18529 |
| 18519 | [Bug] Cuda graph failure with DeepEP on Hopper | 在 Hopper GPU 上开启 CUDA graph 时 DeepEP 报错，需定位图构建或同步问题。 | https://github.com/sgl-project/sglang/issues/18519 |
| 18514 | [Bug] DeepGEMM shape mismatch error with FP8 MTP layer under PD disagg setting for nvfp4 models | PD 分解模式下，FP8 MTP 层与 DeepGEMM 形状不匹配导致崩溃。 | https://github.com/sgl-project/sglang/issues/18514 |

--- 

**总结**  
本日提交集中在以下几个方向：  
1. **调度与度量**：新增 `PrefillStats`，完善 prefill 日志与指标；修正 metric 键名。  
2. **Mamba 配置**：统一通过环境变量或配置文件控制 SSM 状态 dtype，提升模型兼容性。  
3. **文档与 CI**：完善 NPU 文档、修复链接、优化 CI 触发条件。  
4. **底层算子**：Tilelang 稀疏解码 kernel、DeepGEMM 构建细节回退与恢复。  
5. **新特性**：并行 WAN‑VAE 解码、dLLM 调度框架、GLM‑4 MoE 支持。  

以上改动对性能监控、模型兼容性以及开发者体验都有显著提升。若需进一步技术细节，请参考对应 PR 的完整 diff。
### vllm-project/vllm
**提交概览**  

- **d1481ba** – 引入 `MoERunner` 抽象层，将执行逻辑从 `FusedMoE` 移至 `DefaultMoERunner`（[#32344](https://github.com/vllm-project/vllm/commit/d1481ba78323bcba5937f5ff74f3a8d27ab54f88)）。  
  - 关键文件：`vllm/model_executor/layers/fused_moe/runner/default_moe_runner.py`（新增 743 行实现），`vllm/model_executor/layers/fused_moe/runner/moe_runner.py`（抽象基类），以及 `vllm/model_executor/layers/fused_moe/layer.py`（大幅改动，删除 672 行，新增 69 行）。  
  - 其他相关：文档 `docs/design/moe_kernel_features.md`、测试 `tests/kernels/moe/*` 均同步更新。  

- **dc6de33** – CI 工作流 `cleanup_pr_body.yml` 增加 pip 缓存以加速依赖恢复（[#32979](https://github.com/vllm-project/vllm/commit/dc6de33c3d5e9026cef7b27791dfe0f98e64bbde)）。  

- **c4b9e67** – 新增 pre‑commit 检查，捕获 `with` 语句中的布尔表达式错误（[#34271](https://github.com/vllm-project/vllm/commit/c4b9e6778f9d8054c1665b2d1c2cb0ee36e9e2f5)）。  
  - 关键文件：`.pre-commit-config.yaml`（新增 5 行），`tools/pre_commit/check_boolean_context_manager.py`（实现 70 行检查逻辑）。  

- **341eed3** – 在 `torch.compile` 路径中关闭递归的 `pre_grad_passes`，提升编译稳定性（[#34092](https://github.com/vllm-project/vllm/commit/341eed3d30b7579b730e9959213d83b5dbd4731c)）。  
  - 关键文件：`vllm/compilation/compiler_interface.py`（修改 14 行），`vllm/envs.py`（新增 10 行）。  

- **6f2f59f** – Speculative Decode 支持为 draft 模型提供独立的加载配置（[#34022](https://github.com/vllm-project/vllm/commit/6f2f59f2b333151aac19f8ca7bf71d83c1a7c068)）。  
  - 关键文件：`vllm/config/speculative.py`（新增 5 行），`vllm/model_executor/model_loader/__init__.py`（微调），`vllm/v1/spec_decode/eagle.py`（小幅改动）。  

- **bb2fc8b** – 修复 DeepEP LL all2all 后端在 EPLB 异步模式下的挂起问题（[#32860](https://github.com/vllm-project/vllm/commit/bb2fc8b5e7beca9c5749e464b4607c753db0b630)）。  
  - 新增 `vllm/distributed/eplb/eplb_utils.py`（54 行实现），`vllm/v1/worker/gpu_worker.py`（微调 2 行）。  

- **6713294** – 将 EPLB 重平衡算法迁移至异步线程，提高并发效率（[#30888](https://github.com/vllm-project/vllm/commit/67132945bbad23233fd583e6106ebebe859c8366)）。  
  - 关键改动：`vllm/distributed/eplb/async_worker.py`（+89 / -18 行），`vllm/distributed/eplb/eplb_state.py`（+88 / -54 行），`vllm/distributed/eplb/rebalance_execute.py`（+33 / -26 行），以及 `vllm/distributed/parallel_state.py`（+38 / -1 行）。  

- **f0ca067** – 在 `vllm/engine/arg_utils.py` 中加入对未识别环境变量的警告（[#33581](https://github.com/vllm-project/vllm/commit/f0ca0671c70fae6d1562127e3330eeaedf4abb3f)）。  

- **578977b** – 为 MLA（Mixture‑of‑Logits Attention）重新提交 FMHA FP8 预填充实验，提升性能（[#31195](https://github.com/vllm-project/vllm/commit/578977bb5ed208c62cf9cff80d955836775e0d24)）。  
  - 关键文件：`vllm/model_executor/layers/attention/mla_attention.py`（+138 / -20 行），`vllm/config/attention.py`（+3 行）。  

- **9615575** – 修正 Qwen3.5 模型在 Mamba 缓存中的 dtype 错误（[#34200](https://github.com/vllm-project/vllm/commit/9615575afc0d9a7d5fe98b65ac2a7150b068472e)）。  

- **4293c00** – 调整注意力基准测试的 smoke‑test，确保 CI 稳定（[#34269](https://github.com/vllm-project/vllm/commit/4293c00b84b968ed25f80dfd2af3bb34d1eeeef6)）。  

- **506ad7d** – 修复 sleep 模式下的权重 offloading 逻辑（[#32947](https://github.com/vllm-project/vllm/commit/506ad7d7c178ac20f2140cfaac1ae657683e8013)）。  

- **fdd6f2a** – 将在线 API 改写为使用 `Renderer`，提升渲染抽象层次（[#34084](https://github.com/vllm-project/vllm/commit/fdd6f2ad58b113fe0fdc3fd9998e63d6064b5f16)）。  

- **33bcd3d** – 引入 `ec_both` 角色（Encoder Cache）连接器，实现 encoder‑cache 双向共享（[#34182](https://github.com/vllm-project/vllm/commit/33bcd3dc3bf4d581c051400c8d9bb9433d2c87af)）。  

- **1f5febb** – 修正 `serve` CLI 中 `api_server_count` 的默认提示信息（[#34152](https://github.com/vllm-project/vllm/commit/1f5febb4b8587378a38ea7050503c3cf0431eef6)）。  

- **ae871ca** – Voxtral 模型小幅清理（[#34247](https://github.com/vllm-project/vllm/commit/ae871ca9234be3f6cb6966d998e51a7cb672f912)）。  

- **a2443de** – Model Runner V2 中对 `write_contents` 使用 pinned memory，降低拷贝开销（[#34222](https://github.com/vllm-project/vllm/commit/a2443de5fa4a0605607f6c3d9219022c7f6ac480)）。  

- **f84a2a8** – 加速 Read‑the‑Docs 环境搭建，更新 `.readthedocs.yaml`（[#34240](https://github.com/vllm-project/vllm/commit/f84a2a8f318abdec197b957babe13c9766abb4ed)）。  

- **000214c** – 修复 Qwen3‑Next MTP 中的精度错误（[#34077](https://github.com/vllm-project/vllm/commit/000214c4bb3f4fb61989eea19c625aedd0559ace)）。  

- **c5a66d1** – 修正 PP KV 缓存分片的内存校验逻辑，防止错误的内存分配（[#33698](https://github.com/vllm-project/vllm/commit/c5a66d16970fbbc4633761d30f12ec1fc98a9523)）。  

- **afdce12** – 为 DeepSeek‑V3.2 稀疏注意力新增更快的 `topKperRow` 解码 kernel（[#33680](https://github.com/vllm-project/vllm/commit/afdce12c89555ce7b7bd4f3215b5d844de0a32ed)）。  
  - 关键文件：`csrc/topk.cu`（新增 373 行实现），`tests/kernels/test_top_k_per_row.py`（+111 行测试），以及 `vllm/model_executor/layers/sparse_attn_indexer.py`（+39 行改动）。  

- **82e1197** – 在 trunk 中启用 AOT 编译（torch 2.10），提升编译速度（[#34155](https://github.com/vllm-project/vllm/commit/82e11973cc07909de895a1309ce0f6a2144c576a)）。  

- **b129136** – ROCm 量化路径：支持 AMD‑Quark 格式的 GPT‑OSS 模型加载与仿真（[#29008](https://github.com/vllm-project/vllm/commit/b129136c7a7389133c923123a1ebd76c4401c94d)）。  
  - 关键改动：`vllm/model_executor/layers/quantization/quark/quark.py`（+47 / -23 行），`vllm/model_executor/layers/quantization/quark/quark_moe.py`（+298 / -54 行），以及 `vllm/model_executor/models/gpt_oss.py`（+491 / -18 行）。  

- **599e433** – 为地理空间模型添加基准支持，扩展 `benchmarks` 数据集（[#33922](https://github.com/vllm-project/vllm/commit/599e4335a42bbb6f2cad75ac0b4be81272a77aa3)）。  

- **a194657** – `vllm bench` 新增 `--insecure` 参数，可在测试环境下跳过 TLS（[#34026](https://github.com/vllm-project/vllm/commit/a1946570d80c1bef78063e84b097951d8e8d4e6a)）。  

- **d0bc520** – CI 中将 `mamba-ssm` 版本升级以兼容 Transformers v5（[#34233](https://github.com/vllm-project/vllm/commit/d0bc52056915e108c347aa4b5520e163e5c5b726)）。  

- **748625c** – 修复 EAGLE3 编码器缓存在 `disable_chunked_mm_input` 场景下的 miss（[#34220](https://github.com/vllm-project/vllm/commit/748625cdafd7898b163115d8c33c7c5521a708e8)）。  

- **6141397** – 移除对慢速 tokenizer 的测试，简化 CI（[#34235](https://github.com/vllm-project/vllm/commit/61413973e83b9ca07f3c894a90ddecca0a39d2b6)）。  

- **94de871** – 在 HF 配置中允许显式指定 `is_mm_prefix_lm` 标志（[#34215](https://github.com/vllm-project/vllm/commit/94de871546e8da687c08ed8a7e0a26531500d4bd)）。  

- **e042d7e** – 为 MiniCPM‑o 添加 `flagos` 参数，提升模型可配置性（[#34126](https://github.com/vllm-project/vllm/commit/e042d7e685daacfa9d4df92cc7d330060327a32b)）。  

- **ae4e280** – 修复 Qwen3.5 中 `chunk_gated_delta_rule` kernel 的输出形状（[#34219](https://github.com/vllm-project/vllm/commit/ae4e280602f3c91d322a449f33f5aebbdd59ccc1)）。  

- **cbea11c** – 文档修正：KV 加载失败恢复指南的格式错误（[#34137](https://github.com/vllm-project/vllm/commit/cbea11c9f0ddeef8f5e31449b2e6a37d08e4e653)）。  

**Issues 速览**  

- **#34290** – DeepSeek 模型的 Shared Expert Stream 中仍保留 Clone，建议调查并移除。（[链接](https://github.com/vllm-project/vllm/issues/34290)）  
- **#34289** – DeepSeek 模型的 AR+RMS 与首个 Attention GEMM 需要 PDL 优化。（[链接](https://github.com/vllm-project/vllm/issues/34289)）  
- **#34288** – DeepSeek R1 NVFP4 TRTLLM 中的 DSR1 需要去除多余 Cast。（[链接](https://github.com/vllm-project/vllm/issues/34288)）  
- **#34287** – DeepSeek 模型的 ARRMS + RouterGEMM 需要 PDL 支持。（[链接](https://github.com/vllm-project/vllm/issues/34287)）  
- **#34286** – DeepSeek R1 NVFP4 低延迟（B=1）优化需求：开启 AR 融合、PDL 优化、移除 Clone 等。（[链接](https://github.com/vllm-project/vllm/issues/34286)）  
- **#34284** – CI 中 Metrics Tracing 测试在 2 GPU 环境下出现 segmentation fault，暂未在本地复现。（[链接](https://github.com/vllm-project/vllm/issues/34284)）  
- **#34283** – 多模态模型（Voxtral）在 CI 中的在线服务测试失败，可本地复现。（[链接](https://github.com/vllm-project/vllm/issues/34283)）  
- **#34277** – Disaggregated prefill 中 P2pNcclConnector 的 NCCL send/recv key 不匹配，源于 `assign_request_id()` 随机后缀。（[链接](https://github.com/vllm-project/vllm/issues/34277)）  

> 注：部分提交的 `patch` 被截断（`patch_truncated: true`），具体代码细节请参考对应 PR 页面。  
> 同样，若 Issue 内容被截断或缺失，请在对应页面查看完整描述。
### NVIDIA/cutile-python
昨日无更新。

## 总结
### 展望与关注点
- **硬件适配**：FlashAttention 对 CUDA 12 与 Ascend 的实现需求迫在眉睫，建议关注后续 PR 合并进度。 
- **MoE 与混合专家**：vLLM 的 MoE 抽象层和 DeepSeek 共享专家流的优化仍在讨论中，后续可能带来显著的吞吐提升。 
- **稀疏解码与 TopK 加速**：Tilelang 与 vLLM 的稀疏解码 kernel 已上线，期待在大模型推理中验证其实际加速效果。 
- **CI 与文档**：两项目的 CI 条件细化和文档完善将降低维护成本，建议持续跟进 CI 失败的根因排查。 
- **模型兼容**：Mamba、GLM‑4 MoE、Transformers 5.x 的兼容性提升为多模型部署提供了更稳健的支撑，后续可关注相关性能基准。
