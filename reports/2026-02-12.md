今日的 AI Infra 的新闻如下。

## 摘要
本期更新聚焦于 **FlashInfer** 与 **vLLM** 的功能扩展与兼容性提升：FlashInfer 新增并行单元测试、SM110 FP8 支持、Blackwell‑ptxas 自动检测、FA2 回退逻辑以及 MXFP8 GEMM 实现，显著提升了多算力平台的性能与鲁棒性；vLLM 引入 FlashInfer‑TRTLLM 融合 MoE（FP8/NVFP4）以及稀疏 MLA 后端，并在 CUDA 上强制 DeepGEMM，进一步优化了大模型推理效率。**sglang** 通过更新 ROCm 镜像、复用 Mooncake 传输引擎、精简 gRPC 客户端以及多项 CI/构建加速改动，提升了跨平台部署和 CI 稳定性。**cutlass** 与 **cutile‑python** 主要报告了文档与代码兼容性问题，需关注后续补丁。**flash‑attention** 与 **deepseek‑ai/FlashMLA** 仍无代码提交，仅留待后续跟进。

## 具体内容分析
### deepseek-ai/DeepGEMM
昨日无更新。
### deepseek-ai/FlashMLA
### 提交
暂无提交记录。

### Issues
- **标题**: mla  
  **链接**: https://github.com/deepseek-ai/FlashMLA/issues/164  
  **作者**: TZWX-0  
  **创建时间**: 2026-02-12 09:49:51 UTC  
  **正文**: （暂无正文，可能需要补充说明）
### NVIDIA/cutlass
提交  
（本次暂无代码提交）

Issues  
1. **[QST] [CuTeDSL] `cast_tensor` corner case when no stride = 1**  
   - 链接: https://github.com/NVIDIA/cutlass/issues/3026  
   - 作者: HanGuo97  
   - 创建时间: 2026‑02‑12T15:47:59Z  
   - 说明: 当张量的所有 stride 均不为 1 时，`cute.recast_tensor` 退化为 no‑op，导致 dtype 转换失效。示例代码已在 Issue 中给出。  
   - 对应 PR: 暂无（未关联 PR）

2. **[DOC][CuTeDSL] 4.4.0 quickstart docs points to non‑existent setup.sh**  
   - 链接: https://github.com/NVIDIA/cutlass/issues/3025  
   - 作者: vishnoianil  
   - 创建时间: 2026‑02‑12T08:53:21Z  
   - 说明: 文档中指向的 `setup.sh` 在仓库及标签 v4.4.0 中均不存在，导致快速入门步骤不可执行。  
   - 对应 PR: 暂无（未关联 PR）

3. **[QST] Why does slice_and_offset() (in swizzle_layout.hpp) not consider smem_ptr_flag_bits ?**  
   - 链接: https://github.com/NVIDIA/cutlass/issues/3023  
   - 作者: Allenskyer  
   - 创建时间: 2026‑02‑12T02:34:41Z  
   - 说明: 询问 `slice_and_offset()` 在处理带有 `smem_ptr_flag_bits` 的 swizzle layout 时是否会错误地退化为普通 layout。  
   - 对应 PR: 暂无（未关联 PR）
### flashinfer-ai/flashinfer
**提交**  

- **`1e94c60`** – PR #2531 – *Add parallel testing to unit test script*  
  - **关键改动**  
    - `scripts/task_run_unit_tests.sh`：新增 `export PARALLEL_TESTS=true`，开启并行单元测试。  
    - `scripts/test_utils.sh`：新增 `PARALLEL_TESTS` 环境变量默认 `false`，实现 `detect_gpus()` 用于在并行模式下自动检测可用 GPU（解析 `CUDA_VISIBLE_DEVICES` 或调用 `nvidia-smi`）。  
  - **代码片段**  
    ```bash
    # scripts/task_run_unit_tests.sh
    export PARALLEL_TESTS=true  # Enable parallel test execution
    ```
    ```bash
    # scripts/test_utils.sh (摘录)
    detect_gpus() {
        if [ "$PARALLEL_TESTS" != "true" ]; then echo "0"; return; fi
        if [ -n "$CUDA_VISIBLE_DEVICES" ]; then
            echo "$CUDA_VISIBLE_DEVICES" | tr ',' ' ' | tr -s ' '
            return
        fi
        nvidia-smi --query-gpu=index --format=csv,noheader
    }
    ```
  - **说明**：`test_utils.sh` 的补丁被截断（`patch_truncated: true`），完整实现请在仓库中查看。

- **`002087c`** – PR #2538 – *tests: bmm_fp8 for SM110*  
  - **关键改动**  
    - `flashinfer/gemm/gemm_base.py`：在 `supported_compute_capability` 列表中加入 `110`，使 SM110 支持 FP8 BMM。  
    - `tests/gemm/test_bmm_fp8.py`：新增对 SM110 与 `e5m2` 不兼容的跳过逻辑。  
  - **代码片段**  
    ```python
    @supported_compute_capability([89, 90, 100, 103, 110, 120, 121])
    def _cudnn_bmm_fp8_requirement(...):
        return True
    ```
    ```python
    if compute_capability[0] == 11 and (
        input_dtype == torch.float8_e5m2 or mat2_dtype == torch.float8_e5m2):
        pytest.skip("Invalid combination: only cutlass supports SM110 which does not support e5m2")
    ```

- **`62331fb`** – PR #2543 – *point triton blackwell-ptxas to local cuda ptxas*  
  - **关键改动**  
    - `flashinfer/triton/__init__.py`：新增 `_patch_triton_ptxas_blackwell()`，在检测到系统 `ptxas` 版本 ≥ 13 时自动设置 `TRITON_PTXAS_BLACKWELL_PATH` 环境变量。  
  - **代码片段**  
    ```python
    def _patch_triton_ptxas_blackwell():
        if os.environ.get("TRITON_PTXAS_BLACKWELL_PATH"): return
        ptxas = shutil.which("ptxas")
        out = subprocess.check_output([ptxas, "--version"]).decode()
        if re.search(r"release (\d+)\.", out) and int(m.group(1)) >= 13:
            os.environ["TRITON_PTXAS_BLACKWELL_PATH"] = ptxas
    _patch_triton_ptxas_blackwell()
    ```

- **`8316a3f`** – PR #2536 – *fallback to fa2 (instead of fa3) for unsupported configuration (bf16 Q, Fp8 KV)*  
  - **关键改动**  
    - `flashinfer/utils.py`：在 `is_fa3_backend_supported` 中加入对 **FP8 KV 缓存但查询非 FP8** 的检测，返回 `False` 触发 FA2 回退。  
  - **代码片段**  
    ```python
    if dtype_kv in {torch.float8_e4m3fn, torch.float8_e5m2} and \
       dtype_q not in {torch.float8_e4m3fn, torch.float8_e5m2}:
        return False
    ```

- **`b492d3f`** – PR #2464 – *feat: Add MXFP8 GEMM mm_mxfp8 (cutlass)*  
  - **关键改动**（涉及多文件）  
    - `flashinfer/__init__.py`、`flashinfer/gemm/__init__.py`：导出 `mm_mxfp8`。  
    - `flashinfer/gemm/gemm_base.py`：新增 `_create_cutlass_mxfp8_gemm_module` 与公开函数 `mm_mxfp8`。  
    - `flashinfer/jit/gemm/__init__.py`、`flashinfer/jit/gemm/core.py`：加入 `gen_gemm_sm100_module_cutlass_mxfp8` 生成器。  
    - 新增 C++ 实现：`csrc/mxfp8_gemm_cutlass.cu`、`csrc/mxfp8_gemm_cutlass.jinja`、头文件 `include/flashinfer/gemm/mxfp8_gemm_cutlass.h`、模板 `include/flashinfer/gemm/mxfp8_gemm_cutlass_template.h`、`include/flashinfer/gemm/mxfp8_gemm_template_sm100.h`。  
    - 新增基准与测试：`benchmarks/routines/flashinfer_benchmark_utils.py`、`benchmarks/routines/gemm.py`、`tests/gemm/test_mm_mxfp8.py`。  
  - **代码片段**（`gemm_base.py` 中的核心实现）  
    ```python
    def _create_cutlass_mxfp8_gemm_module(module, op_name, tuner_name):
        class CutlassMxfp8GemmRunner(TunableRunner):
            def get_valid_tactics(self, inputs, profile):
                return list(range(module.mxfp8_gemm_tactic_num()))
            def forward(self, inputs, tactic=-1, do_preparation=False, **kwargs):
                # 调用底层 Cutlass 实现
                return module.mxfp8_gemm_forward(...)
    ```
  - **说明**：多个补丁（如 `gemm_base.py`、`core.py`、`test_mm_mxfp8.py`）的 `patch_truncated` 为 `true`，请在仓库中获取完整 diff。

- **`579435f`** – PR #2535 – *Add sm90 guard to fence.acquire*  
  - **关键改动**  
    - `csrc/nv_internal/tensorrt_llm/kernels/communicationKernels/moeAlltoAllKernels.cu`：在 CUDA 架构 ≥ 900 时使用 `asm volatile("fence.acquire.sys;")`，否则回退 `__threadfence_system()`，确保在 Hopper 上的同步语义。  
  - **代码片段**  
    ```cpp
    #if (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 900))
        asm volatile("fence.acquire.sys;");
    #else
        __threadfence_system();
    #endif
    ```

---

**Issues**  

- **#2551** – *Shared expert fusion integration*  
  - 讨论将 TensorRT‑LLM 的共享专家融合功能合并到 FlashInfer，涉及跨库 API 对齐与性能验证。  
  - 链接：https://github.com/flashinfer-ai/flashinfer/issues/2551  

- **#2546** – *Does MNNVL support SM90?*  
  - 用户在 Hopper 环境下使用 MNNVL 时遇到 `cudaErrorInsufficientDriver (error code 35)`，询问是否需要更高版本的 CUDA 驱动或 Blackwell 兼容性。  
  - 链接：https://github.com/flashinfer-ai/flashinfer/issues/2546  

- **#2544** – *Installing FlashInfer from source failed due to an invalid format in the pyproject.toml file.*  
  - 在执行 `pip install -e .` 时，`pyproject.toml` 解析报错，导致源码安装失败。需要检查 TOML 格式或升级 `pip`。  
  - 链接：https://github.com/flashinfer-ai/flashinfer/issues/2544
### Dao-AILab/flash-attention
提交  
- **标题**：Where can I find the prebuilt package for cu13 Flash Attention?  
  **作者**：lvhan028  
  **创建时间**：2026‑02‑12 11:45:45 UTC  
  **链接**：[https://github.com/Dao-AILab/flash-attention/issues/2255](https://github.com/Dao-AILab/flash-attention/issues/2255)  
  **概要**：用户反馈在 Release 页面提供的 cu13 预编译 wheel 链接失效，询问新的获取方式。  
  **关联 PR / Commit**：当前 Issue 未关联任何 PR，也未提供 commit SHA，故无法给出对应链接。  

- **标题**：Regarding the FixedPosition option in Rotary  
  **作者**：doubleXnine  
  **创建时间**：2026‑02‑12 07:14:14 UTC  
  **链接**：[https://github.com/Dao-AILab/flash-attention/issues/2254](https://github.com/Dao-AILab/flash-attention/issues/2254)  
  **概要**：提出 FixedPosition 选项在对 query (`q`) 使用 RoPE 时生效，但对 key (`k`) 不生效的疑问，包含三个具体技术点的探讨。  
  **关联 PR / Commit**：当前 Issue 未关联任何 PR，也未提供 commit SHA，故无法给出对应链接。  

> **说明**：上述 Issue 均未附带代码改动（diff）或对应的 Pull Request/Commit 信息，故在本期简报中只能呈现问题本身及其链接。若后续有补充的 PR 或 commit，请及时更新。
### sgl-project/sglang
**提交**  

- **Build ROCm7.2 Image with latest AITER v0.1.10.post3**  
  [PR #18741](https://github.com/sgl-project/sglang/commit/f4417475b83238f7fcd1ab8fde17e74a349f9e3b)  
  作者: HAI, 日期: 2026‑02‑12T22:30:13Z  
  变更文件:  
  - `docker/rocm720.Dockerfile` – 将 `ENV AITER_COMMIT` 从 `v0.1.10.post2` 更新为 `v0.1.10.post3`，并在 `gfx950` 基础镜像中开启 `BUILD_AITER_ALL=1`。  
    ```diff
    -ENV AITER_COMMIT="v0.1.10.post2"
    +ENV AITER_COMMIT="v0.1.10.post3"
    -ENV BUILD_AITER_ALL="0"
    +ENV BUILD_AITER_ALL="1"
    ```

- **Reuse initialized transfer engine in mooncake store**  
  [PR #18460](https://github.com/sgl-project/sglang/commit/2a8a48c0caaac3f8e8de13ff4c07a39c4f5e8eaf)  
  作者: Shangming Cai, 日期: 2026‑02‑12T17:21:35Z  
  变更文件:  
  - `python/sglang/srt/distributed/device_communicators/mooncake_transfer_engine.py` – 新增 `get_ib_device` 方法以暴露底层 IB 设备。  
    ```python
    def get_ib_device(self):
        return self.ib_device
    ```
  - `python/sglang/srt/mem_cache/storage/mooncake_store/mooncake_store.py` – 在 `MooncakeStore` 初始化时尝试复用已创建的 `MooncakeTransferEngine`，并在设备名称匹配时使用。  
    ```python
    try:
        from sglang.srt.distributed.parallel_state import get_mooncake_transfer_engine
        self._shared_mooncake_transfer_engine = get_mooncake_transfer_engine()
    except Exception:
        self._shared_mooncake_transfer_engine = None
    ```

- **refactor: remove crate re‑export aliases from lib.rs**  
  [PR #18737](https://github.com/sgl-project/sglang/commit/023652221421d006e7234f0f080fd640ea155a18)  
  作者: Simo Lin, 日期: 2026‑02‑12T17:17:24Z  
  变更文件:  
  - `sgl-model-gateway/benches/consistent_hash_bench.rs` – 将 `smg::mesh::consistent_hash::ConsistentHashRing` 替换为 `smg_mesh::consistent_hash::ConsistentHashRing`。  
    ```diff
    -use smg::mesh::consistent_hash::ConsistentHashRing;
    +use smg_mesh::consistent_hash::ConsistentHashRing;
    ```

- **fix: image version in pypi pr workflow**  
  [PR #18735](https://github.com/sgl-project/sglang/commit/e730c728d33532d8674105b9b2fc12163d5ed909)  
  作者: Douglas Yang, 日期: 2026‑02‑12T16:18:01Z  
  变更文件:  
  - `.github/workflows/release-pypi-pr.yml` – 调整镜像标签的拼写（8 行新增，7 行删除），完整 diff 未显示。

- **speed up sgl‑kernel build**  
  [PR #18586](https://github.com/sgl-project/sglang/commit/9e9e94926162d0b2bf189c9722e97dbdc59b7375)  
  作者: Xiaoyu Zhang, 日期: 2026‑02‑12T15:43:22Z  
  变更文件:  
  - `.github/workflows/release-whl-kernel.yml` – 小幅优化 CI 步骤。  
  - `sgl-kernel/Dockerfile` – 添加构建缓存层。  
  - `sgl-kernel/build.sh` – 新增 77 行脚本，加速编译流程。  
  - `sgl-kernel/csrc/elementwise/concat_mla.cu` – 增加 1 行实现细节（具体代码未展示）。

- **refactor: consolidate gRPC client into shared crate dependency**  
  [PR #18730](https://github.com/sgl-project/sglang/commit/2f283d81525a19b1fd706f38c66a6f25924b0eb3)  
  作者: Simo Lin, 日期: 2026‑02‑12T14:43:24Z  
  变更文件（大量删除）:  
  - 移除 `sgl-model-gateway/src/grpc_client/*`（共 1 587 行删除），改为在 `Cargo.toml` 中依赖共享 crate。  
  - `sgl-model-gateway/src/lib.rs`、`src/observability/otel_trace.rs`、`src/routers/grpc/client.rs` 等文件相应更新，仅保留必要的导入。  
  - 删除 `proto/sglang_scheduler.proto` 与 `proto/vllm_engine.proto`（共 761 行删除）。  
  完整 diff 过长，已省略。

- **Fix B200 installation issue**  
  [PR #18725](https://github.com/sgl-project/sglang/commit/1b8f68af57c039055ec9b0b08b972989ee72a995)  
  作者: Kangyan‑Zhou, 日期: 2026‑02‑12T14:06:23Z  
  变更文件:  
  - `scripts/ci/cuda/ci_install_dependency.sh` – 新增 2 行解决 B200 环境下的依赖冲突。

- **[BUGFIX] fix bug in handle mamba radix cache in server_args**  
  [PR #18723](https://github.com/sgl-project/sglang/commit/b16872342485a365d0496f7a8901b1c90a58b5f0)  
  作者: Yi Zhang, 日期: 2026‑02‑12T13:33:32Z  
  变更文件:  
  - `python/sglang/srt/server_args.py` – 修正 `mamba` radix 缓存处理逻辑（6 行新增，1 行删除）。

- **refactor: replace local proto compilation with smg‑grpc‑proto package**  
  [PR #18682](https://github.com/sgl-project/sglang/commit/92c5749f412d05720888fddc6de6e56fb6b928d6)  
  作者: Simo Lin, 日期: 2026‑02‑12T13:29:24Z  
  变更文件:  
  - `.github/workflows/lint.yml`、`.gitignore`、多个 `python/pyproject*.toml`（增删 10 行）  
  - 删除 `python/setup.py`（125 行删除）以及 `python/sglang/srt/grpc/compile_proto.py`、`sglang_scheduler.proto`、`vllm_engine.proto`（共 1 383 行删除），改为使用 `smg-grpc-proto` 包。  
  完整 diff 过长，已省略。

- **Add `spec_accept_histogram` request statistic**  
  [PR #18332](https://github.com/sgl-project/sglang/commit/c59b9223e6839ddd971f872358ef4d02cad74e21)  
  作者: Scott Lee, 日期: 2026‑02‑12T13:09:21Z  
  变更文件（核心片段）:  
  - `python/sglang/srt/managers/schedule_batch.py` – 新增 `spec_accept_histogram` 统计字段（+18 行）。  
  - 其他 manager 文件均有少量（6‑10 行）字段或注释的补充。

- **Fix flaky penalty tests by using higher temperature for effect comparison**  
  [PR #18380](https://github.com/sgl-project/sglang/commit/0abe4a22c6764ccf2be52f418fc2662e6eb4a2d0)  
  作者: Alison Shao, 日期: 2026‑02‑12T13:08:37Z  
  变更文件:  
  - `test/registered/sampling/test_penalty.py` – 调整 `temperature` 参数，新增 44 行，删除 42 行，以提升测试稳定性。

- **Make PR based docker and pypi workflow work for forked PR**  
  [PR #18720](https://github.com/sgl-project/sglang/commit/f116b3a51ba182be67843cdd8e307aa5a91db694)  
  作者: Kangyan‑Zhou, 日期: 2026‑02‑12T13:05:17Z  
  变更文件:  
  - `.github/workflows/release-docker-dev-pr.yml`、`.github/workflows/release-pypi-pr.yml` – 调整触发条件（共 4 行新增，12 行删除）。

- **add tool_choice=auto nightly test case**  
  [PR #18302](https://github.com/sgl-project/sglang/commit/f3656432c7a365c9e678f5f2dd02d9acb05437fb)  
  作者: Hudson Xing, 日期: 2026‑02‑12T11:28:05Z  
  变更文件:  
  - `python/sglang/test/tool_call_test_runner.py` – 新增 11 行测试用例，验证 `tool_choice=auto` 行为。

- **[AMD] Fix accuracy issue when running TP4 dsv3 model with mtp**  
  [PR #18607](https://github.com/sgl-project/sglang/commit/e20e6c28b9bf56f632c64fdfeb5b8871237e8d88)  
  作者: Thomas Wang, 日期: 2026‑02‑12T09:13:16Z  
  变更文件:  
  - `docker/rocm.Dockerfile` – 小幅改动（+2 / -2 行）。  
  - `python/sglang/srt/layers/attention/aiter_backend.py` – 调整数值计算以提升 TP4 精度（+7 / -3 行）。

- **[AMD] reset AMD image release time and reduce CI queue time**  
  [PR #18707](https://github.com/sg
### vllm-project/vllm
## 提交

**[Kernel] Support Flashinfer trtllm fused MoE non gated FP8 & NVFP4 (#33506)**
- 为 Flashinfer trtllm fused MoE 添加了对非门控 FP8 和 NVFP4 的支持。
- 修改了多个量化和 MoE 相关文件，包括 `flashinfer_trtllm_moe.py`、`flashinfer_fp4_moe.py` 和 `flashinfer_utils.py`。
- [f120bd4](https://github.com/vllm-project/vllm/commit/f120bd42d3daf733425d7feaaeffc2a23ba71c17)

**[Bugfix] Enforce DeepGEMM when using sparse_attn_indexer on CUDA (#34374)**
- 在 CUDA 上使用 `sparse_attn_indexer` 时强制启用 DeepGEMM。
- 修改了 `sparse_attn_indexer.py` 文件。
- [6d4e27c](https://github.com/vllm-project/vllm/commit/6d4e27ce29bac0e4cd4975cddf5b0dacc6cb727a)

**[ROCm][CI] Pin TorchCodec to v0.10.0 for ROCm compatibility (#34447)**
- 为 ROCm 兼容性，将 TorchCodec 固定到 v0.10.0。
- 修改了 `tools/install_torchcodec_rocm.sh` 文件。
- [4c078fa](https://github.com/vllm-project/vllm/commit/4c078fa546016eacab87f833ff625463421f7d29)

**[Voxtral Realtime] Refactor & Improve buffering logic (#34428)**
- 重构并改进了 Voxtral Realtime 的缓冲逻辑。
- 修改了 `voxtral_realtime.py` 和相关测试文件。
- [6c0baee](https://github.com/vllm-project/vllm/commit/6c0baee61025f258c6d56830d0150feab34c45ab)

**[ROCm][quantization] improve OCP weight quant parser robust (#34431)**
- 改进了 ROCm 平台上 OCP 权重量化解析器的健壮性。
- 修改了 `quark/quark.py` 文件。
- [766e167](https://github.com/vllm-project/vllm/commit/766e1678210d797757dcfe28f05184a251685dfe)

**[Bugfix] Remove broken raw url GGUF model loading support (#34433)**
- 移除了对原始 URL GGUF 模型加载的支持（已损坏）。
- 修改了 `gguf_loader.py` 和相关测试文件。
- [becbe24](https://github.com/vllm-project/vllm/commit/becbe2480871573f9464e4941b179c1c21f2c786)

**[Attention] Add FlashInfer Sparse MLA backend (#33451)**
- 添加了 FlashInfer Sparse MLA 注意力后端。
- 修改了多个注意力基准测试文件和配置文件。
- [f2c4788](https://github.com/vllm-project/vllm/commit/f2c47886fdbabfeae7ddad871ee7889ee472d026)

**[V0 Deprecation] Remove code related to per-request logits processors (#34400)**
- 移除了与每个请求的 logits 处理器相关的代码（V0 弃用）。
- 修改了多个入口点和配置文件。
- [fb455ed](https://github.com/vllm-project/vllm/commit/fb455ed547a63e97e15deccfc493f8eef7a2da5c)

## Issues

**[Bug]: NVIDIA Jetson Thor: Value 'sm_110a' is not defined for option 'gpu-name'**
- 在 NVIDIA Jetson Thor 上运行时，出现 GPU 名称未定义的错误。
- Issue body 已截断，需进一步查看完整内容。
- [#34470](https://github.com/vllm-project/vllm/issues/34470)

**[CI Failure]: Intel HPU Test**
- Intel HPU 测试失败，提示 `vllm.envs` 模块缺少 `VLLM_TORCH_PROFILER_DIR` 属性。
- [#34465](https://github.com/vllm-project/vllm/issues/34465)

**[CI Failure]: Ascend NPU Test**
- Ascend NPU 测试失败，Issue body 缺失。
- [#34464](https://github.com/vllm-project/vllm/issues/34464)

**[CI Failure]: ARM CPU Test (test_moe.py)**
- ARM CPU 上的 `test_moe.py` 测试失败，Issue body 缺失。
- [#34463](https://github.com/vllm-project/vllm/issues/34463)

**[CI Failure]: Distributed Tests (2 GPUs)(H100) test_dbo.py**
- 分布式测试（2 GPUs H100）中的 `test_dbo.py` 失败，Issue body 缺失。
- [#34460](https://github.com/vllm-project/vllm/issues/34460)

**[CI Failure]: Entrypoints Integration (Responses API) (test_parsable_context.py)**
- 入口点集成测试（Responses API）中的 `test_parsable_context.py` 失败，Issue body 缺失。
- [#34459](https://github.com/vllm-project/vllm/issues/34459)

**[Bug]: AR+rms broken for TP=2 DP=2**
- 在 TP=2 DP=2 配置下，AR+rms 功能损坏。
- [#34458](https://github.com/vllm-project/vllm/issues/34458)

**[Usage]: Issue Running Nemotron 3 Nano NVFP4 on sm12x (RTX 5090 / Blackwell Pro 6000)**
- 在 sm12x（RTX 5090 / Blackwell Pro 6000）上运行 Nemotron 3 Nano NVFP4 时出现问题。
- [#34452](https://github.com/vllm-project/vllm/issues/34452)
### NVIDIA/cutile-python
**提交**  
- **SHA**: `181a487`  
- **链接**: https://github.com/NVIDIA/cutile-python/commit/181a487bd170534973c7d49bf9f4b35898cd70c6  
- **作者**: jiahuil  
- **日期**: 2026‑02‑12 03:45:55 UTC  
- **信息**: fix np.bool  

**文件变更**  

| 模块/文件 | 类型 | 增删行数 | 说明 |
|----------|------|----------|------|
| `samples/AttentionFMHA.py` | 修改 | +1 / -1 | 代码中对 `np.bool` 的使用被修正。<br>*注：patch 内容被截断，未显示具体代码片段。* |
| `test/kernels/attention.py` | 修改 | +1 / -1 | 对应测试文件同步更新，以匹配修正后的 `np.bool` 用法。<br>*注：patch 内容被截断，未显示具体代码片段。* |

**Issues**  
- 无关联 Issue。

## 总结
⚡ **关注点**：
- 及时验证 FlashInfer 的 MXFP8 与 FA2 回退在实际工作负载中的数值一致性；
- 在使用 vLLM 的 MoE FP8/NVFP4 时，检查对应的硬件（Hopper/Blackwell）驱动版本以避免兼容性错误；
- 关注 sglang 对 ROCm 与 CUDA 环境的依赖变更，确保 CI/CD 流水线同步更新；
- 对 cutlass 文档中缺失的 `setup.sh` 与 `cast_tensor` 边界情况保持警惕，必要时自行提交补丁。
