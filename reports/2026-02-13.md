ä»Šæ—¥çš„ AI Infra çš„æ–°é—»å¦‚ä¸‹ã€‚

## æ‘˜è¦
## æœ¬å‘¨å…³é”®å˜æ›´æ¦‚è§ˆ

- **NVIDIA/cutlass**ï¼š4.4.0 æ­£å¼å‘å¸ƒï¼ŒCuTeDSL å¼•å…¥ CTA çº§åˆ«ç¤ºä¾‹å¹¶æ”¯æŒ **LayerNorm / RMSNorm**ï¼Œ`fence_proxy` è¯­æ³•ç®€åŒ–ä¸ºå­—ç¬¦ä¸²ï¼ŒTVMâ€¯FFI åˆå§‹åŒ–é»˜è®¤æµ `-1`ï¼Œæ–°å¢ **AoT ç¼–è¯‘ã€JAX æ”¯æŒã€å®éªŒæ€§ CUDAâ€¯13.1 / GB300** ç­‰ç‰¹æ€§ï¼Œå¹¶åŒæ­¥æ›´æ–°å¤§é‡ç¤ºä¾‹ä»£ç ï¼ˆå¦‚ `cta_norm.py`ã€`cooperative_launch.py`ï¼‰ã€‚

- **flashinfer-ai/flashinfer**ï¼šä¿®å¤ç±»å‹ä¸åŒ¹é… bugï¼Œ**AOTâ€¯JIT ç¼“å­˜**åŠ å…¥ `fp8_blockscale_gemm_90` ä¸ `gen_gemm_sm100_module_cutlass_mxfp8`ï¼Œä»¥åŠ **CUDAâ€¯Graph** é‡‡æ ·æ¨¡å—çš„ç¨³å¥æ€§æå‡ã€‚

- **Dao-AILab/flash-attention**ï¼šå¤§å¹… DSL é‡æ„ï¼Œ`ParamsBase` ç»Ÿä¸€å‚æ•°ç®¡ç†ï¼ŒSM90/SM100 åå‘æ ¸å®ç° **å¤§å¹…å®½åº¦é‡æ„** ä¸ **dK/dV R2S** ä¼˜åŒ–ï¼Œè°ƒåº¦å™¨é€»è¾‘ç®€åŒ–ï¼Œæ•´ä½“ä»£ç è¡Œæ•°å‡€å‡å°‘çº¦ 300 è¡Œã€‚

- **sgl-project/sglang**ï¼šæœ¬å‘¨æäº¤é‡æœ€é«˜ï¼Œæ¶‰åŠ **VAE `channels_last_3d` ä¼˜åŒ–ã€fastâ€‘hadamardâ€‘transform JITã€Specâ€¯V2 å¼€å…³ã€MROPE/YARN æ”¯æŒã€NSA å¤šè§„æ ¼åç«¯èåˆã€AMD/ROCm å…¼å®¹æ€§ã€åˆ†å¸ƒå¼åˆå§‹åŒ–å˜é‡ã€æš‚åœ/æ¢å¤ APIã€ä»¥åŠå¤§é‡ CI/æ€§èƒ½åŸºå‡†æ›´æ–°**ï¼Œä»£ç æ”¹åŠ¨ç´¯è®¡è¶…è¿‡ 13â€¯k è¡Œï¼Œæ˜¾è‘—æå‡å¤šæ¨¡æ€ã€æ‰©æ•£ä¸å¤§æ¨¡å‹æ¨ç†çš„ååä¸å¯ç»´æŠ¤æ€§ã€‚

- **vllm-project/vllm**ï¼šèšç„¦ **Mamba å‰ç¼€ç¼“å­˜å¯¹é½ã€è¾“å…¥é¢„å¤„ç†è¿å…¥æ¸²æŸ“å±‚ã€ROCâ€¯m å…¼å®¹æ€§é˜²æŠ¤ã€Qwen3.5 é…ç½®ä¿®å¤ã€COLQwen3 å¤šæ¨¡æ€æ¨¡å‹ã€Specâ€¯è§£ç åœ¨å¯¹é½æ¨¡å¼ä¸‹å¯ç”¨ã€Pipelineâ€‘Parallel å¼‚æ­¥é€šä¿¡æå‡ 2.9% ååã€CPUâ€¯Offload åŒåˆ†é…ä¼˜åŒ–ã€KV è¿æ¥å™¨ç²¾ç®€ã€æš‚åœ/æ¢å¤è¿å…¥å¼•æ“å±‚**ï¼Œå¹¶åŒæ­¥å¤§é‡æµ‹è¯•ä¸ç¤ºä¾‹ã€‚

- **NVIDIA/cutile-python**ï¼šåœ¨ CUDA Tile ç¼–è¯‘ä¸Šä¸‹æ–‡ä¸­åŠ å…¥ **ä¸´æ—¶ç›®å½•è‡ªåŠ¨åˆ›å»º**ï¼Œæå‡è„šæœ¬é²æ£’æ€§ã€‚

## å½±å“ä¸è¶‹åŠ¿
- **DSL ä¸ AOT/JIT ç»“åˆ**ï¼šCutlassã€FlashAttention ä¸ vLLM å‡åœ¨ DSL å±‚é¢åŠ å…¥ AoT/JIT ç¼“å­˜ï¼Œé™ä½è¿è¡Œæ—¶ç¼–è¯‘å¼€é”€ã€‚
- **æ–°ç¡¬ä»¶é€‚é…**ï¼šCUDAâ€¯13.1ã€SM100/SM120ã€AMDâ€¯ROCm ç­‰å¹³å°çš„ä¸“å±ä¼˜åŒ–æŒç»­åŠ é€Ÿï¼Œå°¤å…¶åœ¨æ³¨æ„åŠ›ã€MoE ä¸æ··åˆç²¾åº¦åœºæ™¯ã€‚
- **å¤šæ¨¡æ€ä¸æ‰©æ•£**ï¼šSGLang ä¸ FlashInfer å¤§å¹…å¼ºåŒ– VAEã€MROPEã€Diffusion ç›¸å…³å®ç°ï¼Œè¡¨æ˜è¡Œä¸šå¯¹è§†è§‰â€‘è¯­è¨€ã€ç”Ÿæˆå¼æ¨¡å‹çš„éœ€æ±‚ä¸Šå‡ã€‚
- **è°ƒåº¦ä¸å¹¶è¡Œ**ï¼švLLM çš„ Pipelineâ€‘Parallel å¼‚æ­¥é€šä¿¡ã€Mamba å‰ç¼€ç¼“å­˜å¯¹é½ã€SGLang çš„åˆ†å¸ƒå¼åˆå§‹åŒ–å˜é‡ç­‰ï¼Œå‡¸æ˜¾å¯¹å¤§è§„æ¨¡æ¨ç†è°ƒåº¦çš„æ·±åº¦ä¼˜åŒ–ã€‚
- **CI ä¸å¯ç»´æŠ¤æ€§**ï¼šå¤šé¡¹ç›®åŠ å…¥ ROCm å…¼å®¹é˜²æŠ¤ã€CI å¹¶å‘ç­–ç•¥ã€æ–‡æ¡£é“¾æ¥æ£€æŸ¥ç­‰ï¼Œæå‡å‘å¸ƒè´¨é‡ä¸è·¨å¹³å°ç¨³å®šæ€§ã€‚

## å…·ä½“å†…å®¹åˆ†æ
### deepseek-ai/DeepGEMM
æ˜¨æ—¥æ— æ›´æ–°ã€‚
### deepseek-ai/FlashMLA
æ˜¨æ—¥æ— æ›´æ–°ã€‚
### NVIDIA/cutlass
## æäº¤

| SHA | PR | ä½œè€… | æ—¥æœŸ | ç®€è¦æè¿° |
|-----|----|------|------|----------|
| [291300f](https://github.com/NVIDIA/cutlass/commit/291300ffffa3533a78ee104f08a8490a29ce9ccb) | [#3009](https://github.com/NVIDIA/cutlass/pull/3009) | Yihan Chen | 2026â€‘02â€‘14 | åœ¨ CuTeDSL ä¸­å®ç°äº† CTA çº§åˆ«çš„å½’ä¸€åŒ–ç¤ºä¾‹ï¼Œæ”¯æŒ LayerNorm ä¸ RMSNormã€‚ |
| [f9a5f76](https://github.com/NVIDIA/cutlass/commit/f9a5f76b7a4f4298d4ec4039c9ae2a673c9fab93) | [#3027](https://github.com/NVIDIA/cutlass/pull/3027) | aragornâ€‘guan | 2026â€‘02â€‘14 | å°† `fence_proxy` è°ƒç”¨ç®€åŒ–ä¸ºå­—ç¬¦ä¸²å½¢å¼ï¼Œæå‡ä»£ç å¯è¯»æ€§ã€‚ |
| [ec7e6cb](https://github.com/NVIDIA/cutlass/commit/ec7e6cb17be771ee502e96f16ec923843c12c6d0) | åˆå¹¶ PR #2971 | drazi | 2026â€‘02â€‘14 | TVMâ€¯FFI åˆå§‹åŒ–æ”¹ä¸º `tvm_ffi.from_dlpack(tensor, stream=-1)`ï¼Œæ˜¾å¼æŒ‡å®šé»˜è®¤æµã€‚ |
| [395ab57](https://github.com/NVIDIA/cutlass/commit/395ab575f659a82a26dc577e0314017758f07d42) | åˆå¹¶åˆ†æ”¯ `tvâ€‘mâ€‘ffi` | Yuanâ€¯Xiaolan | 2026â€‘02â€‘14 | æ›´æ–° CHANGELOG ä¸ READMEï¼Œæ ‡è®° 4.4.0 æ­£å¼å‘å¸ƒå¹¶åˆ—å‡º CuTeDSL æ–°ç‰¹æ€§ï¼ˆCUDAâ€¯13.1ã€GB300ã€experimental API ç­‰ï¼‰ã€‚ |
| [d4bbf72](https://github.com/NVIDIA/cutlass/commit/d4bbf728ca52ce23a549a299e229666e734ffb7c) | [#3032](https://github.com/NVIDIA/cutlass/pull/3032) | Junkaiâ€‘Wu | 2026â€‘02â€‘14 | 4.4.0 å‘è¡Œè¯´æ˜ç»†åŒ–ï¼Œæ–°å¢å®éªŒæ€§ç¤ºä¾‹ã€AoT ç¼–è¯‘ã€JAX æ”¯æŒç­‰ã€‚ |
| [01687cf](https://github.com/NVIDIA/cutlass/commit/01687cfba1c7d27c68b7b3c5cca3b5b4654e0da7) | åˆå¹¶ PR #3004 | drazi | 2026â€‘02â€‘13 | ä¸º `sub_packed_f32x2` æ·»åŠ æ¶æ„å±‚æ”¯æŒï¼›åœ¨ `arch/__init__.py` ä¸ `arch/nvvm_wrappers.py` ä¸­åŠ å…¥å°‘é‡ä»£ç ã€‚ |
| [5c42d0f](https://github.com/NVIDIA/cutlass/commit/5c42d0f28c9dc728d874fd552492a355036a53e1) | åˆå¹¶ PR #3021 | drazi | 2026â€‘02â€‘13 | åœ¨ `arch/clc.py` ä¸­å»é™¤å¤šæ’­ç›¸å…³å®ç°ï¼Œç®€åŒ– CLC æŒ‡ä»¤è·¯å¾„ã€‚ |
| [1d36152](https://github.com/NVIDIA/cutlass/commit/1d36152f34c06bce45cf196269c08a78a71079aa) | åˆå¹¶ PR #3022 | drazi | 2026â€‘02â€‘13 | ä¸º `nvvm_wrappers.py` æ·»åŠ  `fmin` åŒ…è£…å‡½æ•°ï¼Œæå‡ DSL å¯¹æœ€å°å€¼æ“ä½œçš„æ”¯æŒã€‚ |

### å…³é”®æ–‡ä»¶å˜æ›´

- **examples/python/CuTeDSL/hopper/cta_norm.py** (æ–°å¢)  
  ```python
  # Copyright (c) 2025 - 2026 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  ```
- **examples/python/CuTeDSL/distributed/all_reduce_tma.py** (ä¿®æ”¹)  
  ```diff
  - cute.arch.fence_proxy(
  -     cute.arch.ProxyKind.async_shared,
  -     space=cute.arch.SharedSpace.shared_cta,
  - )
  + cute.arch.fence_proxy("async.shared", space="cta")
  ```
- **python/CuTeDSL/cutlass/cute/runtime.py** (ä¿®æ”¹)  
  ```diff
  - self._tvm_ffi_tensor = tvm_ffi.from_dlpack(tensor)
  + self._tvm_ffi_tensor = tvm_ffi.from_dlpack(tensor, stream=-1)
  ```
- **CHANGELOG.md** (ä¿®æ”¹) â€“ ç« èŠ‚æ ‡é¢˜æ›´æ–°ä¸ºæ­£å¼å‘å¸ƒé“¾æ¥ï¼Œåˆ—å‡º CuTeDSL å¯¹ CUDAâ€¯13.1ã€GB300ã€experimental API ç­‰çš„æ”¯æŒã€‚  
  *æ³¨ï¼šdiff è¿‡é•¿ï¼Œä»…å±•ç¤ºå‰å‡ è¡Œã€‚*
- **README.md** (ä¿®æ”¹) â€“ åŒä¸Šï¼Œæ›´æ–°ç‰ˆæœ¬æ—¶é—´æˆ³å¹¶åŒæ­¥æ–°ç‰¹æ€§åˆ—è¡¨ã€‚  
  *æ³¨ï¼šdiff è¿‡é•¿ï¼Œä»…å±•ç¤ºå‰å‡ è¡Œã€‚*
- **examples/python/CuTeDSL/ampere/cooperative_launch.py** (æ–°å¢)  
  ```python
  # Copyright (c) 2025 - 2026 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  ```
- **examples/python/CuTeDSL/ampere/elementwise_add.py** (ä¿®æ”¹) â€“ ä¸º kernel è®¾ç½®åç§°å‰ç¼€ï¼š  
  ```python
  kernel_name = f"cutlass_dsl_elementwise_add_kernel"
  elementwise_add_kernel.set_name_prefix(kernel_name)
  ```
- **examples/python/CuTeDSL/ampere/elementwise_add_autotune.py** (æ–°å¢) â€“ åŒä¸Šï¼ŒåŒ…å«å®Œæ•´ç¤ºä¾‹ä»£ç ï¼ˆçœç•¥ï¼‰ã€‚  
- **examples/python/CuTeDSL/ampere/elementwise_apply.py** (ä¿®æ”¹) â€“ åœ¨ `leaky_relu_ref` ä¸­å±€éƒ¨å¯¼å…¥ `torch`ï¼Œå¹¶åœ¨ `run_and_verify` ä¸­æ˜¾å¼å¯¼å…¥ `torch` ä¸ `cutlass.torch`ã€‚  
- **examples/python/CuTeDSL/ampere/flash_attention_v2.py** (ä¿®æ”¹) â€“ åœ¨ `run` ä¸ `create_tensor` ä¸­åŠ å…¥ `import torch` ä¸ `cutlass.torch`ï¼Œå¹¶æ¼”ç¤ºç¼–è¯‘é€‰é¡¹ä¼ é€’ã€‚  
- **examples/python/CuTeDSL/ampere/hstu_attention.py** (ä¿®æ”¹) â€“ ç§»é™¤æ˜¾å¼çš„ `torch` ç±»å‹æ³¨è§£ï¼Œæ”¹ä¸ºè¿è¡Œæ—¶å¯¼å…¥ï¼›æ›´æ–°æ–‡æ¡£æè¿°ã€‚  
- **examples/python/CuTeDSL/ampere/inline_ptx.py** (ä¿®æ”¹) â€“ æ–‡æ¡£ä¸­å°† â€œnvvm dialectâ€ è¡¨è¿°ç»Ÿä¸€ä¸º â€œnvvm dialectâ€ã€‚  
- **examples/python/CuTeDSL/ampere/sgemm.py** (ä¿®æ”¹) â€“ åœ¨ `run` å‡½æ•°é¡¶éƒ¨åŠ å…¥ `import torch`ï¼Œå¹¶åœ¨å…¥å£å¤„ä¿ç•™ `torch.manual_seed(1024)`ã€‚  
- **examples/python/CuTeDSL/ampere/smem_allocator.py** (ä¿®æ”¹) â€“ åœ¨ `run_and_verify` ä¸­å±€éƒ¨å¯¼å…¥ `torch`ï¼Œä¿æŒ GPU Tensor åˆå§‹åŒ–ã€‚  
- **python/CuTeDSL/cutlass/cute/arch/__init__.py** (ä¿®æ”¹) â€“ æ–°å¢ä¸€è¡Œå¯¼å…¥ï¼ˆå…·ä½“ä»£ç ç‰‡æ®µæœªæ˜¾ç¤ºï¼Œdiff ä»… 1 è¡Œæ·»åŠ ï¼‰ã€‚  
- **python/CuTeDSL/cutlass/cute/arch/nvvm_wrappers.py** (ä¿®æ”¹) â€“ æ–°å¢ `fmin` åŒ…è£…å‡½æ•°ï¼ˆ14 è¡Œæ–°å¢ä»£ç ï¼‰ã€‚  
- **python/CuTeDSL/cutlass/cute/arch/clc.py** (ä¿®æ”¹) â€“ åˆ é™¤å¤šæ’­ç›¸å…³å®ç°ï¼Œä¿ç•™ 15 è¡Œæ–°å¢ã€6 è¡Œåˆ é™¤çš„ä»£ç ã€‚  

> **è¯´æ˜**ï¼šè‹¥ `patch_truncated` ä¸º `true`ï¼Œè¡¨ç¤ºå®Œæ•´ diff æœªå±•ç¤ºï¼Œä»…æä¾›å…³é”®ç‰‡æ®µæˆ–æ¦‚è¿°ã€‚

## Issues

| ç¼–å· | æ ‡é¢˜ | é“¾æ¥ | ä½œè€… | åˆ›å»ºæ—¶é—´ |
|------|------|------|------|----------|
| #3031 | **[FEA] Assume leading dim when size 1 =** | https://github.com/NVIDIA/cutlass/issues/3031 | drisspg | 2026â€‘02â€‘13 |
| #3029 | **[FEA] Add CopyReduceBulkS2G** | https://github.com/NVIDIA/cutlass/issues/3029 | tridao | 2026â€‘02â€‘13 |

*ä¸¤æ¡ Issue å‡ä¸º CuTeDSL åŠŸèƒ½éœ€æ±‚ï¼Œåˆ†åˆ«æ¶‰åŠåŠ¨æ€å¸ƒå±€æ¨æ–­ä¸æ–°å¢ `CopyReduceBulkS2G` æŒ‡ä»¤å°è£…ã€‚*
### flashinfer-ai/flashinfer
**æäº¤è®°å½•**  

- **f4d10a7** â€“ *bugfix*: ä¿®å¤ #2507 ä¸­æåˆ°çš„æšä¸¾/æ•´æ•°ç±»å‹ä¸åŒ¹é…ã€‚  
  - ä½œè€…ï¼šZihaoâ€¯Yeâ€ƒæ—¥æœŸï¼š2026â€‘02â€‘14 17:26:34â€¯UTCâ€ƒ[PR é“¾æ¥](https://github.com/flashinfer-ai/flashinfer/commit/f4d10a7dca0082f45ac8010bf3a798f90f3d05a6)  
  - å˜æ›´æ–‡ä»¶ï¼š  
    - `flashinfer/fused_moe/core.py`ï¼ˆä¿®æ”¹ 4 è¡Œï¼Œåˆ é™¤ 4 è¡Œï¼‰  
    - `tests/moe/test_trtllm_gen_fused_moe.py`ï¼ˆä¿®æ”¹ 8 è¡Œï¼Œåˆ é™¤ 8 è¡Œï¼‰  
  - è¯´æ˜ï¼šdiff å·²è¢«æˆªæ–­ï¼Œå…·ä½“ä»£ç ç‰‡æ®µæœªæ˜¾ç¤ºã€‚

- **292f9be** â€“ *fix*: å°† `fp8_blockscale_gemm_90` çº³å…¥ AOT JITâ€‘cacheã€‚  
  - ä½œè€…ï¼šEdwardâ€ƒæ—¥æœŸï¼š2026â€‘02â€‘13 04:29:43â€¯UTCâ€ƒ[PR é“¾æ¥](https://github.com/flashinfer-ai/flashinfer/commit/292f9be3f5f6d76248d4d3577c167fd178d7952d)  
  - å˜æ›´æ–‡ä»¶ï¼š  
    - `flashinfer/aot.py`ï¼ˆæ–°å¢ 3 è¡Œï¼‰  
  - è¯´æ˜ï¼šä»…æ·»åŠ äº†å¯¹æ–° kernel çš„æ³¨å†Œä»£ç ï¼Œdiff å·²è¢«æˆªæ–­ã€‚

- **c5b8a2e** â€“ *fix*: Sampling æ¨¡å—çš„ CUDA Graph ä¿®å¤ã€‚  
  - ä½œè€…ï¼šIzzyâ€¯Puttermanâ€ƒæ—¥æœŸï¼š2026â€‘02â€‘13 00:55:46â€¯UTCâ€ƒ[PR é“¾æ¥](https://github.com/flashinfer-ai/flashinfer/commit/c5b8a2e0e2a54e115f80fa6d096000536243c6ec)  
  - å˜æ›´æ–‡ä»¶ï¼š  
    - `csrc/flashinfer_sampling_binding.cu`ï¼ˆ+19â€¯â€‘8ï¼‰  
    - `csrc/sampling.cu`ï¼ˆ+104â€¯â€‘16ï¼‰  
    - `flashinfer/sampling.py`ï¼ˆ+291â€¯â€‘76ï¼‰  
    - `include/flashinfer/sampling.cuh`ï¼ˆ+82â€¯â€‘37ï¼‰  
  - è¯´æ˜ï¼šå¤§å¹…åº¦é‡æ„äº†é‡‡æ ·ç»‘å®šä¸ CUDA Graph é€»è¾‘ï¼Œdiff å·²è¢«æˆªæ–­ã€‚

- **2fff6b6** â€“ *feat*: å°† `gen_gemm_sm100_module_cutlass_mxfp8` åŠ å…¥ JITâ€‘cacheã€‚  
  - ä½œè€…ï¼šYongâ€¯Wuâ€ƒæ—¥æœŸï¼š2026â€‘02â€‘13 00:25:54â€¯UTCâ€ƒ[PR é“¾æ¥](https://github.com/flashinfer-ai/flashinfer/commit/2fff6b6a3791bd1abf9a7e829ab3096a563dc86b)  
  - å˜æ›´æ–‡ä»¶ï¼š  
    - `flashinfer/aot.py`ï¼ˆæ–°å¢ 2 è¡Œï¼‰  
  - è¯´æ˜ï¼šä»…åœ¨ AOT æ³¨å†Œè¡¨ä¸­æ·»åŠ äº†å¯¹åº”æ¨¡å—å…¥å£ï¼Œdiff å·²è¢«æˆªæ–­ã€‚

---

**Issues**  

- **griddepcontrol.wait should use "memory" clobber**  
  - URL: <https://github.com/flashinfer-ai/flashinfer/issues/2558>  
  - ä½œè€…ï¼šMatthiasKohlâ€ƒåˆ›å»ºæ—¶é—´ï¼š2026â€‘02â€‘13 10:08:59â€¯UTC  
  - ç®€è¿°ï¼šæŒ‡å‡º `griddepcontrol.wait` ç¼ºå°‘ `"memory"` clobberï¼Œå¯èƒ½å¯¼è‡´æœªå®šä¹‰è¡Œä¸ºï¼›å»ºè®®ç›´æ¥ä½¿ç”¨ `cudaTriggerProgrammaticLaunchCompletion` ä¸ `cudaGridDependencySynchronize`ã€‚  
  - å¤‡æ³¨ï¼šæ­£æ–‡è¢«æˆªæ–­ï¼Œè¯¦ç»†è®¨è®ºæœªå±•ç¤ºã€‚

- **`trtllm_batch_context_with_kv_cache` uses SM103 cubin but not `trtllm_batch_decode_with_kv_cache`**  
  - URL: <https://github.com/flashinfer-ai/flashinfer/issues/2556>  
  - ä½œè€…ï¼šb8zhongâ€ƒåˆ›å»ºæ—¶é—´ï¼š2026â€‘02â€‘13 02:47:28â€¯UTC  
  - ç®€è¿°ï¼šè¯¢é—® SM103 å˜ä½“åœ¨ decode é˜¶æ®µç¼ºå¤±çš„åŸå› ï¼Œå…³æ³¨ FP8 Q/K/V ä¸ BF16 è¾“å‡ºçš„å…¼å®¹æ€§ã€‚  
  - å¤‡æ³¨ï¼šæ­£æ–‡è¢«æˆªæ–­ï¼Œç»†èŠ‚æœªå®Œæ•´å‘ˆç°ã€‚

- **SM120 attention kernels exist but are blocked by wiring issues (fmha_v2, backend selector, MLA)**  
  - URL: <https://github.com/flashinfer-ai/flashinfer/issues/2555>  
  - ä½œè€…ï¼šblake-sncâ€ƒåˆ›å»ºæ—¶é—´ï¼š2026â€‘02â€‘13 01:44:39â€¯UTC  
  - ç®€è¿°ï¼šå‘ç° SM120 æ³¨æ„åŠ› kernel å·²å®ç°ä½†æœªåœ¨è¿è¡Œæ—¶è·¯å¾„ä¸­å¯ç”¨ï¼Œåˆ—å‡ºä¸‰å¤„ç¯å¢ƒå˜é‡æˆ–æ¡ä»¶åˆ¤æ–­å¯¼è‡´çš„å±è”½ã€‚  
  - å¤‡æ³¨ï¼šæ­£æ–‡è¢«æˆªæ–­ï¼Œå…·ä½“ä»£ç ä½ç½®ä¸å»ºè®®ä¿®å¤æ–¹æ¡ˆä»…éƒ¨åˆ†å¯è§ã€‚
### Dao-AILab/flash-attention
**æäº¤**  

- `ad2f470` â€“ [æŸ¥çœ‹æäº¤](https://github.com/Dao-AILab/flash-attention/commit/ad2f4702bdb26d15b630bb8b11a69ced671d7a74)  
  - **flash_attn/cute/cute_dsl_utils.py** â€“ åˆ é™¤ 24 è¡Œï¼ˆæ— æ–°å¢ï¼‰ï¼Œå®ç° `ParamsBase` çš„ DSL é‡æ„ã€‚  
  - **flash_attn/cute/flash_bwd.py** â€“ +2 / -1 è¡Œï¼Œå¾®è°ƒåå‘ä¼ æ’­å…¥å£ã€‚  
  - **flash_attn/cute/flash_bwd_postprocess.py** â€“ +1 / -1 è¡Œï¼Œåå¤„ç†ç»†èŠ‚è°ƒæ•´ã€‚  
  - **flash_attn/cute/flash_bwd_preprocess.py** â€“ +1 / -1 è¡Œï¼Œå‰å¤„ç†ç»†å¾®æ”¹åŠ¨ã€‚  
  - **flash_attn/cute/flash_bwd_sm100.py** â€“ +1 / -1 è¡Œï¼ŒSM100 ç‰ˆåå‘ä»£ç å°å¹…æ›´æ–°ã€‚  
  - **flash_attn/cute/flash_bwd_sm90.py** â€“ +2 / -1 è¡Œï¼ŒSM90 ç‰ˆåå‘ä»£ç å°å¹…æ›´æ–°ã€‚  
  - **flash_attn/cute/flash_fwd.py** â€“ +1 / -1 è¡Œï¼Œå‰å‘å…¥å£å¾®è°ƒã€‚  
  - **flash_attn/cute/flash_fwd_sm100.py** â€“ +1 / -1 è¡Œï¼ŒSM100 å‰å‘ä»£ç å¾®è°ƒã€‚  
  - **flash_attn/cute/paged_kv.py** â€“ +1 / -1 è¡Œï¼Œåˆ†é¡µ KV å®ç°ç»†èŠ‚ä¿®æ”¹ã€‚  
  - **flash_attn/cute/pyproject.toml** â€“ +1 è¡Œï¼Œæ„å»ºé…ç½®å¾®è°ƒã€‚  
  - **flash_attn/cute/softmax.py** â€“ +1 / -1 è¡Œï¼Œsoftmax å®ç°å°æ”¹ã€‚  
  - **flash_attn/cute/tile_scheduler.py** â€“ +4 / -26 è¡Œï¼Œè°ƒåº¦å™¨é€»è¾‘å¤§å¹…ç®€åŒ–ã€‚  

- `16d16d8` â€“ [æŸ¥çœ‹æäº¤](https://github.com/Dao-AILab/flash-attention/commit/16d16d8cba2cc967123e8c6efd3b67e90b9cf2d5)  
  - **flash_attn/cute/block_sparse_utils.py** â€“ +2 / -1 è¡Œï¼Œå—ç¨€ç–å·¥å…·å‡½æ•°å¾®è°ƒã€‚  
  - **flash_attn/cute/flash_bwd_postprocess.py** â€“ +4 / -2 è¡Œï¼Œåå¤„ç†é€»è¾‘ç®€åŒ–ã€‚  
  - **flash_attn/cute/flash_bwd_preprocess.py** â€“ +5 / -2 è¡Œï¼Œå‰å¤„ç†é€»è¾‘ç®€åŒ–ã€‚  
  - **flash_attn/cute/flash_bwd_sm90.py** â€“ +79 / -132 è¡Œï¼Œæ ¸å¿ƒåå‘å®ç°å¤§å¹…é‡æ„ï¼Œç®€åŒ– dK/dV R2S æ‹·è´ã€‚  
  - **flash_attn/cute/flash_fwd_sm100.py** â€“ +2 / -1 è¡Œï¼ŒSM100 å‰å‘ä»£ç å°å¹…æ›´æ–°ã€‚  
  - **flash_attn/cute/pyproject.toml** â€“ +1 / -1 è¡Œï¼Œæ„å»ºé…ç½®å¾®è°ƒã€‚  

> **è¯´æ˜**ï¼šæ‰€æœ‰æ–‡ä»¶çš„ `patch` å‡è¢«æˆªæ–­ï¼ˆ`patch_truncated: true`ï¼‰ï¼Œå› æ­¤æœªå±•ç¤ºå…·ä½“ä»£ç ç‰‡æ®µã€‚å¦‚éœ€æŸ¥çœ‹å®Œæ•´æ”¹åŠ¨ï¼Œè¯·è®¿é—®å¯¹åº”çš„æäº¤é“¾æ¥ã€‚

**Issues**  
- æœ¬æ¬¡æäº¤æœªå…³è”ä»»ä½• Issueã€‚

**ä»“åº“æ•´ä½“å˜åŠ¨æ¦‚è§ˆ**  
- **æ€»è®¡**ï¼š+109 è¡Œï¼Œ-198 è¡Œï¼Œä¿®æ”¹ 307 è¡Œã€‚  
- ä¸»è¦èšç„¦åœ¨ **flash_attn/cute** å­æ¨¡å—ï¼Œæ¶‰åŠå‰å‘/åå‘æ ¸å¿ƒå®ç°ã€è°ƒåº¦å™¨ã€DSL å‚æ•°åŸºç±»ä»¥åŠæ„å»ºé…ç½®ã€‚  
- å…³é”®æ”¹åŠ¨ï¼š  
  - å¼•å…¥ `ParamsBase` ç»Ÿä¸€ DSL å‚æ•°ç®¡ç†ã€‚  
  - å¤§å¹…ç®€åŒ– SM90 åå‘å®ç°ä¸­çš„ dK/dV R2S æ‹·è´é€»è¾‘ï¼Œæå‡å¯è¯»æ€§ä¸ç»´æŠ¤æ€§ã€‚  
  - `tile_scheduler.py` ä¸­çš„è°ƒåº¦ç­–ç•¥è¢«ç²¾ç®€ï¼Œåˆ é™¤å†—ä½™ä»£ç ã€‚  
  - å¤šä¸ªæ–‡ä»¶çš„å‰åå¤„ç†å…¥å£åšäº†ç»†å¾®çš„è¡Œæ•°è°ƒæ•´ï¼Œä»¥é…åˆä¸Šè¿°é‡æ„ã€‚  

è¿™äº›æ”¹åŠ¨æ•´ä½“æå‡äº†ä»£ç ç»“æ„çš„æ¸…æ™°åº¦ï¼Œå¹¶ä¸ºåç»­åœ¨ä¸åŒ GPU æ¶æ„ï¼ˆSM90/SM100ï¼‰ä¸Šçš„ä¼˜åŒ–å¥ å®šäº†æ›´å¥½çš„åŸºç¡€ã€‚
### sgl-project/sglang
- **[diffusion] feat: opt vae decode with `channels_last_3d`**  
  PR: <https://github.com/sgl-project/sglang/commit/4067d9487d371eb4e29d626424bd0e1edc8c3ec8>  
  - å…³é”®æ–‡ä»¶: `python/sglang/multimodal_gen/envs.py`ï¼ˆ+4 è¡Œï¼‰; `python/sglang/multimodal_gen/runtime/loader/component_loaders/vae_loader.py`ï¼ˆ+41 è¡Œï¼‰  
  - å®ç° VAE è§£ç çš„ `channels_last_3d` ä¼˜åŒ–ï¼Œæå‡å¤šæ¨¡æ€ç”Ÿæˆæ•ˆç‡ã€‚  

- **[kernel slimming] Move fast_hadamard_transform to jit_kernel**  
  PR: <https://github.com/sgl-project/sglang/commit/c29394e3c84a61e160da78d2edbf11939ec0ec07>  
  - æ–°å¢ `fast-hadamard-transform` ç›®å½•ï¼ŒåŒ…å« C++/CUDA å®ç°ã€ä»£ç ç”Ÿæˆè„šæœ¬ä»¥åŠ Python åŒ…è£… (`python/sglang/jit_kernel/hadamard.py`)ã€‚  
  - ç›¸å…³æ–‡ä»¶æ€»è®¡çº¦ 2â€¯241 è¡Œæ”¹åŠ¨ï¼ˆ+2â€¯170 è¡Œï¼‰ï¼Œå®ç° JIT ç¼–è¯‘çš„å¿«é€Ÿ Hadamard å˜æ¢ã€‚  

- **Enable SGLANG_ENABLE_SPEC_V2 for nightly speculative decoding tests**  
  PR: <https://github.com/sgl-project/sglang/commit/ae958692927509723ada6e33f368bdba148f1654>  
  - ä¿®æ”¹æµ‹è¯•å·¥å…· (`accuracy_test_runner.py`, `nightly_utils.py`, `performance_test_runner.py`, `test_utils.py`) ä»¥åŠå¤šæ¨¡å‹æ³¨å†Œè„šæœ¬ï¼Œå¼€å¯ Spec V2 é€‰é¡¹ã€‚  

- **feat: Support `mrope_section` with `rope_type: "yarn"`**  
  PR: <https://github.com/sgl-project/sglang/commit/92cdd398cd599b65c42dfa274353e70e0ce668c4>  
  - `python/sglang/srt/layers/rotary_embedding.py`ï¼ˆ+101 è¡Œï¼Œ-10 è¡Œï¼‰åŠ å…¥å¯¹ MROPE/YARN çš„å…¼å®¹ã€‚  
  - æ›´æ–°å¯¹åº”æµ‹è¯• `test/registered/rotary/test_mrope.py`ï¼ˆ+27 è¡Œï¼Œ-2 è¡Œï¼‰ã€‚  

- **Add ci test for ring model**  
  PR: <https://github.com/sgl-project/sglang/commit/f51e9d9ca1a6966c88e93974b2749893ad1c8a39>  
  - æ–°å¢ç¯å½¢æ¨¡å‹æµ‹è¯• `test/registered/8-gpu-models/test_ring_2_5_1t.py`ï¼ˆ+53 è¡Œï¼‰ã€‚  
  - è°ƒæ•´ `accuracy_test_runner.py` ä»¥æ”¯æŒæ–°æµ‹è¯•ã€‚  

- **Fix dsv32 encode_messages**  
  PR: <https://github.com/sgl-project/sglang/commit/c8aa2a65341aaa4a1b23fa7ee734ace4278e37c8>  
  - `python/sglang/srt/entrypoints/openai/serving_chat.py`ï¼ˆ+14 è¡Œï¼‰ä¿®å¤ç¼–ç é”™è¯¯ã€‚  
  - `python/sglang/srt/parser/jinja_template_utils.py`ï¼ˆ+16 è¡Œï¼Œ-5 è¡Œï¼‰åŒæ­¥æ›´æ–°ã€‚  

- **Kernel: optimize decoding metadata in NSA multi-spec backend with fused kernels**  
  PR: <https://github.com/sgl-project/sglang/commit/34132d6da50e0867426962d3681b62a03b5624b9>  
  - æ–°å¢èåˆå…ƒæ•°æ®æ‹·è´å®ç°ï¼š`fused_metadata_copy.cuh`ï¼ˆ+722 è¡Œï¼‰å’Œ `fused_metadata_copy.py`ï¼ˆ+316 è¡Œï¼‰ã€‚  
  - å¯¹ NSA åç«¯ç›¸å…³æ–‡ä»¶è¿›è¡Œå¤§é‡æ”¹åŠ¨ï¼ˆæ€»è®¡çº¦ 2â€¯878 è¡Œï¼‰ï¼Œæå‡å¤šè§„æ ¼æ¨ç†çš„å…ƒæ•°æ®å¤„ç†æ•ˆç‡ã€‚  

- **[AMD] Fix sgl-model-gateway Build Errors in ROCm Docker Release**  
  PR: <https://github.com/sgl-project/sglang/commit/38473f8ee06ac5b6f2074d8bc1f84f88ee7a9da0>  
  - è°ƒæ•´ CI å·¥ä½œæµæ–‡ä»¶å’Œ `sgl-model-gateway/Cargo.toml`ï¼ˆå„ +1 / -1 è¡Œï¼‰ï¼Œè§£å†³ ROCm Docker æ„å»ºé—®é¢˜ã€‚  

- **[DLLM] Update CODEOWNERS for diffusion LLM**  
  PR: <https://github.com/sgl-project/sglang/commit/e2eb5bf28d5f06b1e2ee857531cf52648e62b187>  
  - åœ¨ `.github/CODEOWNERS` ä¸­æ–°å¢ä¸€è¡Œæ‰€æœ‰è€…å£°æ˜ã€‚  

- **[VLM][LLM] Optimize fused_moe triton kernel tma**  
  PR: <https://github.com/sgl-project/sglang/commit/fa0ef6e4f7f7c8937e498d8db6fa8599405d2a18>  
  - `fused_moe_triton_kernels.py`ï¼ˆ+67 è¡Œï¼Œ-8 è¡Œï¼‰åŠ å…¥ TMA ä¼˜åŒ–ï¼Œæå‡ MoE æ¨ç†ååã€‚  

- **Fix/partial gen from waiting queue miss metadata**  
  PR: <https://github.com/sgl-project/sglang/commit/f6c18c3a85a9f9e7b388e849bedced97896522f1>  
  - `tokenizer_manager.py`ï¼ˆ+7 è¡Œï¼Œ-1 è¡Œï¼‰ä¿®å¤ç­‰å¾…é˜Ÿåˆ—å…ƒæ•°æ®ç¼ºå¤±å¯¼è‡´çš„éƒ¨åˆ†ç”Ÿæˆé”™è¯¯ã€‚  
  - æ›´æ–° `test/registered/scheduler/test_abort.py`ï¼ˆ+33 è¡Œï¼Œ-12 è¡Œï¼‰å¯¹åº”æµ‹è¯•ã€‚  

- **[diffusion][MUSA] fix: MUSA platform breakage**  
  PR: <https://github.com/sgl-project/sglang/commit/45a4697d452d294c591c38742491cc478d31c80a>  
  - `python/sglang/multimodal_gen/runtime/platforms/musa.py`ï¼ˆ+8 è¡Œï¼‰ä¿®å¤ MUSA å…¼å®¹æ€§ã€‚  

- **Fix CI concurrency collision between scheduled runs and fork PRs**  
  PR: <https://github.com/sgl-project/sglang/commit/8ef3e3d56be5461de223297dd186bb5fe1b9d11f>  
  - `.github/workflows/pr-test.yml`ï¼ˆ+4 è¡Œï¼Œ-3 è¡Œï¼‰è°ƒæ•´å¹¶å‘ç­–ç•¥ã€‚  

- **Handle abort for retracted requests in disagg decode prealloc queue**  
  PR: <https://github.com/sgl-project/sglang/commit/066b0b70d9803fa3668453bb6aa97e86c7bc330c>  
  - `scheduler.py`ï¼ˆ+14 è¡Œï¼‰åŠ å…¥å¯¹å·²æ’¤å›è¯·æ±‚çš„å®‰å…¨ä¸­æ­¢å¤„ç†ã€‚  

- **[Env] centralize hicache vars in environ.py**  
  PR: <https://github.com/sgl-project/sglang/commit/bd39de7d5e468197ddf2907cd372ebb1e853cb19>  
  - `environ.py`ï¼ˆ+2 è¡Œï¼‰ç»Ÿä¸€ HICACHE ç¯å¢ƒå˜é‡ã€‚  
  - ç›¸å…³ç¼“å­˜å®ç°æ–‡ä»¶ç•¥æœ‰å¾®è°ƒï¼ˆ+2 è¡Œï¼Œ-1 è¡Œï¼‰ã€‚  

- **Add timeout abort kits for normal / eagle**  
  PR: <https://github.com/sgl-project/sglang/commit/dcea74d63fed017e1e8e76091702a02f1f3de761>  
  - æ–°å¢ `abort_timeout_kit.py`ï¼ˆ+144 è¡Œï¼‰ç”¨äºè¶…æ—¶ä¸­æ­¢æµ‹è¯•ã€‚  
  - è°ƒæ•´ `test/registered/scheduler/test_abort.py`ï¼ˆ-63 è¡Œï¼Œ+3 è¡Œï¼‰ä»¥åŠ Eagle ç›¸å…³æµ‹è¯•ã€‚  

- **[PD-Disagg] Fix double free when prebuilt batch is aborted**  
  PR: <https://github.com/sgl-project/sglang/commit/4474fb98b435cf6ebcd6b1e61d9f9463834609b6>  
  - `decode_schedule_batch_mixin.py`ï¼ˆ+6 è¡Œï¼Œ-5 è¡Œï¼‰ä¿®å¤åŒé‡é‡Šæ”¾é”™è¯¯ã€‚  

- **feat: add SGLANG_DISTRIBUTED_INIT_METHOD_OVERRIDE env var**  
  PR: <https://github.com/sgl-project/sglang/commit/ab0fb248fdd170b527fde68b61a6f36dbf81f688>  
  - `environ.py`ï¼ˆ+3 è¡Œï¼‰æ–°å¢ç¯å¢ƒå˜é‡è¯´æ˜ã€‚  
  - `model_runner.py`ï¼ˆ+8 è¡Œï¼Œ-1 è¡Œï¼‰å®ç°è¦†ç›–åˆå§‹åŒ–æ–¹æ³•ã€‚  

- **[Perf] refactor piecewise cuda graph support of Qwen3-Next**  
  PR: <https://github.com/sgl-project/sglang/commit/8be18c655d0ff1c2e673395102c97c9d5cbd7d1c>  
  - å…³é”®æ”¹åŠ¨æ¶‰åŠ `layernorm_gated.py`ï¼ˆ+13 è¡Œï¼Œ-1 è¡Œï¼‰å’Œ `radix_linear_attention.py`ï¼ˆ+61 è¡Œï¼Œ-7 è¡Œï¼‰ï¼Œä¼˜åŒ– Qwen3â€‘Next çš„ CUDA å›¾æ”¯æŒã€‚  

- **Update performance dashboard for nightly tests**  
  PR: <https://github.com/sgl-project/sglang/commit/3a1c388b43a371e282a82a8c56b88cd8541b4092>  
  - å‰ç«¯æ–‡ä»¶ `app.js`, `index.html` ä»¥åŠåç«¯ `server.py` å¤§å¹…æ›´æ–°ï¼ˆå…± +1â€¯160 è¡Œï¼‰ï¼Œæå‡ä»ªè¡¨ç›˜å¯è§†åŒ–ä¸æ•°æ®å±•ç¤ºã€‚  

- **[CI] feat: add early exit to wait_for_server when process dies**  
  PR: <https://github.com/sgl-project/sglang/commit/3299c4f9c1735c17305175ce1685d0ddbc2e7f91>  
  - å¤šä¸ª Notebook ç¤ºä¾‹æ–‡ä»¶å¾®è°ƒï¼ˆ+/-5 è¡Œï¼‰ï¼ŒåŠ å…¥å¯¹æœåŠ¡å™¨è¿›ç¨‹å¼‚å¸¸çš„æ—©æœŸé€€å‡ºæ£€æµ‹ã€‚  

- **[CI] Revive 8â€‘GPU trace upload in nightly test workflow**  
  PR: <https://github.com/sgl-project/sglang/commit/eccf875d4904a37dc4f6c72aafc254a0e1f1f452>  
  - `nightly-test-nvidia.yml` å¢åŠ  38 è¡Œæ­¥éª¤ï¼›`publish_traces.py` è°ƒæ•´ 32 è¡Œæ–°å¢ã€17 è¡Œåˆ é™¤ï¼Œç”¨äºä¸Šä¼ è¿è¡Œè½¨è¿¹ã€‚  

- **[FlashInfer] Bump FlashInfer version from 0.6.2 to 0.6.3**  
  PR: <https://github.com/sgl-project/sglang/commit/1be41e9036e19fc620d39e8f7b22572fa36751a9>  
  - æ›´æ–° CIã€Dockerã€`pyproject.toml` ç­‰é…ç½®æ–‡ä»¶ï¼ˆæ¯å¤„ +1/-1 è¡Œï¼‰ï¼ŒåŒæ­¥ FlashInfer ä¾èµ–ã€‚  

- **Update notified user in post_ci_failures_to_slack.py**  
  PR: <https://github.com/sgl-project/sglang/commit/710d873ba6f3fc47e3af3ef7f6c219f396b98238>  
  - å¾®è°ƒ Slack é€šçŸ¥è„šæœ¬ï¼ˆ+1 è¡Œï¼Œ-1 è¡Œï¼‰ï¼Œæ”¹è¿›å¤±è´¥æŠ¥å‘Šçš„æ¥æ”¶äººã€‚  

- **fix double-free kv cache for requests that have already finished**  
  PR: <https://github.com/sgl-project/sglang/commit/191d354f538f81ea383e536f3b4421ed441374d8>  
  - `schedule_policy.py`ï¼ˆ+9 è¡Œï¼Œ-1 è¡Œï¼‰é˜²æ­¢å·²å®Œæˆè¯·æ±‚çš„ KV ç¼“å­˜è¢«é‡å¤é‡Šæ”¾ã€‚  

- **[Auto Sync] Update loader.py, weight_utils.py (20260213)**  
  PR: <https://github.com/sgl-project/sglang/commit/008ea46af13e496240680932ad52c21b9e0589fb>  
  - `loader.py`ï¼ˆ+/-2 è¡Œï¼‰å’Œ `weight_utils.py`ï¼ˆ+66 è¡Œï¼Œ-45 è¡Œï¼‰åŒæ­¥æœ€æ–°æ¨¡å‹åŠ è½½é€»è¾‘ã€‚  

- **[bugfix] fix mamba slot leak when scheduling fails with radix cache**  
  PR: <https://github.com/sgl-project/sglang/commit/4c6afbeeaa8d80d59ace8b2707474a446e8a3596>  
  - `scheduler.py`ï¼ˆ+21 è¡Œï¼‰ä¿®å¤ Radix ç¼“å­˜è°ƒåº¦å¤±è´¥æ—¶çš„æ§½ä½æ³„æ¼ã€‚  

- **refactor context parallel state**  
  PR: <https://github.com/sgl-project/sglang/commit/8b4c364960e6cccecd72edcdf47f54d06f3623c4>  
  - å¤§è§„æ¨¡æ”¹åŠ¨ `parallel_state.py`ï¼ˆ+219 è¡Œï¼Œ-16 è¡Œï¼‰ä»¥åŠå¤šå¤„è°ƒåº¦ã€é€šä¿¡ã€æ³¨æ„åŠ›å®ç°æ–‡ä»¶ï¼Œæ•´ä½“æå‡å¹¶è¡Œä¸Šä¸‹æ–‡ç®¡ç†ã€‚  

- **[Kernel Slimming] Migrate GPTQâ€‘Marlin repack kernel to JIT**  
  PR: <https://github.com/sgl-project/sglang/commit/0012d6a4ebff8d87c5f9f99c295f85d3b69bf1e7>  
  - æ–°å¢ `gptq_marlin_repack.py`ã€`gptq_marlin_repack.cuh`ã€åŸºå‡†è„šæœ¬åŠæµ‹è¯•ï¼ˆå…± +610 è¡Œï¼‰ï¼Œå®ç° JIT ç¼–è¯‘çš„ GPTQâ€‘Marlin é‡æ‰“åŒ…ã€‚  

- **[diffusion] chore: use batched P2P ops in VAE parallel decoding**  
  PR: <https://github.com/sgl-project/sglang/commit/37273408ebcde2d16890dc93d4e613825fc22f95>  
  - `wan_dist_utils.py`ï¼ˆ+10 è¡Œï¼Œ-7 è¡Œï¼‰æ”¹ä¸ºæ‰¹é‡ P2P æ“ä½œï¼Œæå‡ VAE å¹¶è¡Œè§£ç æ•ˆç‡ã€‚  

- **[diffusion] fix typo**  
  PR: <https://github.com/sgl-project/sglang/commit/acc940d30217ea6ff67a278a2d894d136466aca2>  
  - `scale_residual_norm_scale_shift.py`ï¼ˆ+1 è¡Œï¼Œ-1 è¡Œï¼‰çº æ­£æ‹¼å†™é”™è¯¯ã€‚  

- **[CI] Move `test_load_lora_from_tensor` test to H100**  
  PR: <https://github.com/sgl-project/sglang/commit/9a32f8ccb97697dd0d6a87ba323fcd2169e3aed9>  
  - `test_lora_load_from_tensor.py`ï¼ˆ+1 è¡Œï¼Œ-1 è¡Œï¼‰è¿ç§»è‡³ H100 æœºå™¨ä¸Šæ‰§è¡Œã€‚  

### æœ¬å‘¨å…³é”® Issue æ±‡æ€»

- **[#18819] KeyError: 'ministral3'**  
  - æè¿°ï¼šåœ¨æœ€æ–° SGLang ä¸ Transformers 0.5.1 ç»„åˆä¸‹ï¼ŒåŠ è½½ Ministralâ€‘3 æ¨¡å‹æ—¶æŠ¥é”™ `KeyError: 'ministral3'`ã€‚  
  - å½±å“ï¼šæ¨¡å‹æ— æ³•å¯åŠ¨ï¼Œéœ€å…¼å®¹æœ€æ–° Transformersã€‚  

- **[#18818] FP8 KV cache scales from JSON not passed to `set_kv_buffer`**  
  - æè¿°ï¼šä½¿ç”¨ `--quantization-param-path` æä¾›çš„ KV ç¼“å­˜å°ºåº¦åœ¨å†™å…¥æ—¶å§‹ç»ˆä¸º `None`ï¼Œå¯¼è‡´ FP8 é‡åŒ–å¤±æ•ˆã€‚  

- **[#18816] Feature: è‡ªåŠ¨æ£€æµ‹æ–‡æ¡£é“¾æ¥å¤±æ•ˆ**  
  - æè¿°ï¼šæè®®åœ¨ CI ä¸­åŠ å…¥æ–‡æ¡£é“¾æ¥æ£€æŸ¥ï¼Œä»¥é˜²æ­¢å› è·¯å¾„é‡æ„å¯¼è‡´çš„æ­»é“¾ã€‚  

- **[#18812] RotaryEmbedding fallback requires CUDA_HOME on HIP**  
  - æè¿°ï¼šåœ¨ ROCm ç¯å¢ƒä¸‹ï¼ŒRotaryEmbedding å›é€€è·¯å¾„ä¾èµ– `CUDA_HOME`ï¼Œå¯¼è‡´ CI å¤±è´¥ã€‚  

- **[#18799] Deepseek v3.2 prefill workers crash with PD disaggregation**  
  - æè¿°ï¼šåœ¨ä»…ä½¿ç”¨ TP=16ï¼ˆæœªå¯ç”¨ DPâ€‘attentionï¼‰æ—¶ï¼ŒDeepseekâ€‘v3.2 çš„é¢„å¡«å……å·¥ä½œè¿›ç¨‹åœ¨ DeepGEMM warmup é˜¶æ®µå´©æºƒã€‚  

- **[#18798] Feature: æ”¯æŒ Kimi K2.5 è§†é¢‘è¾“å…¥**  
  - æè¿°ï¼šè¯·æ±‚ä¸º Kimi K2.5 æ·»åŠ è§†é¢‘è¾“å…¥æ¨ç†èƒ½åŠ›ã€‚  

- **[#18794] Feature: Qwen3â€‘Coderâ€‘480Bâ€‘A35B B200/GB200 æ”¯æŒè¿½è¸ª**  
  - æè¿°ï¼šå»ºç«‹åŠŸèƒ½æ€§ã€å‡†ç¡®æ€§ä»¥åŠä¸åŒå¹¶è¡Œé…ç½®ï¼ˆDEPã€TPï¼‰çš„æ”¯æŒçŠ¶æ€è¿½è¸ªè¡¨ã€‚  

- **[#18792] Bug: Qwen3 MoE ä¸¤æ‰¹æ¬¡é‡å  warmup å¤±è´¥**  
  - æè¿°ï¼šåœ¨ Qwen3â€‘MoE ä½¿ç”¨ä¸¤æ‰¹æ¬¡é‡å æ—¶ï¼Œwarmup å‰å‘é˜¶æ®µè§¦å‘æ³¨æ„åŠ›å±‚é”™è¯¯ã€‚  

> æœ¬æœŸç®€æŠ¥èšç„¦äºå¤§å¹…åº¦å†…æ ¸é‡æ„ã€åˆ†å¸ƒå¼è°ƒåº¦æ”¹è¿›ä»¥åŠå¤šæ¨¡æ€/é‡åŒ–ç‰¹æ€§å¢å¼ºï¼Œæ•´ä½“ä»£ç æ”¹åŠ¨çº¦ **13â€¯k è¡Œ**ï¼Œå…¶ä¸­æ–°å¢æ–‡ä»¶è¶…è¿‡ **30** ä¸ªï¼Œæ˜¾è‘—æå‡äº† SGLang åœ¨é«˜æ€§èƒ½æ¨ç†ã€è·¨å¹³å°å…¼å®¹ä»¥åŠæµ‹è¯•å¯é æ€§æ–¹é¢çš„èƒ½åŠ›ã€‚
### vllm-project/vllm
## æäº¤

- **[d5fe3f7]** Enable mamba prefix cache â€œalignâ€ mode with async scheduling  
  PR: https://github.com/vllm-project/vllm/commit/d5fe3f702c2f4392515cdfde6fd0442271c74dda  
  å…³é”®æ–‡ä»¶: `vllm/v1/core/single_type_kv_cache_manager.py`ï¼ˆ+13 è¡Œï¼‰ï¼Œ`vllm/v1/worker/mamba_utils.py`ï¼ˆ+6 / -1 è¡Œï¼‰ï¼Œ`tests/v1/e2e/test_mamba_prefix_cache.py`ï¼ˆ+58 / -19 è¡Œï¼‰ã€‚åœ¨å¼‚æ­¥è°ƒåº¦ä¸‹ä¸º Mamba å‰ç¼€ç¼“å­˜åŠ å…¥å¯¹é½æ¨¡å¼ã€‚*patch å·²æˆªæ–­*ã€‚

- **[73391a1]** Move InputPreprocessor into Renderer (1/2)  
  PR: https://github.com/vllm-project/vllm/commit/73391a1baa459e78be1ade466517c7206ab7dd7c  
  å…³é”®æ–‡ä»¶: `vllm/inputs/preprocess.py`ï¼ˆ-91 / +23 è¡Œï¼‰ï¼Œ`vllm/entrypoints/llm.py`ï¼ˆ+16 / -29 è¡Œï¼‰ï¼Œ`vllm/engine/protocol.py`ï¼ˆ+14 / -1 è¡Œï¼‰ã€‚å°†è¾“å…¥é¢„å¤„ç†é€»è¾‘è¿å…¥æ¸²æŸ“å±‚ï¼Œç»Ÿä¸€å…¥å£ã€‚*patch å·²æˆªæ–­*ã€‚

- **[b3c1422]** Guard sparse MLA backend imports for ROCm compatibility in tests  
  PR: https://github.com/vllm-project/vllm/commit/b3c14229b032a8bbf93d450a52c9a404ddaea429  
  å…³é”®æ–‡ä»¶: `tests/v1/attention/test_sparse_mla_backends.py`ï¼ˆ+11 è¡Œï¼‰ã€‚åœ¨ ROCm ç¯å¢ƒä¸‹é¿å…ä¸å…¼å®¹çš„ç¨€ç– MLA å¯¼å…¥ã€‚*patch å·²æˆªæ–­*ã€‚

- **[2f18663]** Fix Qwen3.5 config loading  
  PR: https://github.com/vllm-project/vllm/commit/2f186635cbcb38fd85e718a5b7ff9ec698cbb4f8  
  å…³é”®æ–‡ä»¶: `vllm/transformers_utils/configs/qwen3_5.py`ï¼ˆ+9 / -5 è¡Œï¼‰ï¼Œ`vllm/transformers_utils/configs/qwen3_5_moe.py`ï¼ˆ+9 / -5 è¡Œï¼‰ã€‚ä¿®å¤ Qwen3.5 é…ç½®è§£æé”™è¯¯ã€‚*patch å·²æˆªæ–­*ã€‚

- **[342a7cd]** Update tests and examples for Prithvi/Terratorch models  
  PR: https://github.com/vllm-project/vllm/commit/342a7cda2d212205c4874f82a59559400bbec311  
  å…³é”®æ–‡ä»¶: `examples/pooling/plugin/prithvi_geospatial_mae_online.py`ï¼ˆ+2 / -4 è¡Œï¼‰ï¼Œ`tests/plugins_tests/test_io_processor_plugins.py`ï¼ˆ+59 / -22 è¡Œï¼‰ç­‰ã€‚åŒæ­¥ Prithvi/Terratorch ç¤ºä¾‹ä¸æµ‹è¯•ã€‚*patch å·²æˆªæ–­*ã€‚

- **[d1ea65d]** Add COLQwen3 code & Inference  
  PR: https://github.com/vllm-project/vllm/commit/d1ea65d0a1c606ae041b73fd45ccd33980ca08e7  
  å…³é”®æ–‡ä»¶: `vllm/model_executor/models/colqwen3.py`ï¼ˆ+306 è¡Œï¼‰ï¼Œ`examples/pooling/score/colqwen3_rerank_online.py`ï¼ˆ+130 è¡Œï¼‰ï¼Œ`tests/models/multimodal/pooling/test_colqwen3.py`ï¼ˆ+156 è¡Œï¼‰ã€‚æ–°å¢ COLQwen3 å¤šæ¨¡æ€æ¨¡å‹åŠç¤ºä¾‹ã€‚*patch å·²æˆªæ–­*ã€‚

- **[de42abb]** Heavy refactoring of Voxtral multimodal audio model tests  
  PR: https://github.com/vllm-project/vllm/commit/de42abb366032519bca073e057331ead6270e09f  
  å…³é”®æ–‡ä»¶: `tests/models/multimodal/generation/test_voxtral.py`ï¼ˆ+138 / -43 è¡Œï¼‰ï¼Œ`vllm/model_executor/models/voxtral.py`ï¼ˆ+28 è¡Œï¼‰ã€‚é‡æ„ Voxtral æµ‹è¯•ï¼Œæå‡å¯ç»´æŠ¤æ€§ã€‚*patch å·²æˆªæ–­*ã€‚

- **[60ca798]** Add explicit validation error for tool calls  
  PR: https://github.com/vllm-project/vllm/commit/60ca7981bce1bd6e2155df1a58bc9f916f7c4093  
  å…³é”®æ–‡ä»¶: `vllm/tokenizers/mistral.py`ï¼ˆ+10 / -7 è¡Œï¼‰ã€‚åœ¨å·¥å…·è°ƒç”¨ä¸­åŠ å…¥æ˜ç¡®çš„éªŒè¯é”™è¯¯ä¿¡æ¯ã€‚*patch å·²æˆªæ–­*ã€‚

- **[0ef5b91]** Use `__annotations__` instead of `get_type_hints()` for dynamic kwargs detection  
  PR: https://github.com/vllm-project/vllm/commit/0ef5b9147bb1f37c9a90ab2a3ee2a85cf9e84e30  
  å…³é”®æ–‡ä»¶: `vllm/transformers_utils/processor.py`ï¼ˆ+11 / -1 è¡Œï¼‰ã€‚æå‡åŠ¨æ€å‚æ•°æ£€æµ‹æ€§èƒ½ã€‚*patch å·²æˆªæ–­*ã€‚

- **[ed24265]** Make `get_modality_with_max_tokens` deterministic  
  PR: https://github.com/vllm-project/vllm/commit/ed242652d7f9cb4222e8840311b5229295b5d266  
  å…³é”®æ–‡ä»¶: `vllm/multimodal/encoder_budget.py`ï¼ˆ+1 / -1 è¡Œï¼‰ã€‚ç¡®ä¿å¤šæ¨¡æ€ç¼–ç é¢„ç®—çš„ç¡®å®šæ€§ã€‚*patch å·²æˆªæ–­*ã€‚

- **[b37b679]** Support Selective CPU Weight Offloading  
  PR: https://github.com/vllm-project/vllm/commit/b37b679770aade27f33d20c93bf467c6a7fba65d  
  å…³é”®æ–‡ä»¶: `vllm/config/cache.py`ï¼ˆ+11 è¡Œï¼‰ï¼Œ`vllm/model_executor/models/utils.py`ï¼ˆ+23 / -1 è¡Œï¼‰ã€‚å®ç°å¯¹ CPU æƒé‡çš„é€‰æ‹©æ€§å¸è½½ã€‚*patch å·²æˆªæ–­*ã€‚

- **[a0638d0]** Fix ROCm UVA CPU weight offloading broken by #32993  
  PR: https://github.com/vllm-project/vllm/commit/a0638d052db74ba28eada4768b9bbf98720b44a4  
  å…³é”®æ–‡ä»¶: `vllm/utils/torch_utils.py`ï¼ˆ+1 / -1 è¡Œï¼‰ã€‚ä¿®å¤ ROCm ç¯å¢ƒä¸‹çš„ CPU æƒé‡å¸è½½é—®é¢˜ã€‚*patch å·²æˆªæ–­*ã€‚

- **[c027541]** Enable spec decoding in mamba cache align mode  
  PR: https://github.com/vllm-project/vllm/commit/c027541eaf05c6ca1e9a544804afe28caef671fc  
  å…³é”®æ–‡ä»¶: `vllm/model_executor/models/config.py`ï¼ˆ-4 è¡Œï¼‰ï¼Œ`tests/v1/e2e/test_mamba_prefix_cache.py`ï¼ˆ+10 / -4 è¡Œï¼‰ã€‚åœ¨å¯¹é½æ¨¡å¼ä¸‹åŠ å…¥ spec è§£ç æ”¯æŒã€‚*patch å·²æˆªæ–­*ã€‚

- **[fd267bc]** Fix structured output in multiâ€‘turn gptâ€‘oss  
  PR: https://github.com/vllm-project/vllm/commit/fd267bc7b7cd3d001ac5a893eacb9e56ff256822  
  å…³é”®æ–‡ä»¶: `vllm/reasoning/gptoss_reasoning_parser.py`ï¼ˆ+9 è¡Œï¼‰ï¼Œ`tests/reasoning/test_gptoss_reasoning_parser.py`ï¼ˆ+18 / -1 è¡Œï¼‰ã€‚è§£å†³å¤šè½® GPTâ€‘OSS ç»“æ„åŒ–è¾“å‡ºé”™è¯¯ã€‚*patch å·²æˆªæ–­*ã€‚

- **[bfaa559]** Revert â€œFix fused MoE IMA (sans chunking) by using int64 for stridesâ€  
  PR: https://github.com/vllm-project/vllm/commit/bfaa5593050ec9bf60e2361b3b9dc575efeee83f  
  å…³é”®æ–‡ä»¶: `vllm/model_executor/layers/fused_moe/fused_moe.py`ï¼ˆ+27 / -27 è¡Œï¼‰ã€‚æ¢å¤ä¹‹å‰çš„ stride ç±»å‹å®ç°ã€‚*patch å·²æˆªæ–­*ã€‚

- **[87789c8]** `--enforce-eager` should turn off compile and cudagraphs only  
  PR: https://github.com/vllm-project/vllm/commit/87789c836422cf3b666ddf3eca9ede8e03f735ee  
  å…³é”®æ–‡ä»¶: `vllm/config/vllm.py`ï¼ˆ+7 / -7 è¡Œï¼‰ã€‚ç»†åŒ– `--enforce-eager` è¡Œä¸ºï¼Œä»…ç¦ç”¨ç¼–è¯‘ä¸ cudagraphã€‚*patch å·²æˆªæ–­*ã€‚

- **[bcd65c1]** Replace `c10::optional` with `std::optional` in topk kernel  
  PR: https://github.com/vllm-project/vllm/commit/bcd65c1f6a25ab76be325fbc0766eb074519a4fc  
  å…³é”®æ–‡ä»¶: `csrc/topk.cu`ï¼ˆ+1 / -1 è¡Œï¼‰ã€‚æå‡ CUDA kernel çš„å¯ç§»æ¤æ€§ã€‚*patch å·²æˆªæ–­*ã€‚

- **[59d5306]** Support CPU Offloading without PyTorch pinned memory (double allocation)  
  PR: https://github.com/vllm-project/vllm/commit/59d53066d8daed5c2e39e3bce38ac308bc80a9ae  
  å…³é”®æ–‡ä»¶: `vllm/model_executor/model_loader/utils.py`ï¼ˆ+23 / -17 è¡Œï¼‰ï¼Œ`tests/basic_correctness/test_cpu_offload.py`ï¼ˆ+21 / -2 è¡Œï¼‰ã€‚å®ç°ä¸ä¾èµ– pinned memory çš„ CPU å¸è½½è·¯å¾„ã€‚*patch å·²æˆªæ–­*ã€‚

- **[4a9952e]** Add `quant_config` in ViT of Kimiâ€‘K2.5  
  PR: https://github.com/vllm-project/vllm/commit/4a9952ec1b15453053f4ec443d2d81505d344075  
  å…³é”®æ–‡ä»¶: `vllm/model_executor/models/kimi_k25_vit.py`ï¼ˆ+15 è¡Œï¼‰ã€‚ä¸º ViT æ·»åŠ é‡åŒ–é…ç½®ã€‚*patch å·²æˆªæ–­*ã€‚

- **[1dae7b7]** Exclude `language_model_only` key from MM AOT compile hash but include in model one  
  PR: https://github.com/vllm-project/vllm/commit/1dae7b7843062f3468485653779d43ef96c7245c  
  å…³é”®æ–‡ä»¶: `vllm/config/model.py`ï¼ˆ+6 è¡Œï¼‰ï¼Œ`tests/config/test_multimodal_config.py`ï¼ˆ+18 è¡Œï¼‰ã€‚ç»†åŒ–å¤šæ¨¡æ€ AOT ç¼–è¯‘å“ˆå¸Œç­–ç•¥ã€‚*patch å·²æˆªæ–­*ã€‚

- **[5885e33]** Port Qwen3.5 Configs  
  PR: https://github.com/vllm-project/vllm/commit/5885e330efea5212733375f3573990de791d5042  
  å…³é”®æ–‡ä»¶: `vllm/transformers_utils/configs/qwen3_5.py`ï¼ˆ+189 è¡Œï¼‰ï¼Œ`vllm/transformers_utils/configs/qwen3_5_moe.py`ï¼ˆ+201 è¡Œï¼‰ã€‚æ–°å¢ Qwen3.5 ç³»åˆ—é…ç½®ã€‚*patch å·²æˆªæ–­*ã€‚

- **[071d863]** Extend ColBERT support to nonâ€‘standard BERT backbones  
  PR: https://github.com/vllm-project/vllm/commit/071d863e208b40fa1bb986ad230e322b2bbbbcf5  
  å…³é”®æ–‡ä»¶: `vllm/model_executor/models/colbert.py`ï¼ˆ+335 / -68 è¡Œï¼‰ï¼Œ`tests/models/language/pooling/test_colbert.py`ï¼ˆ+242 / -73 è¡Œï¼‰ã€‚è®© ColBERT èƒ½å…¼å®¹è‡ªå®šä¹‰ BERTã€‚*patch å·²æˆªæ–­*ã€‚

- **[0916e79]** Use CPU tensors to build GDN metadata  
  PR: https://github.com/vllm-project/vllm/commit/0916e7960bddf565c33153d2cf753e799de105b7  
  å…³é”®æ–‡ä»¶: `vllm/v1/attention/backends/gdn_attn.py`ï¼ˆ+10 / -7 è¡Œï¼‰ã€‚æ”¹ä¸º CPU å¼ é‡ç”Ÿæˆ GDN å…ƒæ•°æ®ï¼Œæé«˜å…¼å®¹æ€§ã€‚*patch å·²æˆªæ–­*ã€‚

- **[3d2a026]** Pipeline Parallel Async send/recv, 2.9% E2E throughput improvement  
  PR: https://github.com/vllm-project/vllm/commit/3d2a026fd0317204752d7933408aff19aaa80cfd  
  å…³é”®æ–‡ä»¶: `vllm/distributed/parallel_state.py`ï¼ˆ+137 / -76 è¡Œï¼‰ï¼Œ`vllm/v1/worker/gpu_worker.py`ï¼ˆ+54 / -5 è¡Œï¼‰ã€‚å®ç°å¼‚æ­¥é€šä¿¡ï¼Œæå‡ç®¡çº¿å¹¶è¡Œååã€‚*patch å·²æˆªæ–­*ã€‚

- **[dddbff4]** Move pause and resume functions into engine  
  PR: https://github.com/vllm-project/vllm/commit/dddbff46242a9292085e2ae3309dc559f242cad6  
  å…³é”®æ–‡ä»¶: `vllm/v1/engine/core.py`ï¼ˆ+125 / -50 è¡Œï¼‰ï¼Œ`vllm/v1/engine/async_llm.py`ï¼ˆ-49 / +6 è¡Œï¼‰ï¼Œæ–°å¢ç¤ºä¾‹ `examples/online_serving/data_parallel_pause_resume.py`ï¼ˆ+135 è¡Œï¼‰ã€‚å°†æš‚åœ/æ¢å¤é€»è¾‘è¿å…¥å¼•æ“å±‚ã€‚*patch å·²æˆªæ–­*ã€‚

- **[47e9b63]** Clean up redundant code in KV connectors  
  PR: https://github.com/vllm-project/vllm/commit/47e9b63e1afeb074b0fa584e0169e27d517b4e7b  
  å…³é”®æ–‡ä»¶: `vllm/distributed/kv_transfer/kv_connector/v1/example_connector.py`ï¼ˆ-8 è¡Œï¼‰ï¼Œ`vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py`ï¼ˆ+1 / -1 è¡Œï¼‰ã€‚ç²¾ç®€ KV è¿æ¥å™¨å®ç°ã€‚*patch å·²æˆªæ–­*ã€‚

- **[934acdd]** fused_moe: add int4_w4a16 benchmark support and tuning config  
  PR: https://github.com/vllm-project/vllm/commit/934acddef9fa4eb1b6cc897d2e39db77385539c6  
  å…³é”®æ–‡ä»¶: `benchmarks/kernels/benchmark_moe.py`ï¼ˆ+122 / -8 è¡Œï¼‰ï¼Œæ–°å¢é…ç½® `fused_moe/configs/E=128,N=768,device_name=Radeon_8060S_Graphics,dtype=int4_w4a16.json`ï¼ˆ+63 è¡Œï¼‰ã€‚æ”¯æŒ int4_w4a16 åŸºå‡†ã€‚*patch å·²æˆªæ–­*ã€‚

- **[742d214]** Fix import path in moe test utils.py  
  PR: https://github.com/vllm-project/vllm/commit/742d214d6eeb1b0c92aabae36614be6a485fb94d  
  å…³é”®æ–‡ä»¶: `tests/kernels/moe/utils.py`ï¼ˆ+5 / -5 è¡Œï¼‰ã€‚çº æ­£æµ‹è¯•å·¥å…·çš„å¯¼å…¥è·¯å¾„ã€‚*patch å·²æˆªæ–­*ã€‚

- **[4137c5d]** Fix MambaManager.cache_blocks() crash on null blocks in align mode  
  PR: https://github.com/vllm-project/vllm/commit/4137c5dfa7c0de6c0ff74ad3774224b6b3280349  
  å…³é”®æ–‡ä»¶: `vllm/v1/core/single_type_kv_cache_manager.py`ï¼ˆ+2 è¡Œï¼‰ï¼Œ`tests/v1/core/test_prefix_caching.py`ï¼ˆ+46 è¡Œï¼‰ã€‚é˜²æ­¢ç©ºå—å¯¼è‡´çš„å´©æºƒ
### NVIDIA/cutile-python
**æäº¤**  
- **SHA**: `394d302`  
- **é“¾æ¥**: https://github.com/NVIDIA/cutile-python/commit/394d302ae6b31f988cb91d80b466c6a695405a43  
- **ä½œè€…**: Jay Gu  
- **æ—¥æœŸ**: 2026â€‘02â€‘11 21:53:31 UTC  
- **ä¿¡æ¯**: Create temp dir if not exist  

**æ–‡ä»¶å˜æ›´**  

| æ¨¡å—/æ–‡ä»¶ | çŠ¶æ€ | å¢åˆ è¡Œæ•° | è¯´æ˜ |
|---|---|---|---|
| `src/cuda/tile/_compile.py` | modified | +2 / -2 | ä»£ç ä¸­åŠ å…¥äº†åˆ›å»ºä¸´æ—¶ç›®å½•çš„é€»è¾‘ã€‚<br>**Diff**ï¼šæœªæä¾›ï¼ˆ`patch_truncated` ä¸º trueï¼‰ã€‚ |
| `src/cuda/tile/_context.py` | modified | +2 / 0 | åŒæ ·æ·»åŠ äº†ä¸´æ—¶ç›®å½•æ£€æŸ¥ã€‚<br>**Diff**ï¼šæœªæä¾›ï¼ˆ`patch_truncated` ä¸º trueï¼‰ã€‚ |

**æ€»ä½“æ¦‚è¿°**  
æœ¬æ¬¡æäº¤åœ¨ CUDA Tile æ¨¡å—çš„ç¼–è¯‘ä¸ä¸Šä¸‹æ–‡ç®¡ç†ä»£ç ä¸­ï¼ŒåŠ å…¥äº†â€œè‹¥ä¸´æ—¶ç›®å½•ä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»ºâ€çš„å®¹é”™å¤„ç†ï¼Œæå‡äº†è¿è¡Œæ—¶çš„é²æ£’æ€§ã€‚ç”±äºè¡¥ä¸å†…å®¹è¢«æˆªæ–­ï¼Œå…·ä½“å®ç°ç»†èŠ‚æœªå±•ç¤ºã€‚

## æ€»ç»“
ğŸš€ **è¶‹åŠ¿**ï¼šAI åŸºç¡€è®¾æ–½æ­£å‘ **DSLâ€‘é©±åŠ¨çš„é«˜æ•ˆç¼–è¯‘ã€è·¨ç¡¬ä»¶ç»Ÿä¸€åŠ é€Ÿã€ä»¥åŠå¤šæ¨¡æ€å¤§æ¨¡å‹çš„å…¨é“¾è·¯ä¼˜åŒ–** è¿ˆè¿›ã€‚**å…³æ³¨ç‚¹**ï¼šéšç€ CUDAâ€¯13.1ã€SM120 ä¸ ROCm çš„å¿«é€Ÿè¿­ä»£ï¼Œä¿æŒä»£ç åº“çš„è·¨å¹³å°å…¼å®¹æ€§ä¸ AOT/JIT ç¼“å­˜ç­–ç•¥å°†æ˜¯ä¸‹ä¸€æ­¥çš„å…³é”®ã€‚
