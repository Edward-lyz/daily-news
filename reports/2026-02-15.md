今日的 AI Infra 的新闻如下。

## 摘要
## 本期关键变更概览

- **NVIDIA/cutlass**：两条新 Issue 聚焦于 TMA 描述符地址获取以及 TVM FFI 编译函数的最佳实践，提示在主机侧构建 TMA 后需要暴露其地址以便 PTX `gather4` 使用，同时呼吁统一 TVM 与 CUTLASS 的张量包装方式。  
- **flashinfer-ai/flashinfer**：社区关注 JIT 缓存 wheel 在 Rust 绑定中的完整性，尤其是新内核（如 `gdn_decode`）未被打包，暗示后续需要扩展 wheel 规范并制定兼容性基准。  
- **Dao-AILab/flash-attention**：通过将 stride 参数强制转为 `index_t` 修复了 int32 溢出问题，提升了大序列长度下的数值安全性。  
- **sgl-project/sglang**：本期提交量最高，涵盖 Claude 技能文档、Ascend NPU 示例、SM12x（Blackwell）内核调度、CUDA13 预加载提示、libnuma 加载容错、Profiler 参数自动填充、路由一致性哈希、NCCL 临时端口、MoE 量化路径、FP8 测试、Jetson AGX Thor 支持等多维度改进，显著增强跨平台兼容性与 CI 稳定性。  
- **vllm-project/vllm**：多项底层修复包括关闭 `torch.compile` 在特定并行场景下的冲突、为 ARM 添加 BF16 交叉编译支持、引入 Triton ViT 后端、文档更新、请求 ID 随机化开关、FlashInfer FP8 MoE 崩溃修复、Nemotron‑Nano 精度回退、错误响应统一、Token ID 溢出防护以及 Helion GPU 兼容层改进，整体提升了部署灵活性和错误可观测性。

## 具体内容分析
### deepseek-ai/DeepGEMM
昨日无更新。
### deepseek-ai/FlashMLA
昨日无更新。
### NVIDIA/cutlass
提交  

- **Issue:** [QST] [Cute-DSL] How to get address of TMA descriptor  
  - **链接:** https://github.com/NVIDIA/cutlass/issues/3037  
  - **作者:** tridao  
  - **创建时间:** 2026‑02‑15 21:52:14 UTC  
  - **概要:** 询问在主机上使用 `cute.nvgpu.make_tiled_tma_atom` 构造 TMA 描述符后，如何获取该描述符的地址，以便在内联 PTX 中配合 `gather4` TMA 指令使用。  
  - **代码变动:** 无（暂无 PR 或 commit，故未提供链接）。  

- **Issue:** [QST] [TVM FFI] Best Practices Clarification  
  - **链接:** https://github.com/NVIDIA/cutlass/issues/3036  
  - **作者:** HanGuo97  
  - **创建时间:** 2026‑02‑15 17:46:08 UTC  
  - **概要:** 澄清 TVM FFI 编译函数时张量来源的最佳实践，并比较 `make_fake_tensor` 与 `make_fake_compact_tensor` 的区别。  
  - **代码变动:** 无（暂无 PR 或 commit，故未提供链接）。
### flashinfer-ai/flashinfer
## 提交

本期无提交记录。

## Issues

**flashinfer-ai/flashinfer**

*   **Issue**: [Continued JIT cache wheel support for Rust bindings](https://github.com/flashinfer-ai/flashinfer/issues/2567)
    *   **作者**: zackangelo
    *   **内容**: 用户正在为 FlashInfer 开发 Rust 绑定，并尝试直接对接 TVM FFI 导出的 JIT 缓存 wheel。虽然当前效果良好，但发现部分新内核（如 `gdn_decode`）未包含在 wheel 中。询问团队是否计划继续支持这些 wheel，以及内核纳入 JIT 缓存的标准是什么。
### Dao-AILab/flash-attention
## 提交

**Dao-AILab/flash-attention** 仓库发生一次提交，修复了 int32 溢出问题：

- **提交信息**：[Fix int32 overflow (#2260)](https://github.com/Dao-AILab/flash-attention/commit/b62d93f37e802142eb381998c9ebf43a151f8fc9)
- **作者**：Driss Guessous
- **涉及文件**：
  - `csrc/flash_attn/src/flash_bwd_kernel.h`
  - `csrc/flash_attn/src/flash_bwd_preprocess_kernel.h`

该提交通过显式将部分 stride 参数转换为 `index_t` 类型（即 `static_cast<index_t>`）来避免在大序列长度下出现的整数溢出问题。具体修改点包括对 `dq_accum_batch_stride` 和 `dq_accum_row_stride` 的类型安全计算。

由于 diff 内容被截断（`patch_truncated`: true），无法展示完整变更细节。建议查看完整 commit 获取更多信息。
### sgl-project/sglang
- **d3bae71** – *Add claude skills for sgl‑kernel and jit‑kernel* (PR [#18855](https://github.com/sgl-project/sglang/pull/18855))  
  - 作者：Xiaoyu Zhang 2026‑02‑15 23:14 UTC  
  - 新增 `.claude/skills/add‑jit‑kernel/SKILL.md` 与 `.claude/skills/add‑sgl‑kernel/SKILL.md`，提供两套 “Skill” 教程。示例开头：

    ```markdown
    ---
    name: add-jit-kernel
    description: Step-by-step tutorial for adding a lightweight JIT CUDA/C++ kernel …
    ---
    ```

- **fd5a45d** – *Update Ascend NPU support docs* (PR [#18868](https://github.com/sgl-project/sglang/pull/18868))  
  - 作者：chenxu214 2026‑02‑15 17:41 UTC  
  - `docs/platforms/ascend_npu_support.rst` 新增两篇示例文档链接：`ascend_npu_qwen3_5_examples.md`、`ascend_npu_glm5_examples.md`。

- **f2d7286** – *Create Ascend NPU Qwen‑3.5 examples* (PR [#18864](https://github.com/sgl-project/sglang/pull/18864))  
  - 作者：chenxu214 2026‑02‑15 17:15 UTC  
  - 新增 `docs/platforms/ascend_npu_qwen3_5_examples.md`，包含 Docker 拉取与启动指令（示例片段）：

    ```bash
    docker pull swr.cn-southwest-2.myhuaweicloud.com/base_image/dockerhub/lmsysorg/sglang:cann8.5.0-a3-qwen35
    docker run -itd --shm-size=16g --privileged=true --name ${NAME} …
    ```

- **0d30896** – *Fix SM12x kernel dispatch (>=120)* (PR [#18750](https://github.com/sgl-project/sglang/pull/18750))  
  - 作者：blake‑snc 2026‑02‑15 16:44 UTC  
  - 修改三处 `.cu` 文件，将 `if (sm_version == 120)` 替换为 `if (sm_version >= 120)`，确保 SM12x（Blackwell）GPU 正确走新路径。

- **5fc3284** – *Support CUDA‑13 runtime preloading for DGX Spark* (PR [#18747](https://github.com/sgl-project/sglang/pull/18747))  
  - 作者：blake‑snc 2026‑02‑15 16:43 UTC  
  - `sgl_kernel/python/sgl_kernel/load_utils.py` 新增 CUDA‑13 安装提示：

    ```python
    if cuda_version and cuda_version.startswith("13"):
        install_hint = "pip install sgl-kernel --index-url https://docs.sglang.ai/whl/cu130/"
    ```

- **b79808b** – *Robust libnuma loading* (PR [#15355](https://github.com/sgl-project/sglang/pull/15355))  
  - 作者：Mike Qiu 2026‑02‑15 16:38 UTC  
  - `python/sglang/srt/utils/common.py` 新增 `get_libnuma()`，在 `numa_bind_to_node` 中安全检查并在缺失时记录错误而非抛异常。

- **48eac1b** – *Improve profiler options for bench_serving* (PR [#16991](https://github.com/sgl-project/sglang/pull/16991))  
  - 作者：akhilg‑nv 2026‑02‑15 16:36 UTC  
  - `python/sglang/bench_serving.py` 增加对 `/start_profile` 接口的参数自动填充（如 `profile_num_steps`、`profile_output_dir`）。

- **f759960** – *Expose `consistent_hashing` policy in router CLI* (PR [#17972](https://github.com/sgl-project/sglang/pull/17972))  
  - 作者：Blake Ledden 2026‑02‑15 16:33 UTC  
  - `sgl-model-gateway/bindings/python/src/sglang_router/router_args.py` 为 `--policy`、`--prefill-policy`、`--decode-policy` 添加新选项 `consistent_hashing`、`prefix_hash`。

- **597d17d** – *Use ephemeral NCCL port via `get_free_port()`* (PR [#18009](https://github.com/sgl-project/sglang/pull/18009))  
  - 作者：Chanh Nguyen 2026‑02‑15 16:33 UTC  
  - `python/sglang/srt/server_args.py` 替换 `is_port_available` 为 `get_free_port`，并在单元测试中 mock 该函数。

- **7a607c4** – *Fix quant method selection for fused MoE* (PR [#18459](https://github.com/sgl-project/sglang/pull/18459))  
  - 作者：tjp_zju 2026‑02‑15 16:32 UTC  
  - `python/sglang/srt/layers/quantization/moe_wna16.py` 引入 `UnquantizedFusedMoEMethod`，在层被跳过量化时返回对应实现。

- **536ed31** – *Add Modelopt FP8 test on SM90* (PR [#18463](https://github.com/sgl-project/sglang/pull/18463))  
  - 作者：Zack Yu 2026‑02‑15 16:30 UTC  
  - 新增 `test/registered/quant/test_modelopt_fp8.py`，启动服务器并使用 GSM8K 基准验证 FP8 推理。

- **b2f74d6** – *Add SM110 (Jetson AGX Thor) to Blackwell check* (PR [#18787](https://github.com/sgl-project/sglang/pull/18787))  
  - 作者：WiwilZ 2026‑02‑15 16:27 UTC  
  - `python/sglang/srt/utils/common.py` 将 `device_capability_majors` 扩展为 `[10, 11, 12]`。

- **57f7e06** – *Update Blackwell log messages* (PR [#18751](https://github.com/sgl-project/sglang/pull/18751))  
  - 作者：blake‑snc 2026‑02‑15 16:24 UTC  
  - `python/sglang/srt/server_args.py` 将日志中 “SM100” 替换为 “Blackwell”，提升可读性。

- **07a24f1** – *Upgrade pre‑commit & clang‑format versions* (PR [#18860](https://github.com/sgl-project/sglang/pull/18860))  
  - 作者：SoluMilken 2026‑02‑15 16:19 UTC  
  - `.github/workflows/lint.yml` 使用 `clang-format-lint-action@v0.20` 与 `clangFormatVersion: 20`；`.pre-commit-config.yaml` 升级至 `pre‑commit‑hooks v6.0.0`、`isort v7.0.0`、`ruff v0.15.1`、`black v26.1.0`。

- **f760320** – *Enable DeepGemm fast warm‑up in CI* (PR [#18823](https://github.com/sgl-project/sglang/pull/18823))  
  - 作者：Alison Shao 2026‑02‑15 16:03 UTC  
  - `.github/workflows/pr-test.yml` 新增一行配置，防止 DeepGemm 冷启动超时。

- **4ef8ece** – *Add build commit to sgl‑kernel workflow* (PR [#18853](https://github.com/sgl-project/sglang/pull/18853))  
  - 作者：Douglas Yang 2026‑02‑15 15:29 UTC  
  - `release-whl-kernel.yml` 中加入 `BUILD_COMMIT` 环境变量，以便追踪构建来源。

- **ddfe147** – *Layerwise offload buffer reuse for diffusion* (PR [#18611](https://github.com/sgl-project/sglang/pull/18611))  
  - 作者：Ratish P 2026‑02‑15 14:18 UTC  
  - `python/sglang/multimodal_gen/runtime/utils/layerwise_offload.py` 增加 39 行优化，提升显存复用效率。

- **88010e9** – *AMD nightly fixes & bench_serving regression* (PR [#18761](https://github.com/sgl-project/sglang/pull/18761))  
  - 作者：Michael 2026‑02‑15 12:37 UTC  
  - 更新 AMD CI 工作流、`bench_serving.py` 小幅回滚以及相关测试。

- **90555a0** – *Add missing dumper tests* (PR [#18859](https://github.com/sgl-project/sglang/pull/18859))  
  - 作者：fzyzcjy 2026‑02‑15 10:43 UTC  
  - `test/registered/debug_utils/test_dumper.py` 新增 83 行单元测试。

- **4c7f986** – *Extract dumper & prefill‑delayer common utils* (PR [#18857](https://github.com/sgl-project/sglang/pull/18857))  
  - 作者：fzyzcjy 2026‑02‑15 10:33 UTC  
  - 新增 `python/sglang/test/test_utils.py`，并在 `test_dumper.py`、`test_prefill_delayer.py` 中复用。

- **b992828** – *Fix Kimi‑2.5 DP2/TP4 bug* (PR [#18604](https://github.com/sgl-project/sglang/pull/18604))  
  - 作者：haowen‑han 2026‑02‑15 08:32 UTC  
  - `python/sglang/srt/layers/attention/trtllm_mla_backend.py` 微调 1 行逻辑。

- **274bf66** – *Enable `torch.compile` for UlyssesAttention* (PR [#18840](https://github.com/sgl-project/sglang/pull/18840))  
  - 作者：Ratish P 2026‑02‑15 07:55 UTC  
  - 删除 `python/sglang/multimodal_gen/runtime/layers/attention/layer.py` 中两行阻止编译的代码。

- **ad1bdb9** – *Add MiniMax‑2.5 fused‑MoE tuning config for H20* (PR [#18833](https://github.com/sgl-project/sglang/pull/18833))  
  - 作者：zhangxiaolei123456 2026‑02‑15 07:47 UTC  
  - 新增 JSON 配置文件 `.../E=256,N=384,device_name=NVIDIA_H20,...json`（146 行）。

- **922fbc2** – *Tune MiniMax‑M2 fused‑MoE kernel on H100* (PR [#18851](https://github.com/sgl-project/sglang/pull/18851))  
  - 作者：jackey hua 2026‑02‑15 07:31 UTC  
  - 同上，针对 H100 GPU 添加专用 JSON 配置。

- **944a9f6** – *Fix Qwen‑3.5 AMD rope fallback* (PR [#18753](https://github.com/sgl-project/sglang/pull/18753))  
  - 作者：andyluo7 2026‑02‑15 06:10 UTC  
  - `python/sglang/srt/configs/qwen3_5.py` 与 `hybrid_linear_attn_backend.py` 小幅改动，确保 AMD 上的 RoPE 正常。

- **91230d** – *Fix JIT kernel compilation on newer GPUs* (PR [#18496](https://github.com/sgl-project/sglang/pull/18496))  
  - 作者：muse‑coder 2026‑02‑15 04:15 UTC  
  - `python/sglang/jit_kernel/include/sgl_kernel/utils.cuh` 与 `utils.py` 调整驱动元数据检查，提升兼容性。

- **1ce3420** – *Support IBM Granite (Dense
### vllm-project/vllm
**提交概览**  

| SHA（短） | 模块/文件 | 关键改动 | 关联 PR（Commit URL） |
|---|---|---|---|
| `23d825a` | `vllm/config/model.py`、`vllm/config/vllm.py`、`vllm/model_executor/models/config.py` | 关闭 `torch.compile` 在 ds3‑fp4 与 DP 场景下的 ar‑rms 融合，修复 CI。<br>（代码片段缺失，diff 被截断） | [23d825a](https://github.com/vllm-project/vllm/commit/23d825aba11afcc6713e9b11acb54c473a734501) |
| `f07a128` | `cmake/cpu_extension.cmake`、`docker/Dockerfile.cpu`、`docs/getting_started/installation/cpu.arm.inc.md` | 为 ARM 添加 BF16 交叉编译支持并完善文档。<br>（代码片段缺失，diff 被截断） | [f07a128](https://github.com/vllm-project/vllm/commit/f07a128413ff6900f407e46fe2b36d93eb2a0c12) |
| `71cd892` | `vllm/model_executor/layers/attention/mm_encoder_attention.py`、`vllm/v1/attention/ops/vit_attn_wrappers.py` 等 | 引入 Triton ViT 注意力后端，提升多模态编码器性能。<br>（代码片段缺失，diff 被截断） | [71cd892](https://github.com/vllm-project/vllm/commit/71cd89264f6c88ee4cca7c3cc556885e8844fc92) |
| `19fab44` | `docs/models/supported_models.md`、`docs/usage/v1_guide.md` | 更新文档，加入对 Florence‑2 编码‑解码模型的支持说明。 | [19fab44](https://github.com/vllm-project/vllm/commit/19fab441526b39990e292845ede8886a348d0d7e) |
| `79c7e09` | `vllm/envs.py`、`vllm/v1/engine/input_processor.py` | 新增实验性环境变量 `VLLM_DISABLE_REQUEST_ID_RANDOMIZATION`（默认关闭），用于临时规避请求 ID 随机化问题。 | [79c7e09](https://github.com/vllm-project/vllm/commit/79c7e092350e4ae82d679ea4b2cdaaa4b580944b) |
| `79f3fab` | `tests/kernels/moe/test_flashinfer.py`、`vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py` | 修复 FlashInfer 块级 FP8 MoE 在 `num_expert_group=None` 时的崩溃。 | [79f3fab](https://github.com/vllm-project/vllm/commit/79f3fab05a2d88a5db73591cd3b8afdf956ee723) |
| `604b9ea` | `vllm/model_executor/layers/mamba/mamba_mixer2.py` | 纠正 NVIDIA‑Nemotron‑3‑Nano‑30B‑A3B‑NVFP4 在多张卡（TP>1）下的精度回退。 | [604b9ea](https://github.com/vllm-project/vllm/commit/604b9eaec53a11ae5193348f6a623dd7cdef48bf) |
| `50dbd6c` | 多个 `vllm/entrypoints/*/api_router.py` 文件 | 修复在所有使用 `handler.create_error_response` 的路径上导致的关键错误报告缺失。 | [50dbd6c](https://github.com/vllm-project/vllm/commit/50dbd6c9e6637589b404a499dbc34df85ad9b1ad) |
| `98bcc6c` | `vllm/entrypoints/serve/tokenize/protocol.py` | 在 detokenize 时校验 token ID，防止 int64 溢出导致 500 错误。 | [98bcc6c](https://github.com/vllm-project/vllm/commit/98bcc6ca593293cf650699e54e499e7189c24ac1) |
| `f13e86d` | `vllm/kernels/helion/utils.py` | 将 Helion GPU 实用函数改为使用平台无关的设备名称 API，提升跨平台兼容性。 | [f13e86d](https://github.com/vllm-project/vllm/commit/f13e86d8ddf81c638bacce6f8876cf6acf421d58) |
| `9ca768c` | `vllm/v1/worker/gpu/sample/sampler.py`、`vllm/v1/worker/gpu/sample/states.py` | 对 Sampler 进行小幅清理，去除冗余代码并优化状态管理。 | [9ca768c](https://github.com/vllm-project/vllm/commit/9ca768c7404ed8d8a42c5ea3279d804ae454a874) |

> **说明**：多数提交的 diff 被标记为 `patch_truncated: true`，因此未展示具体代码片段，仅概述核心改动。

---

**近期 Issue 摘要**  

| 编号 | 标题 | 简要描述 |
|---|---|---|
| #34591 | **Ministral‑3 LoRA 适配器静默失败** | 用户在加载 LoRA 时没有任何错误提示，导致模型行为异常。 |
| #34586 | **RTX 6000 Pro Workstation 在 Docker 中不被 Torch 识别** | 在官方 `vllm-openai:cu130-nightly` 镜像里，容器无法检测到 GPU，报 `CUDA_MODULE_LOADING` 配置错误。 |
| #34583 | **MTP / Eagle 预测方法缺失词表校验，可能导致 OOB 访问** | 词表未验证导致潜在的越界读取风险。 |
| #34579 | **单 Prompt 场景下吞吐量落后于 llama.cpp** | 在 AMD Strix Halo APU 上，vLLM 仅 7.5 tok/s，远低于 llama.cpp 的 50 tok/s。 |
| #34578 | **单 Prompt 吞吐性能进一步讨论** | 同上，用户希望在不增加并发的情况下提升性能。 |

> 所有 Issue 均已在 2026‑02‑15 创建，当前暂无评论或解决方案。若您遇到类似问题，可在对应 Issue 中补充环境信息或复现步骤。
### NVIDIA/cutile-python
昨日无更新。

## 总结
### 展望与建议

- **关注即将合并的 Cutlass PR**：TMA 地址暴露和 TVM FFI 统一方案将直接影响高性能张量传输和跨框架集成，建议提前在内部测试环境验证对应 API。  
- **FlashInfer 与 Rust 绑定**：若项目依赖 Rust 生态，需监控 JIT wheel 的完整性更新，避免因缺失新内核导致运行时回退。  
- **sglang 的跨平台扩展**：新增的 Ascend NPU 示例和 Jetson 支持为多硬件部署打开了可能，建议在相应硬件上跑完整 CI，以捕获潜在的库依赖或驱动兼容问题。  
- **vllm 的 ARM 与 Triton 路线**：BF16 编译和 ViT Triton 加速已落地，团队可评估在 ARM 服务器上开启 BF16 以提升吞吐，同时利用 ViT 后端探索视觉大模型的加速路径。  
- **整体监控**：上述仓库的改动多涉及底层算子、编译器选项和硬件适配，建议在 CI 中加入对应的回归测试（如 int32 溢出、Token ID 边界、NCCL 端口冲突），并在发布前进行跨硬件的性能基准对比，以确保新特性不会引入隐藏的回归。
