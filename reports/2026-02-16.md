今日的 AI Infra 的新闻如下。

## 摘要
### 本周关键变更概览

1. **CUDA 与 CI 持续升级**
   - `flashinfer` 更新 Docker CI 标记，覆盖多版本 CUDA，提升跨平台兼容性。 
   - `sglang` 与 `vllm` 均加入缺失的 FlashInfer cubin 下载脚本及 CI 镜像构建改进，防止构建缓存冲突。
2. **MoE 与稀疏算子强化**
   - `flashinfer` 为 Mamba 添加 `selective_state_update` 支持，并在 CuTe DSL 中实现 TMA block‑reduce，提升 MoE 计算效率。 
   - `flash-attention` 通过 mbarrier 协议解决 SM100/SM110 上空 tile 导致的 kernel 死锁，显著提升高稀疏度前向的稳定性。 
   - `sglang` 将压缩张量方案迁移至 `compressed_tensors/schemes`，并在多套 MoE 实现中加入 W4A4/W8A8 方案，提升显存利用率。 
3. **量化与 FP8 进展**
   - `sglang` 在 Blackwell GPU 上实现 MXFP4 MoE 加速，性能提升 9.5×；同时为 DeepSeek‑v3.2、Qwen‑3 系列加入 FP8 支持。 
   - `vllm` 完成 Qwen3.5 GDN 中 `qkvz_proj` 与 `ba_proj` 融合，减少算子调用；修复 ModelOpt FP8 相关 KV‑scale 逻辑。 
4. **Diffusion 与多模态改进**
   - `sglang` 多次重构 Diffusion 的层级 offload、日志记录与 dump 格式，提升可追溯性与显存回收效率。 
   - 引入 Ernie4.5‑VL Triton 融合 kernel，显著加速跨模态注意力。 
5. **ROCm 与 AMD 支持**
   - `vllm` 新增 MI355 完整测试矩阵，更新 Terratorch 依赖，修复 ROCm 上的插件分组错误。 
   - `sglang` 为 MORI‑EP 交叉 kernel 添加切换开关，完善 AMD ROCm 环境。 
6. **API 稳定性与监控**
   - `vllm` 将 `generation_config.max_tokens` 设为默认上限，统一 API 行为；加入 KV‑offload ARC 错误修复、Instrumentation 指标增强。 
   - `sglang` 通过 `SGLANG_TORCH_PROFILER_DIR` 环境变量统一 Torch profiler 输出路径，提升调试体验。 
7. **已报告的关键问题**
   - `vllm` 仍面临 MTP 推理导致的 JSON 解析错误、OOM 后孤儿进程残留、Blackwell GPU 上的 `SharedStorageConnector` 断言等阻塞问题。 
   - `flash-attention` 的空 tile mbarrier 修复已关闭相关 kernel hang 报告。 
   - `sglang` 的 MoE 在 Blackwell GPU 上出现乱码（MiniMax‑M2.5）以及 GLM‑4.7‑Flash 在 Blackwell 上的错误仍待进一步验证。

## 具体内容分析
### deepseek-ai/DeepGEMM
昨日无更新。
### deepseek-ai/FlashMLA
昨日无更新。
### NVIDIA/cutlass
昨日无更新。
### flashinfer-ai/flashinfer
## 提交

**flashinfer-ai/flashinfer**

- **更新 Docker CI 标签至 20260209-a2d3b39** ([a2d6d49](https://github.com/flashinfer-ai/flashinfer/commit/a2d6d49822df7f07a84c792fc7d1b421ad6198b9))
  - 更新了多个 CUDA 版本的 CI 镜像标签。
  - 修改文件：`ci/docker-tags.yml`

- **为 Mamba selective_state_update 添加微基准测试支持** ([0ebf05e](https://github.com/flashinfer-ai/flashinfer/commit/0ebf05ee48a244cbe061ab186f6d5913d321795f))
  - 在基准测试框架中新增对 Mamba API 的支持，包括 `selective_state_update`。
  - 修改了 `benchmarks/README.md`、`benchmarks/flashinfer_benchmark.py` 和相关工具文件以支持新功能。
  - 新增文件：`benchmarks/routines/mamba.py`
  - 示例测试列表已更新。

- **Chore: Cute dsl moe update (TMA.RED 实现)** ([b0f1c96](https://github.com/flashinfer-ai/flashinfer/commit/b0f1c9602623804c0848af35b7860a86d0e38a97))
  - 更新了 CuTe DSL 中 MoE 的实现，引入了 TMA 块归约（block reduce）操作。
  - 添加了新的块归约函数，并在配置中启用了该选项。
  - 修改了参考计算逻辑，现在直接使用 GPU 进行运算而非 CPU。

- **添加 Qwen3N 测试用例** ([a003c02](https://github.com/flashinfer-ai/flashinfer/commit/a003c021407c60c1ae3ffd142b1dee7219574325))
  - 扩展了 prefill delta rule 的测试参数范围，增加了更多头数和序列长度组合。
  - 修改文件：`tests/gdn/test_prefill_delta_rule.py`
### Dao-AILab/flash-attention
**提交**  
- **SHA**: `fec3a6a`  
- **标题**: `[Cute][Flex] Fix kernel hang w/ multiple empty tiles (#2258)`  
- **作者**: Driss Guessous  
- **日期**: 2026‑02‑16  
- **链接**: <https://github.com/Dao-AILab/flash-attention/commit/fec3a6a18460c1b40f097208d4c16fe8964a679d>  

**影响文件**  

| 文件路径 | 变更类型 | 行数增删 |
|---|---|---|
| `flash_attn/cute/block_sparse_utils.py` | 修改 | +65 / –10 |
| `flash_attn/cute/flash_fwd_sm100.py` | 修改 | +3 / –2 |
| `tests/cute/test_mask_mod.py` | 修改 | +56 / 0 |

**代码片段**  

- **`block_sparse_utils.py`**（关键注释）  
  ```python
  # NOTE [SM100 block-sparse empty tiles: mbarrier contract]
  # 对于 SM100 前向，若 (m_block, stage) Q tile 没有活跃 KV block，
  # 需要在 softmax 与 correction warp‑group 之间仍然完成
  # mbarrier handshake，以避免相位不同步导致死锁。
  # 这里实现了 full ↔ empty 的 barrier 合约。
  ```

- **`flash_fwd_sm100.py`**（软最大循环）  
  ```python
  if const_expr(self.use_block_sparsity) or has_work:
      # 参见 block_sparse_utils.py 中的 empty‑tile mbarrier 合约
      cute.arch.mbarrier_wait(
          mbar_ptr + self.mbar_softmax_corr_empty_offset + stage,
          si_corr_producer_phase
      )
  ...
  cute.arch.mbarrier_arrive(
      mbar_ptr + self.mbar_softmax_corr_full_offset + stage
  )
  ```

- **`test_mask_mod.py`**（新增回归测试）  
  ```python
  @pytest.mark.skipif(COMPUTE_CAPABILITY not in (10, 11),
                      reason="SM100/SM110 persistent forward only")
  def test_persistent_blocksparse_empty_tiles():
      """验证在高度稀疏的 block mask 下，持久化前向不会因空 tile
      的 barrier 相位不同步而死锁。"""
      ...
  ```

**备注**  

- `block_sparse_utils.py` 的 diff 被截断（`patch_truncated: true`），但核心逻辑已在注释中概述，主要是为 empty‑tile 场景补全 mbarrier 合约，确保 softmax 与 correction 两个 warp‑group 能正确同步。  
- 该修复解决了在多空 tile 情形下 kernel 卡死的问题，提升了 SM100/SM110 上的持久化前向稳定性。  

**Issues**  
- 本次提交未关联任何 Issue。
### sgl-project/sglang
**提交**

- **f9c3def** – 修复 CI：在 `scripts/ci/cuda/ci_install_dependency.sh` 中新增调用 `ci_download_flashinfer_cubin.sh`，确保缺失的 flashinfer cubin 能自动下载。涉及文件：`scripts/ci/cuda/ci_download_flashinfer_cubin.sh`（新增 32 行）和 `scripts/ci/cuda/ci_install_dependency.sh`（新增 3 行）。  
  <https://github.com/sgl-project/sglang/commit/f9c3def7fe464a19d8321474a4e149a3a8d3dcc4>

- **1b659bc** – 为 GLM‑5 MoE 添加 fused shared expert 支持：在 `python/sglang/srt/models/glm4_moe.py` 中实现 `determine_num_fused_shared_experts` 方法。  
  <https://github.com/sgl-project/sglang/commit/1b659bcb088fd303ee472c0720eeb1a3797cffa1>

- **0ff2415** – ModelOpt FP8 权重创建函数新增 `input_size`、`output_size` 参数；在 `NemotronHForCausalLM` 中补全 KV scale 重映射逻辑。涉及文件：`python/sglang/srt/layers/quantization/modelopt_quant.py`（+2 行）和 `python/sglang/srt/models/nemotron_h.py`（+15 行）。  
  <https://github.com/sgl-project/sglang/commit/0ff24159a5bcd0872a65135b0b1eeb3350273949>

- **eba6af3** – 量化重构：将压缩张量 MoE 方案迁移至 `compressed_tensors/schemes`，新增多套 scheme（如 `compressed_tensors_w4a4_nvfp4_moe`、`compressed_tensors_w8a8_fp8_moe` 等），并在 `ep_moe/layer.py`、`fused_moe_triton/layer.py` 中更新引用。此 PR 包含 **4880** 文件改动（新增 2643 行，删除 2237 行），核心文件包括 `base_scheme.py`（新增 99 行）和 `schemes/` 目录下的多个实现。  
  <https://github.com/sgl-project/sglang/commit/eba6af385d17a3fe72719e14f70fe6230026c4f8>

- **1b3513a** – 重构 FAKE 传输后端，移除 `--disaggregation-decode-enable-fake-auto` 参数；相应修改 `server_arguments.md`、`ascend/conn.py`、`fake/` 相关实现。  
  <https://github.com/sgl-project/sglang/commit/1b3513a7e49f0182ffe1762bd72442879608b3ae>

- **c1d1337** – 修复 Diffusion 中稀疏注意力后端错误地被用于 cross‑attention。涉及文件：`dits/causal_wanvideo.py`、`dits/wanvideo.py`、`platforms/interface.py`。  
  <https://github.com/sgl-project/sglang/commit/c1d1337afc52e17f6f724fb7294dc8997e9ea0d1>

- **b86c649** – Blackwell GPU 上 MXFP4 MoE 权重加载提升约 **9.5×**，优化 `quantization/mxfp4.py`。  
  <https://github.com/sgl-project/sglang/commit/b86c6491fa7080ccea00630935c9c88f9407b847>

- **4f0409f** – 新增 Qwen‑3 Reward Model 并修复 Qwen‑3 Sequence Classification。关键文件：`model_config.py`（+1 行）、`qwen3_classification.py`（+81 行）和新加入的 `qwen3_rm.py`（+47 行）。  
  <https://github.com/sgl-project/sglang/commit/4f0409f8aa59baa26892712d5ad65fc6a4e1fc13>

- **de833f9** – 回滚 Diffusion 中的 layerwise offload 缓冲区复用改动，恢复原有实现。涉及文件：`multimodal_gen/runtime/utils/layerwise_offload.py`（+9 行，‑39 行）。  
  <https://github.com/sgl-project/sglang/commit/de833f9e8e9353d83a3ba06738861ee45f034bf0>

- **d0c94e1** – Diffusion 运行时新增 VRAM 峰值日志，改进 `generate.py`、`gpu_worker.py`、`schedule_batch.py`、`perf_logger.py` 中的日志输出。  
  <https://github.com/sgl-project/sglang/commit/d0c94e136af60f28c5d9d9b7e22dcbe828a85603>

- **ed22720** – JIT QK 归一化 kernel 支持 `hd=512,1024`（基于 CTA），更新 `bench_qknorm.py`、`qknorm.cuh`、对应测试。  
  <https://github.com/sgl-project/sglang/commit/ed22720c07b12b9d577d2c02d21618b2081e84b5>

- **206accd** – 修复 GLM‑4V 处理器在缺少 `glm_ocr` 时的注册错误。文件：`processors/glm4v.py`（+12 行）。  
  <https://github.com/sgl-project/sglang/commit/206accd15de341e10be68ae6139a3cdfb610eac3>

- **61da34a** – LoRA 权重快照别名问题修复，避免在 unmerge 逻辑中产生冲突。文件：`layers/lora/linear.py`（+3 行）。  
  <https://github.com/sgl-project/sglang/commit/61da34ad0b8a0f9b844ea123b1f206fad419c793>

- **d85884c** – 更新 Ascend NPU Qwen‑3.5 示例文档，微调说明文字。文件：`docs/platforms/ascend_npu_qwen3_5_examples.md`（±3 行）。  
  <https://github.com/sgl-project/sglang/commit/d85884ca5759feebe7164b4d6884746776582218>

- **86c181e** – 修复 `test_lora_qwen3` 夜间跑失败：将 adapter 替换为 `added_tokens`。文件：`test/lora_utils.py`（±1 行）。  
  <https://github.com/sgl-project/sglang/commit/86c181e33597952a8a483134ca05db6d6699d32e>

- **f1efb46** – 为 Diffusion 夜间测试加入性能日志，更新 CI workflow、前端仪表盘以及测试脚本。关键文件：`.github/workflows/nightly-test-nvidia.yml`（+42 行）等。  
  <https://github.com/sgl-project/sglang/commit/f1efb46bdd1b1b082ea3f32f416000018b30f83c>

- **2050875** – 统一 Docker 镜像构建流程，删除旧的 release‑docker‑cu13 与 dev‑pr 工作流，更新 `release-docker-dev.yml`。  
  <https://github.com/sgl-project/sglang/commit/2050875424b780aea83fa724b72f0f2e441e680c>

- **f554b3c** – 支持调试工具导出梯度、参数及惰性值；`dumper.py` 大幅改动（+144 行），对应测试 `test_dumper.py` 更新。  
  <https://github.com/sgl-project/sglang/commit/f554b3c27bb96a9a238d8c4d6506d4a356676336>

- **9a7d8d5** – 在 dump 输出中收集上层元数据，提升可追溯性。涉及 `dumper.py`（+115 行）和测试。  
  <https://github.com/sgl-project/sglang/commit/9a7d8d5eb025d4686f284815ee835cfaf73207eb>

- **949792d** – 将 dump 输出格式改为包含 `value` 与 `metadata` 的字典结构。涉及 `dump_comparator.py`、`dump_loader.py`、`dumper.py`。  
  <https://github.com/sgl-project/sglang/commit/949792d0c658323249afc503e5b5655cdfdbab2d>

- **02816ab** – 默认关闭 dumper，重构环境变量处理逻辑。`dumper.py`（+64 行，‑15 行）和 `environ.py`（+11 行）同步更新。  
  <https://github.com/sgl-project/sglang/commit/02816abc0d1712284bcc47c7163593675cc751a2>

- **5ddc84e** – AMD ROCm 支持 MORI‑EP 交叉 kernel 类型切换，更新 Dockerfile 与文档。  
  <https://github.com/sgl-project/sglang/commit/5ddc84e33e64c7891597ecb9a4346a13c94e94c0>

- **bc79a64** – 新增 `SGLANG_TORCH_PROFILER_DIR` 环境变量，用于指定 Torch profiler 日志目录。文件：`utils/profiler.py`（+7 行）。  
  <https://github.com/sgl-project/sglang/commit/bc79a64d3a81b1540c15d376b967dd436503d926>

- **0af9dcc** – 重构 Diffusion `server_args` 调整与校验逻辑，代码行数大幅变动（+209，‑167）。涉及 `wanvideo.py` 与 `server_args.py`。  
  <https://github.com/sgl-project/sglang/commit/0af9dcc407fd5a0363a08dad40c536d6ebc90b3d>

- **78b4c9e** – 防止在 warm‑up 请求中保存输出，避免不必要的 I/O。修改 `scheduler.py` 与 `schedule_batch.py`。  
  <https://github.com/sgl-project/sglang/commit/78b4c9e248b8fe1f1bf3126d0eb00d345b5f6920>

- **8a82c70** – 为 Ernie4.5‑VL 旋转嵌入实现 Triton 融合 kernel，显著提升性能（+268 行）。  
  <https://github.com/sgl-project/sglang/commit/8a82c70297fdf1a24c4e32d9111f0299d389d9c0>

- **45715af** – 修复 nightly wheel 版本号后缀错误，确保发布流程一致。  
  <https://github.com/sgl-project/sglang/commit/45715af50cc4312037ab62fda0ae0bd9bb6060a2>

- **0ffd0a3** – 为 DeepSeek‑v3.2 添加 NVIDIA TRT‑LLM MLA 稀疏 FP8 支持，更新 attention 后端与相关 utils。涉及 `nsa_backend.py`（+172 行）等多个文件，总改动 **535** 行。  
  <https://github.com/sgl-project/sglang/commit/0ffd0a3995e537c2ec6f450cbb7d4b4db44550aa>

- **8290171** – 移除 gpt‑oss 测试中的 `--mem-fraction-static 0.93` 参数，简化 CI。  
  <https://github.com/sgl-project/sglang/commit/8290171f5247061d48ca6545d2e298599db0359c>

**Issues**

- **#18893** – 提议在 Refit 单元测试中加入 SHA256 校验，以确保权重更新后保持位级一致性。  
  <https://github.com/sgl-project/sglang/issues/18893>

- **#18891** – 建议在 Diffusion 模型的 `update_weights_from_disk` 流程中，对 VAE 与文本编码器的服务器磁盘校验和进行比较，提升权重同步可靠性。  
  <https://github.com/sgl-project/sglang/issues/18891>

- **#18875** – 报告 MiniMax‑M2.5 在 8×A100（TP=8, EP=8）上运行时，FP8 Triton 编译失败且 Marlin 回退后 MoE kernel 崩溃。  
  <https://github.com/sgl-project/sglang/issues/18875>

- **#18874** – GLM‑4.7‑Flash 在 NVIDIA Blackwell (sm_120) GPU 上输出乱码，模型在其他框架下正常。  
  <https://github.com/sgl-project/sglang/issues/18874>
### vllm-project/vllm
**提交**  

- **[3b30e61]** – <https://github.com/vllm-project/vllm/commit/3b30e6150777de549b11f67dde3ecc0d3b1f3f50>  
  - **模块/文件**：`vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py`（+4 行），`vllm/model_executor/models/nemotron_h.py`（-4 行，+1 行）  
  - **改动要点**：取消对 Nemotron‑H 路由器强制使用 fp32 的限制，提升混合精度兼容性。  

- **[824f9e8]** – <https://github.com/vllm-project/vllm/commit/824f9e8f3c7f0a688b6093d0c85ed6b39ba314e1>  
  - **模块/文件**：`.buildkite/test-amd.yaml`（+1666 行）  
  - **改动要点**：新增针对 MI355 代理池的完整测试矩阵，覆盖所有现有单元测试。  

- **[6cc403e]** – <https://github.com/vllm-project/vllm/commit/6cc403e67d9ca8b4fc8c93a84096ed98161c938b>  
  - **模块/文件**：`tests/entrypoints/openai/responses/test_harmony.py`（-6 行，+3 行）  
  - **改动要点**：修复 `test_function_calling` 在 Harmony 环境下的间歇性失败。  

- **[72d5951]** – <https://github.com/vllm-project/vllm/commit/72d5951d02f2e76228b162fcb63068877850b724>  
  - **模块/文件**：`tests/entrypoints/openai/test_serving_chat.py`（+50 行，-10 行），`tests/entrypoints/test_utils.py`（+73 行，-1 行），`vllm/entrypoints/openai/chat_completion/serving.py`、`completion/serving.py`、`engine/serving.py`、`responses/serving.py`、`entrypoints/utils.py`（若干行）  
  - **改动要点**：将 `generation_config.max_tokens` 视为默认值而非上限，统一 API 行为。  

- **[a3205be]** – <https://github.com/vllm-project/vllm/commit/a3205beffb6b3d2923fd9ad8e1ef8b4fd5f7ed29>  
  - **模块/文件**：`tools/pre_commit/mypy.py`（-4 行），`vllm/config/*.py`（若干行），`vllm/engine/arg_utils.py`（+36 行，-13 行），`vllm/logger.py`（+4 行，-3 行），`vllm/outputs.py`（±1 行），`vllm/v1/cudagraph_dispatcher.py`（+13 行）  
  - **改动要点**：为被排除的文件启用 mypy 覆盖率，提升类型检查完整性。  

- **[6930bec]** – <https://github.com/vllm-project/vllm/commit/6930becd453ab81dce074505521a41a397d0c727>  
  - **模块/文件**：`examples/pooling/plugin/prithvi_geospatial_mae_io_processor.py`（+4 行，-2 行），`tests/plugins_tests/test_io_processor_plugins.py`（同），`vllm/entrypoints/llm.py`（+9 行，-1 行）  
  - **改动要点**：修正 IOProcessor 插件在 LLM 入口的编码逻辑，防止字符错位。  

- **[03a8770]** – <https://github.com/vllm-project/vllm/commit/03a8770a6d9ca4fcb7ff24fa5e6c75c25662919c>  
  - **模块/文件**：`requirements/rocm-test.txt`（+6 行，-2 行），`vllm/model_executor/models/terratorch.py`（±1 行）  
  - **改动要点**：更新 Terratorch 及其依赖，修复 ROCm 插件测试分组。  

- **[bc56a1d]** – <https://github.com/vllm-project/vllm/commit/bc56a1d56e98a747de7a6c0610673cecc681d808>  
  - **模块/文件**：`vllm/v1/kv_offload/arc_manager.py`（+2 行，-1 行）  
  - **改动要点**：修复 ARC 在非就绪 T1 块上的 `KeyError`，提升 KV offload 稳定性。  

- **[ec7d9e6]** – <https://github.com/vllm-project/vllm/commit/ec7d9e67459dd8ca6e2e3e77a40993291cab152c>  
  - **模块/文件**：`vllm/model_executor/layers/quantization/modelopt.py`（+10 行，-10 行）  
  - **改动要点**：纠正 MoE 模块中对 `moe_mk` 的调用，确保 LoRA 兼容。  

- **[3bb4e43]** – <https://github.com/vllm-project/vllm/commit/3bb4e4311c6da31257e6c8e5b1027ef516e025c8>  
  - **模块/文件**：`vllm/model_executor/layers/linear.py`（+28 行，-6 行），`vllm/model_executor/models/qwen3_5.py`（+32 行，-166 行），`vllm/model_executor/models/qwen3_next.py`（+27 行，-10 行）  
  - **改动要点**：将 Qwen3.5 GDN 的 `qkvz_proj` 与 `ba_proj` 融合，减少算子调用。  

- **[08f8c19]** – <https://github.com/vllm-project/vllm/commit/08f8c198ae211f0374fed0f0627a9119c457509f>  
  - **模块/文件**：`.buildkite/image_build/image_build.sh`（±2 行）  
  - **改动要点**：在 CI 镜像构建中禁用预编译 wheel 路径，防止缓存冲突。  

- **[a21cedf]** – <https://github.com/vllm-project/vllm/commit/a21cedf4ff1facaee601a635e3c092fe02742290>  
  - **模块/文件**：多处 `.buildkite/*.sh`（±1 行），`docs/features/quantization/*.md`（±1 行），`requirements/*.txt`（若干行）  
  - **改动要点**：升级 `lm-eval` 以兼容 Transformers v5，更新文档与依赖。  

- **[3ef74cd]** – <https://github.com/vllm-project/vllm/commit/3ef74cde5d253333e993ea26931956962b6f70db>  
  - **模块/文件**：`tests/tracing/conftest.py`（+23 行）  
  - **改动要点**：在追踪测试中加入服务器就绪检查，消除竞态。  

- **[cd81cdb]** – <https://github.com/vllm-project/vllm/commit/cd81cdb399e5ead89ac10eb3f8eff1fa85b427a1>  
  - **模块/文件**：`tests/v1/core/test_scheduler.py`（+294 行），`vllm/v1/core/sched/scheduler.py`（+11 行，-13 行）  
  - **改动要点**：修正可变长度编码器输入下的 CrossAttn 块计数逻辑。  

- **[1e82857]** – <https://github.com/vllm-project/vllm/commit/1e828573b4a788971220c17a41350c2068b4c810>  
  - **模块/文件**：`tests/entrypoints/instrumentator/test_metrics.py`（+66 行，-24 行）  
  - **改动要点**：为指标测试加入轮询与子进程守护，提升可靠性。  

- **[a5ccc85]** – <https://github.com/vllm-project/vllm/commit/a5ccc85c8c98115981a39b010b36f255e6446e77>  
  - **模块/文件**：`vllm/model_executor/layers/quantization/input_quant_fp8.py`（±3 行）  
  - **改动要点**：修复 Dynamo 调用时的意外关键字参数错误。  

- **[b5475d0]** – <https://github.com/vllm-project/vllm/commit/b5475d0534421df9eb93a67f046462cdaed43d1d>  
  - **模块/文件**：`vllm/transformers_utils/configs/qwen3_5*.py`（±2 行）  
  - **改动要点**：撤销对 Qwen3.5 配置的错误修改。  

- **[9521002]** – <https://github.com/vllm-project/vllm/commit/9521002f0acef67fa8d5ec61ad6bbdde64cde819>  
  - **模块/文件**：同上（`qwen3_5*.py`）  
  - **改动要点**：修正 Qwen3.5 配置文件中的小错误。  

- **[ec17bdd]** – <https://github.com/vllm-project/vllm/commit/ec17bdd8940798a5e74dc83ed489aacfbc32736d>  
  - **模块/文件**：`vllm/inputs/*.py`、`vllm/model_executor/models/*.py`、`vllm/multimodal/*.py`、`vllm/renderers/base.py`（共约 345 行改动）  
  - **改动要点**：将 `InputPreprocessor` 移入 Renderer，简化渲染管线并清理旧代码。  

- **[bb59c90]** – <https://github.com/vllm-project/vllm/commit/bb59c902480ddb054e7f3f0762b386e0d4e269bd>  
  - **模块/文件**：`.buildkite/image_build/image_build.sh`（+6 行，-1 行）  
  - **改动要点**：将 bake 配置写入临时目录，避免对仓库根目录的写入。  

- **[5bff999]** – <https://github.com/vllm-project/vllm/commit/5bff999d12dd061c102381b0c9c5d364c5953ea2>  
  - **模块/文件**：`vllm/lora/layers/fused_moe.py`（+3 行，-2 行），`vllm/model_executor/layers/fused_moe/layer.py`（+18 行，-10 行）  
  - **改动要点**：新增 `swap_quant_method` 方法，解决 LoRA 与 FusedMoE 的兼容性问题。  

- **[bb85929]** – <https://github.com/vllm-project/vllm/commit/bb85929aa6f3790a4fc4eae2b6504c1e0d8e4ffc>  
  - **模块/文件**：`cmake/external_projects/flashmla.cmake`（±1 行）  
  - **改动要点**：兼容 Python 3.13 的 FlashMLA 导入路径。  

- **[5653021]** – <https://github.com/vllm-project/vllm/commit/56530210944675b6adde20dae1ba6d224f68baf2>  
  - **模块/文件**：`docs/features/batch_invariance.md`（+1 行）  
  - **改动要点**：在批次不变性文档中加入 Mistral‑7b‑v0.3 示例。  

- **[974d829]** – <https://github.com/vllm-project/vllm/commit/974d829b0532a27d55ac625271a4149225dec5ba>  
  - **模块/文件**：`tests/entrypoints/openai/test_openai_schema.py`（+1 行），`vllm/entrypoints/anthropic/protocol.py`（+7 行，-1 行）  
  - **改动要点**：当 `tool_choice` 参数非法时返回 422 而非 500，提升前端错误语义。  

- **[91ac5d9]** – <https://github.com/vllm-project/vllm/commit/91ac5d9bfda99745ece40f5258f17a4c0585db40>  
  - **模块/文件**：`tests/models/*`、`vllm/model_executor/models/interns1_pro.py`（若干行）  
  - **改动要点**：开启对最新 Day‑0 模型的测试，确保新模型兼容性。  

---

**Issues**  

- **[Bug: Speculative Decoding (MTP) Causes Detection Failure]** – <https://github.com/vllm-project/vllm/issues/34650>  
  - **作者**：cicirori  
  - **概要**：在 MTP 推测解码配合结构化输出时，`` 标记未被检测，导致 `reasoning_ended` 永不为真，JSON 约束失效。  

- **[Orphan Processes on OOM]** – <https://github.com/vllm-project/vllm/issues/34643>  
  - **作者**：pymhq  
  - **概要**：父进程因 OOM 退出后，子进程未被回收，导致残留进程占用资源。  

- **[Incorrect Warning for Mixed Mistral & HF Formats]** – <https://github.com/vllm-project/vllm/issues/34642>  
  - **作者**：patrickvonplaten  
  - **概要**：当仓库同时包含 Mistral 与 HuggingFace 格式时，错误地触发警告。  

- **[ROCm Default AITER FP4BMM Crashes on MI300X]** – <https://github.com/vllm-project/vllm/issues/34641>  
  - **作者**：khairulkabir1661  
  - **概要**：默认开启 `VLLM_ROCM_USE_AITER_FP4BMM=True` 在不支持 FP4 的 MI300X 上导致运行时错误。  

- **[Qwen3‑Next‑80B‑A3B‑Instruct‑FP8 Random Symbols]** – <https://github.com/vllm-project/vllm/issues/34640>  
  - **作者**：stavinsky  
  - **概要**：在提交 `3bb4e43` 之后，FP8 量化的 Qwen3‑Next‑80B‑A3B‑Instruct 输出出现随机字符。  

- **[CI Failure: mi325_1 Entrypoints Integration Test (API Server 2)]** – <https://github.com/vllm-project/vllm/issues/34637>  
  - **作者**：AndreasKaratzas  
  - **概要**：`test_abort_metrics_reset` 在 AMD CI 环境中失败，已在 PR #34566 中修复。  

- **[AllReduceFusionPass Construction Takes >1 s]** – <https://github.com/vllm-project/vllm/issues/34635>  
  - **作者**：zou3519  
  - **概要**：`AllReduceFusionPass(config)` 初始化耗时过长，影响冷启动。  

- **[SharedStorageConnector Assertion on Blackwell GPUs]** – <https://github.com/vllm-project/vllm/issues/34634>  
  - **作者**：mmkamani7  
  - **概要**：在 Blackwell (B200) GPU 上执行 `vectorized_gather_kernel` 时触发断言，导致运行中断。
### NVIDIA/cutile-python
昨日无更新。

## 总结
### 展望与建议

- **加速器多样化**：随着 CUDA、ROCm 与 Blackwell GPU 的快速迭代，项目需继续同步更新 CI 镜像与依赖，确保新硬件的即插即用。 
- **MoE 与稀疏算子**：空 tile mbarrier 与 TMA block‑reduce 已显著提升稀疏前向的鲁棒性，建议在后续的模型库（如 Qwen‑3、DeepSeek）中统一采用这些实现，以降低维护成本。 
- **量化统一化**：FP8 与 MXFP4 已在多个项目落地，建议制定统一的量化接口规范（包括 KV‑scale、GDN 支持），避免因模型差异导致的随机符号或精度回退。 
- **监控与资源回收**：OOM 导致的孤儿进程与显存泄漏仍是高并发部署的痛点，建议在 `vllm` 与 `sglang` 中引入统一的进程守护与显存回收框架，配合更细粒度的日志（如 Diffusion VRAM 日志）进行预警。 
- **社区协作**：多个仓库的改动均围绕 MoE、量化与跨平台兼容展开，建议建立跨项目的技术共享渠道（如共用的 CuTe DSL、FlashInfer 量化库），以加速创新并降低重复实现的风险。
