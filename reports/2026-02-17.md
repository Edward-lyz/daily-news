今日的 AI Infra 的新闻如下。

## 摘要
```json
{
  "summary": "NVIDIA发布了针对Hopper架构的CuTeDSL BF16 MoE GEMM功能请求，同时修复了batched GEMM在非规则矩阵下的计算错误。FlashInfer集成了MxFP8 x MxFP8 MoE量化支持，优化了BatchDecode后端自动选择机制，并解决了FP4 AllReduce的不稳定性问题。SGLang完成了量化重构，改进了多种扩散模型性能，并添加了CUDA核心转储CI支持。vLLM简化了模型运行器V2的流水线并行实现，引入了基于Triton的高效Top-k/Top-p采样内核，并完善了输入预处理器到渲染器的迁移。",
  "conclusion": "值得关注的风险包括：vLLM中NVIDIA Llama-3.3-70B-Instruct-NVFP4在TRITON_ATTN模式下出现异常输出，以及Qwen3-Coder-Next-FP8在多GPU环境下导致系统冻结的问题。FlashInfer的trtllm_fp4_block_scale_moe存在NaN输出问题，已在更新驱动版本后解决。SGLang的Deepseek 3.2 NVFP4模型存在seqlens_k形状不匹配问题，需要关注后续修复进展。"
}
```

## 具体内容分析
### deepseek-ai/DeepGEMM
昨日无更新。
### deepseek-ai/FlashMLA
昨日无更新。
### NVIDIA/cutlass
## 提交

无提交记录。

## Issues

### [FEA] CuTeDSL BF16 Hopper MoE GEMM/Grouped GEMM example
- **作者**: nandor
- **链接**: https://github.com/NVIDIA/cutlass/issues/3040
- **内容简述**: 请求为 Hopper 架构添加 CuTeDSL 的 BF16 混合精度 MoE/GEMM 示例。当前仅有 C++ 版本，但 CuTeDSL 更易于集成和调整。
- **备注**: 该请求未提供具体代码或补丁，仅描述功能需求。

### [QST][CuTe] Unable to get batched gemm work by modifying the sgemm_2.cu example
- **作者**: zcbenz
- **链接**: https://github.com/NVIDIA/cutlass/issues/3039
- **内容简述**: 用户在修改 `sgemm_2.cu` 实现不规则形状的 batched GEMM 时遇到问题：当 batch size 大于 1 且 M 不规则时，除第一个 batch 外其余结果均为 0。
- **关键信息**:
  - 错误输出示例：
    ```
    !!Wrong result found at 69632: 0.000000 : -27.531250
    ```
  - 使用设备：NVIDIA GB10 (SM121)
- **备注**: Issue 包含部分代码片段，但由于 `body_truncated` 为 true，完整代码不可见。建议查看原始 issue 获取更多信息。
### flashinfer-ai/flashinfer
## 提交

### flashinfer-ai/flashinfer

#### 新增 MxFP8 x MxFP8 MoE 支持
- **PR**: [#2505](https://github.com/flashinfer-ai/flashinfer/pull/2505)
- 在 `trtllm_gen` MoE 中集成 MxFP8 x MxFP8 量化模式，支持 `Fp8QuantizationType.MxFp8` 类型。
- 修改了 Python 接口和 CUDA 核心代码以适配新的量化类型。例如在 `flashinfer/fused_moe/core.py` 中新增 `Fp8QuantizationType` 枚举，并更新相关函数签名：
  ```python
  # 原有 use_deepseek_fp8 参数被替换为 fp8_quantization_type
  def __init__(
      ...
      fp8_quantization_type: Fp8QuantizationType,
      ...
  )
  ```
- 更新基准测试脚本 `benchmarks/bench_trtllm_gen_fused_moe_autotuner.py` 以支持新量化选项。

#### 自动选择 BatchDecode 后端
- **PR**: [#2530](https://github.com/flashinfer-ai/flashinfer/pull/2530)
- 允许 `BatchDecodeWithPagedKVCacheWrapper` 使用 `"auto"` 后端自动选择最优实现（当前默认为 FA2）。
- 更新文档说明与命令行参数解析逻辑，在 `benchmarks/routines/attention.py` 中添加对 `auto` 的支持并调整帮助信息。

#### 修复 FP4 AllReduce 不稳定问题
- **Commit**: [432f343](https://github.com/flashinfer-ai/flashinfer/commit/432f3438286a20003f8fc568111cad222b6acf4d)
- 解决了在某些 SM 配置下 FP4 one-shot kernel launch config 导致的调度不稳定性问题。
- 引入基线配置保存机制并在 `include/flashinfer/comm/trtllm_allreduce_fusion.cuh` 中优化了集群大小调整策略。

#### 添加 Gated Delta Rule CuTe DSL 内核
- **PR**: [#2498](https://github.com/flashinfer-ai/flashinfer/pull/2498)
- 实现基于 CuTe DSL 的 GDN decode 内核，针对 T=1~4、bf16 状态进行优化。
- 新增模块 `flashinfer.cute_dsl.gated_delta_rule` 和对应基准测试文件 `benchmarks/bench_gdn_decode.py`。
- 包含性能对比功能，可比较 FlashInfer/CuTe DSL/Triton 多种布局下的表现差异。

## Issues

### RMSNorm+SiLU 融合核需求
- **Issue**: [#2571](https://github.com/flashinfer-ai/flashinfer/issues/2571)
- 请求为 VAE 模型集成融合的 RMSNorm+SiLU kernel 以提升推理效率。

### trtllm_fp4_block_scale_moe NaN 问题
- **Issue**: [#2569](https://github.com/flashinfer-ai/flashinfer/issues/2569)
- 在使用 MXFP8 激活量化且 `intermediate_size` 非 1024 对齐时，NVIDIA Driver 570 上出现间歇性 NaN 输出。
- 此问题已在更新驱动版本后解决。
### Dao-AILab/flash-attention
昨日无更新。
### sgl-project/sglang
提交：
- Fix eval tests not capturing server launch failures (#18886) [`34d975b`](https://github.com/sgl-project/sglang/commit/34d975b18fa5a01b0950735faac85b79dc57507b)
- 受影响文件: python/sglang/srt/model_loader/ci_weight_validation.py, python/sglang/test/nightly_utils.py, test/registered/eval/test_text_models_gsm8k_eval.py, test/registered/eval/test_vlms_mmmu_eval.py
- Refactor sampler: Use a better hash function for deterministic sampling and clear dispatch for probs/logprobs/logits sampling paths (#18915) [`e02a9be`](https://github.com/sgl-project/sglang/commit/e02a9bec8d9cc02ffd867b58a8edb33fe4ae880a)
- 受影响文件: python/sglang/srt/layers/sampler.py, python/sglang/srt/layers/utils/hash.py, python/sglang/test/send_one.py
- feat: add cuda core dump CI warpper (#18909) [`83a475e`](https://github.com/sgl-project/sglang/commit/83a475e8d7de58c032d3e13551208cc1d5549ea2)
- 受影响文件: .github/actions/upload-cuda-coredumps/action.yml, .github/workflows/nightly-test-nvidia.yml, .github/workflows/pr-test.yml, python/sglang/srt/debug_utils/cuda_coredump.py, python/sglang/srt/environ.py, python/sglang/test/ci/ci_utils.py, test/registered/debug_utils/test_cuda_coredump_smoke.py
- cleanup prefill metrics logging to fix dp-attn metrics (#18778) [`9a7d6be`](https://github.com/sgl-project/sglang/commit/9a7d6be56775307bada7cadc5513cea733feccf7)
- 受影响文件: python/sglang/srt/disaggregation/decode.py, python/sglang/srt/disaggregation/prefill.py, python/sglang/srt/managers/scheduler.py, python/sglang/srt/managers/scheduler_dp_attn_mixin.py, python/sglang/srt/managers/scheduler_metrics_mixin.py, python/sglang/srt/managers/scheduler_output_processor_mixin.py, python/sglang/srt/managers/scheduler_pp_mixin.py, python/sglang/srt/metrics/collector.py
- Fix benchmark_sglang_fused_moe_triton.py (#18940) [`355127c`](https://github.com/sgl-project/sglang/commit/355127c2e98e5bf62d22f4d814796891b47e86ab)
- 受影响文件: benchmark/kernels/fused_moe_triton/benchmark_sglang_fused_moe_triton.py
- Fix generated-shared-prefix bench_serving (#18769) [`3c601db`](https://github.com/sgl-project/sglang/commit/3c601db0312abf824c3aa8523183c138a32b3f53)
- 受影响文件: python/sglang/bench_serving.py
- fix(glm-image): single-GPU T5 config + SP support for 4D latents (#18… (#18739) [`48fcd62`](https://github.com/sgl-project/sglang/commit/48fcd62d1f60110a4656805468dbd38e20c12bef)
- 受影响文件: python/sglang/multimodal_gen/configs/pipeline_configs/base.py, python/sglang/multimodal_gen/configs/pipeline_configs/glm_image.py, python/sglang/multimodal_gen/runtime/models/dits/glm_image.py
- [3/N] Quantization Refactor: ModelSlim MoE schemes (#17993) [`aeca7d3`](https://github.com/sgl-project/sglang/commit/aeca7d348c6bce7af5ae8eb198f07be495f49295)
- 受影响文件: python/sglang/srt/layers/quantization/modelslim/modelslim.py, python/sglang/srt/layers/quantization/modelslim/schemes/__init__.py, python/sglang/srt/layers/quantization/modelslim/schemes/modelslim_scheme.py, python/sglang/srt/layers/quantization/modelslim/schemes/modelslim_w4a4_int4.py, python/sglang/srt/layers/quantization/modelslim/schemes/modelslim_w4a8_int8_moe.py, python/sglang/srt/layers/quantization/modelslim/schemes/modelslim_w8a8_int8.py, python/sglang/srt/layers/quantization/modelslim/schemes/modelslim_w8a8_int8_moe.py
- [diffusion] update code owner (#18495) [`10569d0`](https://github.com/sgl-project/sglang/commit/10569d04bb2028f74ef453011ec66487b940ec4a)
- 受影响文件: .github/CODEOWNERS
- [gRPC] Fix scheduler startup broken by context parallel refactor (#18933) [`bf08d3f`](https://github.com/sgl-project/sglang/commit/bf08d3f43c3d8a62dd1c9f8f4044b19e6a2afb2c)
- 受影响文件: python/sglang/srt/grpc/scheduler_launcher.py
- [diffusion] improve: improve torch.compile for MOVA (#18914) [`504b2c5`](https://github.com/sgl-project/sglang/commit/504b2c58cf94db16813745383dc71750a4a816aa)
- 受影响文件: python/sglang/multimodal_gen/runtime/pipelines_core/stages/model_specific_stages/mova.py
- [PCG] support piecewise cuda graph for kimi-linear model (#18849) [`bf52388`](https://github.com/sgl-project/sglang/commit/bf523883545917d225cd5839d0ac8b0d715c5515)
- 受影响文件: python/sglang/srt/layers/attention/hybrid_linear_attn_backend.py, python/sglang/srt/layers/radix_linear_attention.py, python/sglang/srt/models/kimi_linear.py, test/registered/models/test_kimi_linear_models_pcg.py
- Revert "[diffusion] operator: unify rotary embedding impl" (#18929) [`bfe34c9`](https://github.com/sgl-project/sglang/commit/bfe34c90ffe57a78b5c2556c504292026e388a61)
- 受影响文件: python/sglang/multimodal_gen/runtime/layers/rotary_embedding.py, python/sglang/multimodal_gen/runtime/layers/triton_ops.py, python/sglang/multimodal_gen/runtime/models/bridges/mova_dual_tower.py, python/sglang/multimodal_gen/runtime/models/dits/causal_wanvideo.py, python/sglang/multimodal_gen/runtime/models/dits/flux.py, python/sglang/multimodal_gen/runtime/models/dits/flux_2.py, python/sglang/multimodal_gen/runtime/models/dits/glm_image.py, python/sglang/multimodal_gen/runtime/models/dits/hunyuanvideo.py, python/sglang/multimodal_gen/runtime/models/dits/mova_audio_dit.py, python/sglang/multimodal_gen/runtime/models/dits/mova_video_dit.py, python/sglang/multimodal_gen/runtime/models/dits/qwen_image.py, python/sglang/multimodal_gen/runtime/models/dits/wanvideo.py, python/sglang/multimodal_gen/runtime/models/dits/zimage.py, python/sglang/multimodal_gen/runtime/pipelines_core/stages/model_specific_stages/mova.py
- [Diffusion] [NPU] Fix CI run (#18921) [`2aa0db7`](https://github.com/sgl-project/sglang/commit/2aa0db7d9cfadc5f0a596328a641a6f09d5887e3)
- 受影响文件: .github/workflows/pr-test-npu.yml, python/sglang/multimodal_gen/runtime/managers/gpu_worker.py, python/sglang/multimodal_gen/test/server/ascend/perf_baselines_npu.json, python/sglang/multimodal_gen/test/server/ascend/testcase_configs_npu.py
- ROCm use rotary_embedding from sgl-kernel (#18920) [`8bb1037`](https://github.com/sgl-project/sglang/commit/8bb103779624bb899dfe02d6ee9eab4b5ad8b4de)
- 受影响文件: python/sglang/srt/layers/rotary_embedding.py
- [Diffusion] Fix get model name when model local path end with "/" (#18918) [`5f81ec1`](https://github.com/sgl-project/sglang/commit/5f81ec1ad5212be474c41b8140f0580891679227)
- 受影响文件: python/sglang/multimodal_gen/registry.py
- [diffusion]: fix sparse video gen 2 backend being applied to cross-attention (#18900) [`f6cc024`](https://github.com/sgl-project/sglang/commit/f6cc02489fb26af8eb6704170821359b6edb91c7)
- 受影响文件: python/sglang/multimodal_gen/runtime/models/dits/wanvideo.py, python/sglang/multimodal_gen/runtime/platforms/interface.py
- Revert "[AMD] Fix RotaryEmbedding crash on AMD/ROCm (regression from #17934)" (#18922) [`b158f5d`](https://github.com/sgl-project/sglang/commit/b158f5d4a2b57512e5e75a5d7aa1a2c15a4c3e90)
- 受影响文件: python/sglang/srt/layers/rotary_embedding.py
- [Diffusion] [NPU] [Doc] Add NPU documentation for sglang-diffusion (#18894) [`14c95d2`](https://github.com/sgl-project/sglang/commit/14c95d255c1d0f43fe96f592339cf00eb197e2ec)
- 受影响文件: docs/diffusion/index.md, docs/diffusion/installation.md, docs/diffusion/performance/attention_backends.md
- [TBO] fix cuda graph intermittently becomes disabled bug (#18320) [`899e2be`](https://github.com/sgl-project/sglang/commit/899e2be7d06047aa95b16abf06e553148c1418f5)
- 受影响文件: python/sglang/srt/batch_overlap/two_batch_overlap.py
- [AMD] Fix RotaryEmbedding crash on AMD/ROCm (regression from #17934) (#18903) [`5e3103a`](https://github.com/sgl-project/sglang/commit/5e3103a7872c645012ce316bc87da41a7cbaae52)
- 受影响文件: python/sglang/srt/layers/rotary_embedding.py
- [Tiny] Fix assert syntax warning in compressed_tensors_w4a4_mxint4_moe.py (#18899) [`90a0d66`](https://github.com/sgl-project/sglang/commit/90a0d66e1e46f67ceba1c1c5061180e6af1cc748)
- 受影响文件: python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_mxint4_moe.py
- Skip flaky test_tool_choice_required_non_streaming for Mistral (#18889) [`7e41ac6`](https://github.com/sgl-project/sglang/commit/7e41ac6c8db6496820f8c9f404db67f1973fd121)
- 受影响文件: test/registered/openai_server/function_call/test_tool_choice.py
- [misc] adding metadata field in UpdateWeightFromDiskReqInput (#18821) [`d5307ce`](https://github.com/sgl-project/sglang/commit/d5307ce022d0af0790b06af48a77465db0458d6f)
- 受影响文件: python/sglang/srt/managers/io_struct.py
- [diffusion] operator: unify rotary embedding impl (#18164) [`26b2c63`](https://github.com/sgl-project/sglang/commit/26b2c63d03665ab70524599c2e146b37f589f1a7)
- 受影响文件: python/sglang/multimodal_gen/runtime/layers/rotary_embedding.py, python/sglang/multimodal_gen/runtime/layers/triton_ops.py, python/sglang/multimodal_gen/runtime/models/bridges/mova_dual_tower.py, python/sglang/multimodal_gen/runtime/models/dits/causal_wanvideo.py, python/sglang/multimodal_gen/runtime/models/dits/flux.py, python/sglang/multimodal_gen/runtime/models/dits/flux_2.py, python/sglang/multimodal_gen/runtime/models/dits/glm_image.py, python/sglang/multimodal_gen/runtime/models/dits/hunyuanvideo.py, python/sglang/multimodal_gen/runtime/models/dits/mova_audio_dit.py, python/sglang/multimodal_gen/runtime/models/dits/mova_video_dit.py, python/sglang/multimodal_gen/runtime/models/dits/qwen_image.py, python/sglang/multimodal_gen/runtime/models/dits/wanvideo.py, python/sglang/multimodal_gen/runtime/models/dits/zimage.py, python/sglang/multimodal_gen/runtime/pipelines_core/stages/model_specific_stages/mova.py
- Adapt the Qwen2Model._update_causal_mask for transformers==4.57.1 (#18774) [`b21390f`](https://github.com/sgl-project/sglang/commit/b21390f8f396f734a0d516009fa2c87f2b5e645b)
- 受影响文件: python/sglang/srt/models/deepseek_ocr.py
- [diffusion]: fix scheduler crash on ZMQ messages with unexpected frame counts (#17890) [`50ca24a`](https://github.com/sgl-project/sglang/commit/50ca24aebb7d2dfcd63f2759d43819845ed597c6)
- 受影响文件: python/sglang/multimodal_gen/runtime/managers/scheduler.py
Issues：
- [Bug] Deepseek 3.2 NVFP4 SpecV2 seqlens_k must have shape (batch_size) (https://github.com/sgl-project/sglang/issues/18943)
- Issue 内容已截断。
- [Diffusion]Support for /metrics endpoint in SGLang Diffusion (Qwen-Image) (https://github.com/sgl-project/sglang/issues/18935)
### vllm-project/vllm
## vLLM 项目更新 2026-02-17

### 主要更新

1. **[Model Runner V2] 进一步简化流水并行 (PP)**
   - 移除了 `pp_handler.py` 文件，新增 `pp_utils.py`
   - 简化了模型运行器中的 PP 实现
   - PR: #34724

2. **[内核] 基于 Triton 的 Top-k 和 Top-p 采样器内核**
   - 新增高性能 Triton 实现 `topk_topp_triton.py`
   - 包含基准测试和测试用例
   - PR: #33538

3. **[渲染器] 将 InputPreprocessor 移入渲染器 (2/2)**
   - 完成了 InputPreprocessor 到渲染器的迁移
   - 重构了多个服务相关文件
   - PR: #34560

4. **[核心] 模型运行器 V2 支持流水并行**
   - 为模型运行器 V2 添加了流水并行支持
   - 新增 `pp_handler.py` 处理器
   - PR: #33960

5. **[模型运行器 V2] 支持坏词采样参数**
   - 新增 `bad_words.py` 模块
   - 实现了坏词过滤功能
   - PR: #33433

### 问题与修复

1. **[Bug] NVIDIA Llama-3.3-70B-Instruct-NVFP4 在 TRITON_ATTN 下输出异常**
   - 问题: 使用 TRITON_ATTN 时产生乱码输出
   - Issue: #34759

2. **[Bug] Qwen3-Coder-Next-FP8 在多 GPU 张量并行下导致系统冻结**
   - 问题: 工具调用功能在多 GPU 环境下导致系统硬冻结
   - Issue: #34755

3. **[性能] 通过交换活跃词缀改进 swap_states 性能**
   - 建议: 只交换活跃词缀而非完整行，减少开销
   - Issue: #34731

4. **[功能] 添加 --force-close-connections 标志到 vllm bench serve**
   - 建议: 添加强制关闭连接的选项，便于负载均衡器后测试
   - Issue: #34746

### 其他更新

- 修复了稀疏 MLA 与 MTP 在 FULL cudagraphs 下运行的问题
- 移除了 8 位推理路径中的废弃 bitsandbytes 代码
- 为 Ray 工作者添加了第三方环境变量传播功能
- 修复了多个 CI 问题和小错误
### NVIDIA/cutile-python
提交：
- Add support for custom scan via `ct.scan` [`2117029`](https://github.com/NVIDIA/cutile-python/commit/211702952d2d5578a045174b2c1eda10416ddf86)
- 受影响文件: changelog.d/custom-scan.md, docs/source/operations.rst, src/cuda/tile/__init__.py, src/cuda/tile/_ir/ir.py, src/cuda/tile/_ir/ops.py, src/cuda/tile/_ir2bytecode.py, src/cuda/tile/_passes/alias_analysis.py, src/cuda/tile/_passes/code_motion.py, src/cuda/tile/_passes/dce.py, src/cuda/tile/_stub.py, test/test_scan.py
Issues：
- [BUG]: PTX in Nsight Compute (https://github.com/NVIDIA/cutile-python/issues/73)

## 总结
- Diff 内容已截断以满足 prompt 预算。
- OpenRouter repo summarize failed for sgl-project/sglang: OpenRouter 429 Too Many Requests (z-ai/glm-4.5-air:free): {"error":{"message":"Provider returned error","code":429,"metadata":{"raw":"z-ai/glm-4.5-air:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations","provider_name":"Z.AI","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
- OpenRouter repo summarize failed for NVIDIA/cutile-python: OpenRouter 429 Too Many Requests (z-ai/glm-4.5-air:free): {"error":{"message":"Provider returned error","code":429,"metadata":{"raw":"z-ai/glm-4.5-air:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations","provider_name":"Z.AI","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
