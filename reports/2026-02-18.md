今日的 AI Infra 的新闻如下。

## 摘要
2026年2月18日，AI基础设施领域多个关键项目更新：FlashInfer在FP4量化与MoE支持方面持续优化，并修复了多个稳定性问题；vLLM推进Model Runner V2改进、MoE模块重构及ROCm兼容性增强；SGLang新增对Qwen3.5的支持并完善LoRA和Diffusion功能；NVIDIA CUTLASS移除了mixed_input_fmha相关代码。此外，社区开始关注Blackwell架构（SM120）上的FP8/NVFP4兼容性和性能表现。

## 具体内容分析
### deepseek-ai/DeepGEMM
昨日无更新。
### deepseek-ai/FlashMLA
提交  
- 暂无对应的 PR 或 commit（当前查询未发现相关代码变动）。

Issues  
- **标题**: [Question] why not using FP8 rope in sparse FP8 decoding?  
  **作者**: lyppg  
  **创建时间**: 2026‑02‑18 20:09:35 UTC  
  **链接**: https://github.com/deepseek-ai/FlashMLA/issues/165  
  **摘要**: 询问为何在稀疏 FP8 解码 kernel 中仍将 KV‑cache 从 FP8 反量化为 bf16 再做 rope，而不直接使用 FP8 rope（如 FlashInfer 所示），以避免额外的 dequantize 开销并保持精度。
### NVIDIA/cutlass
提交：
- remove mixed_input_fmha_prefill (#3041) [`3476ddb`](https://github.com/NVIDIA/cutlass/commit/3476ddb7bd6ca4161a0169103ceaa20ce0eb891f)
- 受影响文件: examples/python/CuTeDSL/blackwell/mixed_input_fmha/mixed_input_fmha_prefill.py
Issues：
- resolved (https://github.com/NVIDIA/cutlass/issues/3042)
### flashinfer-ai/flashinfer
## 仓库更新

### 修复提交

1. **修复 W4A8 自动调优崩溃** (#2564)
   - 作者: ipnon
   - 修改文件:
     - `csrc/fused_moe/cutlass_backend/cutlass_fused_moe_kernels.cuh`: 修复了 INT4 权重和 FP8 激活的自动调优中的 dtype_bytes 计算问题 (9 行新增, 6 行删除)
     - `tests/moe/test_trtllm_cutlass_fused_moe.py`: 添加了 use_autotune 参数的测试用例 (16 行新增, 12 行删除)
   - 提交链接: https://github.com/flashinfer-ai/flashinfer/commit/127699c2ca45521f7aa1dd8c3c653a78b363dc6a

2. **修复 Spark 单元测试失败** (#2573)
   - 作者: kahyun
   - 修改文件:
     - `flashinfer/cute_dsl/__init__.py`: 移除了 gated_delta_rule 相关导入 (7 行删除)
     - `flashinfer/cute_dsl/add_rmsnorm_fp4quant.py`: 添加了集群模式下的同步机制 (8 行新增)
   - 提交链接: https://github.com/flashinfer-ai/flashinfer/commit/5c484080087754bb46181bbf9e22b60045c00c34

### 问题报告

1. **功能请求: 确定性 top-k 内核** (#2584)
   - 作者: yzh119
   - 创建时间: 2026-02-18T20:58:47Z
   - 内容: 请求为稀疏注意力添加确定性 top-k 内核，解决 RL 后训练流水线中的非确定性问题
   - 注意: 问题内容被截断

2. **架构 sm87 不支持** (#2579)
   - 作者: EricEttes
   - 创建时间: 2026-02-18T08:05:53Z
   - 内容: FlashInfer 不支持 NVIDIA SM87 架构 (Jetson Orin AGX 使用)，但通过设置环境变量可以 workaround
   - 注意: 问题内容完整

3. **NVFP4 mm_fp4 GEMM 在 SM120 上损坏** (#2577)
   - 作者: isensez
   - 创建时间: 2026-02-18T02:48:35Z
   - 内容: 在 SM120 (RTX PRO 6000 Blackwell) 上，所有 NVFP4 mm_fp4 GEMM 后端都失败
   - 注意: 问题内容被截断

4. **CUDA 异常: Warp 越界地址** (#2575)
   - 作者: YangXu1990uiuc
   - 创建时间: 2026-02-18T02:02:18Z
   - 内容: 与 DeepSeek 路由相关的 CUDA 异常
   - 注意: 问题内容完整，但仅包含 NVIDIA Bugzilla 链接
### Dao-AILab/flash-attention
# 2026-02-18 AI 基础设施更新

## Dao-AILab/flash-attention

- **Bump to 4.4.0 cute dsl pin** ([#2262](https://github.com/Dao-AILab/flash-attention/pull/2262))
  - 作者: Driss Guessous
  - 更新了 flash_attn/cute/pyproject.toml 中的依赖版本
  - 将 `nvidia-cutlass-dsl>=4.4.0.dev1` 更新为 `nvidia-cutlass-dsl>=4.4.0`
### sgl-project/sglang
**提交**

- **e2fccb2** – Fix flaky Qwen3-Next KL divergence tests by reverting mamba slot release  
  <https://github.com/sgl-project/sglang/commit/e2fccb2ee0a0211b75961e40009fe02133fa9983>  
  *修改文件*: `python/sglang/srt/managers/scheduler.py`  
  ```diff
  - self.maybe_release_mamba_cache(req)
  ```  
  （删除了在调度失败时释放 mamba 缓存的调用）

- **2f592c3** – Add `flashinfer_deepgemm` to `--fp8-gemm-backend`  
  <https://github.com/sgl-project/sglang/commit/2f592c3b181b7d75dbab8cc8f95806ee78e6ff01>  
  *修改文件*: `docs/advanced_features/server_arguments.md`  
  ```diff
  -| `--fp8-gemm-backend` | Choose the runner backend for Blockwise FP8 GEMM operations. …
  +| `--fp8-gemm-backend` | Choose the runner backend … `flashinfer_deepgemm` added
  ```

- **4f980f6** – Implement `update_weights_from_disk` for SGLang‑D (Diffusion)  
  <https://github.com/sgl-project/sglang/commit/4f980f6f233f7fff264006c2db5a7467cdb15a01>  
  *新增文件*: `python/sglang/multimodal_gen/runtime/entrypoints/post_training/weights_api.py`（FastAPI 路由实现）  
  *修改文件*: `python/sglang/multimodal_gen/runtime/entrypoints/http_server.py`  
  ```diff
  +from sglang.multimodal_gen.runtime.entrypoints.post_training import weights_api
  …
  +app.include_router(weights_api.router)
  ```

- **150ed88** – Quantization Refactor: Quark MoE schemes  
  <https://github.com/sgl-project/sglang/commit/150ed881be2cf9a9e64b636226b3c6189d341a7d>  
  *修改文件*: `python/sglang/srt/layers/moe/ep_moe/layer.py`（更换 Quark 导入）  
  ```diff
  -from sglang.srt.layers.quantization.quark.quark_moe import QuarkW4A4M…
  +from sglang.srt.layers.quantization.quark.quark_moe import QuarkW4A4M…
  ```  
  *新增文件*: `python/sglang/srt/layers/quantization/quark/schemes/quark_w4a4_mxfp4_moe.py`（MoE 专用方案）  

- **9c5aae4** – Add LoRA tied LM head support (for Qwen2.5, Gemma, etc.)  
  <https://github.com/sgl-project/sglang/commit/9c5aae4df55236214a72d9b60db2250c85583e27>  
  *修改文件*: `python/sglang/srt/lora/lora.py`（实现 tied‑head 逻辑）  
  *新增测试*: `test/registered/lora/test_lora_tied_lm_head.py`  

- **5a7ae05** – Add DP ViT support for Kimi K2.5  
  <https://github.com/sgl-project/sglang/commit/5a7ae059e37f2c481462a2ad965467264718c461>  
  *修改文件*: `python/sglang/srt/models/kimi_k25.py`（加入 DP‑ViT 相关层）  

- **eb6ff4a** – Refactor task resolution logic in benchmark function for multimodal generation  
  <https://github.com/sgl-project/sglang/commit/eb6ff4a94004ccb9d55d8d5c10ffb8b5f94e8e57>  
  *修改文件*: `python/sglang/multimodal_gen/benchmarks/bench_serving.py`（任务解析抽象化）  

- **0215d47** – ROCm 7.2: Add `/sgl-workspace/aiter` to `PYTHONPATH`  
  <https://github.com/sgl-project/sglang/commit/0215d470070621cddf32483cbede5a8b5bab2742>  
  *修改文件*: `docker/rocm720.Dockerfile`（ENV PYTHONPATH 增加 aiter）  

- **90d5e27** – Enable fa3 PDL by compiling it with corresponding flags  
  <https://github.com/sgl-project/sglang/commit/90d5e27f795de9fe86704ff7335ec93c6a9fc0d9>  
  *修改文件*: `sgl-kernel/CMakeLists.txt`（添加 fa3‑PDL 编译选项）  

- **390c154** – Tiny fix: mul_add naive forward bug  
  <https://github.com/sgl-project/sglang/commit/390c154306162b20ab9d98c4268bbecac83bb690>  
  *修改文件*: `python/sglang/multimodal_gen/runtime/layers/elementwise.py`（一行修正）  

- **513c12d** – Remove unused fast‑hadamard‑transform PyTorch extension sources  
  <https://github.com/sgl-project/sglang/commit/513c12d23f349f62afa02c7b57ebe6f07052e1c5>  
  *删除文件*: `python/sglang/jit_kernel/csrc/fast-hadamard-transform/fast_hadamard_transform.cpp`（321 行）  
  *删除文件*: `python/sglang/jit_kernel/csrc/fast-hadamard-transform/fast_hadamard_transform_cuda.cu`（452 行）  

- **934b366** – Reasoning models fix docs  
  <https://github.com/sgl-project/sglang/commit/934b36693c03ba6ecdce55ae115de132ff50b4bb>  
  *修改文件*: `docs/developer_guide/evaluating_new_models.md`（微调一行文字）  

- **420a611** – Refactor: unify `SamplingParams` construction & improve `DiffGenerator` return types  
  <https://github.com/sgl-project/sglang/commit/420a6112754076716ee1fd7d2dbc6bf02d70551f>  
  *涉及文件*: `configs/pipeline_configs/base.py`, `entrypoints/cli/generate.py`, `entrypoints/diffusion_generator.py`, `entrypoints/http_server.py`, `entrypoints/openai/*`, `runtime/managers/scheduler.py`, `runtime/pipelines_core/*`（共 10+ 文件，主要改动为统一参数构造与返回结构）  

- **9d13868** – Fix test and clean up hicache code  
  <https://github.com/sgl-project/sglang/commit/9d138685c1c083321638f5594a4600631c56594a>  
  *修改文件*: `python/sglang/jit_kernel/benchmark/bench_hicache.py`（测试修正）  
  *修改文件*: `python/sglang/jit_kernel/csrc/hicache.cuh`（实现细节清理）  
  *修改文件*: `python/sglang/jit_kernel/hicache.py`（小幅改动）  

- **95c44ce** – Add `return_routed_experts` param to `async_generate` for parity with `generate`  
  <https://github.com/sgl-project/sglang/commit/95c44cea2945ee80f1b174fdbb88205b1556c14f>  
  *修改文件*: `python/sglang/srt/entrypoints/engine.py`（新增参数）  

- **ac0e493** – Add NSA and SWA disaggregation support with nixl  
  <https://github.com/sgl-project/sglang/commit/ac0e493329d017723fc1a3a097504a816d002612>  
  *修改文件*: `python/sgl.../srt/disaggregation/nixl/conn.py`（实现 NSA/SWA 逻辑）  

- **2d85f01** – Revert “Fix generated‑shared‑prefix bench_serving”  
  <https://github.com/sgl-project/sglang/commit/2d85f01d43aeaee0d6b6bb9254c001506b59148e>  
  *修改文件*: `python/sglang/bench_serving.py`（恢复原实现）  

- **fa5698d** – Qwen3.5: Support block‑wise FP8 quantization & model adaptation  
  <https://github.com/sgl-project/sglang/commit/fa5698d7916497288af8fe5a5b57bc4ee7e6fb37>  
  *修改文件*: `srt/layers/linear.py`（FP8 兼容）  
  *修改文件*: `srt/layers/quantization/fp8.py`（细节优化）  
  *修改文件*: `srt/models/qwen3_5_mtp.py`, `srt/models/qwen3_vl.py`（模型注册）  

- **83e24e2** – Expose `priority` parameter in `Engine.generate()` and `Engine.async_generate()`  
  <https://github.com/sgl-project/sglang/commit/83e24e2eb434df20031c511230d849d4f27bafd3>  
  *修改文件*: `srt/entrypoints/EngineBase.py`, `srt/entrypoints/engine.py`, `srt/entrypoints/http_server_engine.py`（新增 `priority` 参数）  

---

**Issues**

- **[Bug] deepseek 3.2 nvfp4 moe‑runner‑backend=flashinfer_trtllm illegal memory access**  
  <https://github.com/sgl-project/sglang/issues/18989> – 作者: *pdasgup* – 创建于: 2026‑02‑18T23:54:14Z  

- **[Bug] Qwen3‑Next, flashinfer attn backend - missing 3 required positional arguments : 'q', 'k', 'v'**  
  <https://github.com/sgl-project/sglang/issues/18983> – 作者: *Swipe4057* – 创建于: 2026‑02‑18T19:37:13Z  

- **[Feature] Debug update weights from disk of SGLang diffusion with parallelism**  
  <https://github.com/sgl-project/sglang/issues/18981> – 作者: *zhaochenyang20* – 创建于: 2026‑02‑18T19:26:07Z  

- **[Bug] GLM 5 Crashes at nsa_backend on B200**  
  <https://github.com/sgl-project/sglang/issues/18980> – 作者: *ynwang007* – 创建于: 2026‑02‑18T19:13:31Z  

- **[Feature] Profile the update weights from disk API of SGLang Diffusion**  
  <https://github.com/sgl-project/sglang/issues/18979> – 作者: *zhaochenyang20* – 创建于: 2026‑02‑18T17:02:54Z  

- **[Feature] Does EAGLE3 speculative decoding support Gemma target models?**  
  <https://github.com/sgl-project/sglang/issues/18971> – 作者: *Ofir408* – 创建于: 2026‑02‑18T09:58:27Z  

- **[Roadmap] [NPU] Sglang Diffusion on Ascend**  
  <https://github.com/sgl-project/sglang/issues/18967> – 作者: *ping1jing2* – 创建于: 2026‑02‑18T08:33:39Z  

- **[Bug] NVFP4 models produce NaN outputs on RTX PRO 6000 Blackwell (SM120)**  
  <https://github.com/sgl-project/sglang/issues/18954> – 作者: *isensez* – 创建于: 2026‑02‑18T02:56:52Z
### vllm-project/vllm
# vLLM Project Updates - February 18, 2026

## Notable Commits

### Model Runner V2 Enhancements
- **[PR #32771]** Support for piecewise & mixed CUDA graph (343 additions, 108 deletions)
- **[PR #34786]** Minor simplification for DCP (111 additions, 95 deletions)
- **[PR #34780]** Avoid prepare prefill kernel launch overhead (18 additions, 14 deletions)
- **[PR #34766]** Additional PP simplification (8 additions, 12 deletions)
- **[PR #34849]** Remove unnecessary copies in PW CUDA graph capture (1 addition, 3 deletions)

### MoE Improvements
- **[PR #34588]** Convert mxfp4 marlin into modular kernel format (42 additions, 23 deletions)
- **[PR #34673]** Fix incorrect routing selection for models without expert groups (32 additions, 96 deletions)

### Performance Optimizations
- **[PR #34758]** DeepSeek R1 BF16 Min Latency QKV A GEMM (0.5% E2E speedup)
- **[PR #34228]** Add unit tests for fp8 output fusion of triton_attn (127 additions, 1 deletion)

### Bug Fixes
- **[PR #34745]** Fix empty tool_call_id in Anthropic messages API tool result conversion
- **[PR #34834]** Fix lora tests (28 additions, 44 deletions)
- **[PR #34696]** Fix activation in cpu_fused_moe_torch call
- **[PR #34723]** Fix prefix creation for Qwen3.5
- **[PR #33255]** Fix quant RMS norm fusion for quantization with TMA-aligned scales (235 additions, 76 deletions)
- **[PR #34725]** Fix NVFP4 TRTLLM MoE non-gated support

### ROCm/AMD Support
- **[PR #34181]** Use torch.testing.assert_close instead of assert torch.allclose in test_rocm_skinny_gemms.py
- **[PR #34655]** Skip tests in test_unquantized_backend_selection that should not run on ROCm
- **[PR #34753]** Removed hard-coded attn backend requirement for Qwen VL

## Open Issues

### Feature Requests
- **[Issue #34851]** Refactor Quark MoE and mxfp4 MoE to align with MoE oracle/MK
- **[Issue #34827]** Support thinking budget for reasoning models (e.g., GLM-4.5V)

### Bug Reports
- **[Issue #34850]** MiniMax M2.5 MI355 compile error
- **[Issue #34845]** Qwen3-Next MTP fails when paired with chunked prefill
- **[Issue #34819]** CI failure: test_can_initialize_small_subset[Llama4ForConditionalGeneration]
- **[Issue #34814]** CI failure: test_can_initialize_large_subset[InternS1ProForConditionalGeneration]
- **[Issue #34812]** GraniteMoeHybridModel not applying embedding_multiplier to input embeddings
### NVIDIA/cutile-python
昨日无更新。

## 总结
当前各主流推理框架正加速适配新一代硬件（如Blackwell GPU）及其低精度计算特性（FP8/NVFP4），同时强化对MoE模型的支持。需特别注意新架构上出现的数值不稳定性和内存访问异常等潜在风险。
