今日的 AI Infra 的新闻如下。

## 摘要
本周 FlashInfer 新增 BF16 GEMM 基准测试支持并发布 v0.6.4 版本，同时为 TRTLLM-Gen MLA 启用 Skip-Softmax 注意力。SGLang 在 diffusion 模块中优化了内存使用和性能基线，并新增 SDAR 模型支持。vLLM 修复了 Qwen3.5 的多个问题，包括 Hopper 架构上的 FP8 kernel 兼容性和权重加载错误，同时改进了 ROCm 支持和 MoE 配置体验。

## 具体内容分析
### deepseek-ai/DeepGEMM
昨日无更新。
### deepseek-ai/FlashMLA
昨日无更新。
### NVIDIA/cutlass
昨日无更新。
### flashinfer-ai/flashinfer
提交：
- feat: BF16 GEMM benchmarking support (#2525) [`1d3ac82`](https://github.com/flashinfer-ai/flashinfer/commit/1d3ac82b01d608c235a81cf5ef24b659ba1e9be6)
- 受影响文件: README.md, benchmarks/README.md, benchmarks/routines/flashinfer_benchmark_utils.py, benchmarks/routines/gemm.py, flashinfer/gemm/gemm_base.py, tests/gemm/test_bmm_bf16.py, tests/gemm/test_mm_bf16.py
- bump version to 0.6.4 (#2565) [`f1e6fdc`](https://github.com/flashinfer-ai/flashinfer/commit/f1e6fdcb8f65104047697f022b5d055ef022d763)
- 受影响文件: CHANGELOG.md, version.txt
- docs: Add note on feature support for compute capabilities (#2578) [`0b85658`](https://github.com/flashinfer-ai/flashinfer/commit/0b8565854cfbaa750608d19176ae001801d6b104)
- 受影响文件: README.md
- feat: Enable TRTLLM-Gen Skip-Softmax attention for MLA (#2547) [`11537c7`](https://github.com/flashinfer-ai/flashinfer/commit/11537c7c8ad9b6c89b021c269c3144bb71897a36)
- 受影响文件: csrc/trtllm_fmha_kernel_launcher.cu, flashinfer/mla.py, flashinfer/prefill.py, tests/attention/test_trtllm_gen_attention.py, tests/attention/test_trtllm_gen_mla.py
- fix: allow fmha_v2_prefill_deepseek on SM121 (DGX Spark) (#2559) [`9e5ec6a`](https://github.com/flashinfer-ai/flashinfer/commit/9e5ec6a0c1f94c48a9593467fcf22b64b3eb2f92)
- 受影响文件: flashinfer/prefill.py, tests/attention/test_fmha_v2_prefill_deepseek.py
- Add Hopper to CI (#2552) [`1556d78`](https://github.com/flashinfer-ai/flashinfer/commit/1556d78d700b396bacd2c58e6aa377e0f860c202)
- 受影响文件: .github/workflows/pr-test.yml, scripts/task_analyze_spot.sh
Issues：
- FlashInfer is missing TRTLLM-GEN ragged kernel for BF16/FP16 -> FP8 output (MLA prefill 192/128) #2596 (https://github.com/flashinfer-ai/flashinfer/issues/2597)
- FlashInfer is missing TRTLLM-GEN ragged kernel for BF16/FP16 -> FP8 output (MLA prefill 192/128) (https://github.com/flashinfer-ai/flashinfer/issues/2596)
- [BUG] NaN in result of cutlass fp8 kernel on hopper (https://github.com/flashinfer-ai/flashinfer/issues/2595)
- FlashInfer API for sparsity updated cubins in nvfp4 moe (https://github.com/flashinfer-ai/flashinfer/issues/2589)
### Dao-AILab/flash-attention
## 提交

无提交记录。

## Issues

### [Cute][GB200] Segfault on test with cute dsl 4.4.0  
[链接](https://github.com/Dao-AILab/flash-attention/issues/2264)  
用户 `henrylhtsang` 报告在 GB200 设备上使用 cute DSL 4.4.0 运行测试时出现段错误（Segfault），B200 上则无此问题。通过运行 `pytest -x -s test_flash_attn.py` 可复现该问题，但具体原因尚不明确。由于 body_truncated 为 true，详细调试信息缺失。

### [Question] Numerics of e2e  
[链接](https://github.com/Dao-AILab/flash-attention/issues/2263)  
用户 `henrylhtsang` 提出关于 exp2 模拟数值精度的问题，询问其是否能保证与标准实现比特位完全一致。尽管此前有评论称 BF16 输出应是安全的，但用户的实验发现存在 **1 ULP** 的偏差。问题中附带了部分代码片段用于验证，但由于 body_truncated 为 true，完整上下文不可见。
### sgl-project/sglang
**提交**

| SHA | 标题 | 作者 | 日期 | 关键改动 | 关联文件（模块/文件） | 代码片段 |
|-----|------|------|------|----------|----------------------|----------|
| `db34c1c` | Tiny remove duplicate coredump env injection (#19023) | Liangsheng Yin | 2026‑02‑19 | 将原先使用 `logging` 的警告改为 `warnings.warn`，并去除重复的 CUDA coredump 环境变量注入逻辑。 | `python/sglang/srt/debug_utils/cuda_coredump.py`（模块 `debug_utils`） | ```python\nif key in os.environ:\n    warnings.warn(\n        f\"CUDA coredump env var {key} is already set to '{os.environ[key]}', skipping injection of '{value}'.\"\n    )\n``` |
| `5ff5aa6` | [spec v2]Fix torch gc of future indices (#18958) | Liangsheng Yin | 2026‑02‑19 | 为 `future_indices` 张量显式绑定当前 GPU 流，防止在 GC 前被回收。 | `python/sglang/srt/managers/overlap_utils.py`（模块 `overlap_utils`） | ```python\nindices.record_stream(torch.get_device_module(self.device).current_stream())\n``` |
| `44ab752` | Add SDAR model support (#18318) | chengshuang18 | 2026‑02‑19 | 新增 SDAR 与 SDAR‑MoE 两个模型实现，更新文档支持列表。 | `python/sglang/srt/models/sdar.py`、`python/sglang/srt/models/sdar_moe.py`（模型实现） | *文件较大，省略具体片段，主要实现 `class SDARModel` 与 `class SDARMoEModel`，并在 `config.py` 中注册。 |
| `3207427` | [diffusion] CI: enable warmup as default (#19010) | Mick | 2026‑02‑19 | CI 基准测试脚本默认开启 warm‑up，调整 perf 基线文件。 | `python/sglang/multimodal_gen/test/server/perf_baselines.json`（基线配置） | *无代码改动，仅 JSON 数据更新。 |
| `d73f06f` | [diffusion] chore: improve memory usage on consumer‑level GPU (#18997) | Mick | 2026‑02‑19 | 调整加载器工具函数，减少中间缓存；在服务器参数中加入 `max_batch_size` 限制。 | `python/sglang/multimodal_gen/runtime/loader/utils.py`、`python/sglang/multimodal_gen/runtime/server_args.py` | ```python\n# utils.py\nif total_mem > limit:\n    reduce_batch()\n``` |
| `963def7` | Move lora request validation to tokenizer_manager from server (#18962) | satyamk7054 | 2026‑02‑19 | 将 LoRA 参数校验迁移至 `TokenizerManager`，简化 OpenAI 接口层。 | `python/sglang/srt/managers/tokenizer_manager.py`（模块 `tokenizer_manager`） | ```python\ndef validate_lora(request):\n    # 检查 LoRA 名称、权重路径等\n``` |
| `d07e8aa` | [Diffusion] [NPU] Enable profiler on NPU (#17807) | Makcum888e | 2026‑02‑19 | 为 NPU 运行时加入 `Profiler`，记录 kernel 时序。 | `python/sglang/multimodal_gen/runtime/utils/profiler.py`（模块 `profiler`） | ```python\nprofiler = NPUProfiler()\nprofiler.start()\n``` |
| `e21fc78` | [diffusion] fix: fix rank used in parallel executor when enable_cfg_parallel is false (#18975) | Prozac614 | 2026‑02‑19 | 修正 `ParallelExecutor` 在 `enable_cfg_parallel=False` 时使用错误的 rank 参数。 | `python/sglang/multimodal_gen/runtime/pipelines_core/executors/parallel_executor.py` | ```python\nrank = 0 if not self.cfg_parallel else self.rank\n``` |
| `19aa19b` | [diffusion] refactor: refactor diffusion triton kernels (#18966) | Xiaoyu Zhang | 2026‑02‑19 | 重构 Triton kernel 目录：`norm.py` 被删除并拆分为 `rmsnorm_onepass.py`、`rotary.py`、`scale_shift.py` 等；新增 NPU fallback 实现。 | `python/sglang/jit_kernel/diffusion/triton/rmsnorm_onepass.py`、`.../rotary.py`、`.../scale_shift.py`、`.../npu_fallback.py`（各 kernel） | *示例（rmsnorm_onepass）*<br/>```python\n@triton.jit\ndef rmsnorm_onepass(x, weight, eps):\n    ...\n``` |
| `48642d5` | [RadixTree][4/N Refactor]: Move available_and_evictable_str to individual radix cache classes (#17852) | pansicheng | 2026‑02‑19 | 将 `available_and_evictable_str` 方法下沉至 `MambaRadixCache` 与 `SWARadixCache`，提升类职责单一性。 | `python/sglang/srt/mem_cache/mamba_radix_cache.py`、`.../swa_radix_cache.py`（缓存实现） | ```python\ndef available_and_evictable_str(self):\n    return f\"available={self.available}, evictable={self.evictable}\"\n``` |
| `82a0baf` | Feat/add fi selective state update kernel call (#18070) | shaharmor98 | 2026‑02‑19 | 新增 `ssu_dispatch.py` 实现 Fi（FlashInfer）选择性状态更新；在调度器与模型运行器中加入调用。 | `python/sglang/srt/layers/attention/mamba/ops/ssu_dispatch.py`（kernel 调度） | ```python\ndef dispatch_selective_update(state, mask):\n    if mask.any():\n        launch_kernel(state, mask)\n``` |
| `0be30d4` | Fix PCG MoE Error (#17739) | Yuwei An | 2026‑02‑19 | 修复 MoE 中 PCG（Parallel Compute Graph）导致的错误，主要在自定义 All‑Reduce 与 NCCL 通信层。 | `python/sglang/srt/distributed/device_communicators/custom_all_reduce.py`、`.../pynccl.py`（通信实现） | ```python\n# custom_all_reduce.py\ndef all_reduce(tensor):\n    torch.distributed.all_reduce(tensor, op=torch.distributed.ReduceOp.SUM)\n``` |
| `bba2fc4` | [Qwen3.5] Enable nvfp4 checkpoint (#18937) | hlu1 | 2026‑02‑19 | 为 Qwen‑3.5 模型开启 `nvfp4` 检查点支持，修改 rotary 嵌入与模型文件。 | `python/sglang/srt/models/qwen3_5.py`、`.../rotary_embedding.py`（模型与 rotary） | ```python\nself.checkpoint_type = "nvfp4"\n``` |
| `443b1a8` | Add batched zero copy to NIXL backend (#18850) | hxie | 2026‑02‑19 | 在 NIXL 存储后端实现批量 zero‑copy，提升数据搬迁效率。 | `python/sglang/srt/mem_cache/storage/nixl/hicache_nixl.py`（后端实现） | ```python\ndef batch_zero_copy(src, dst):\n    dst[:] = src\n``` |
| `4622679` | [AMD] Fix mi35x dsv32 mtp nightly (#18978) | Bingxu Chen | 2026‑02‑19 | 修正 AMD MI35x DSV32 MTP nightly 构建中的小错误，更新 `nsa_backend.py` 中的宏定义。 | `python/sglang/srt/layers/attention/nsa_backend.py`（后端） | ```python\n# 修正宏定义\n#define DSV32_ENABLED 1\n``` |

> **说明**：部分提交的 `patch_truncated` 为 `true`（如 `db34c1c`、`19aa19b` 等），因此只能提供概要和关键代码片段，完整 diff 未展示。

---

**Issues**

| 标题 | 链接 | 作者 | 创建时间 | 简要描述 |
|------|------|------|----------|----------|
| **[Feature] ROCm nightly in upstream lmsysorg docker org** | https://github.com/sgl-project/sglang/issues/19031 | functionstackx | 2026‑02‑19 23:50:29 | 请求将 ROCm nightly 镜像同步至 `lmsysorg/sglang` Docker 组织，以便 AMD 社区获得与 vLLM 类似的官方镜像支持。 |
| **[Bug] GLM5 nightly Mi355 broken due to transformer dependency** | https://github.com/sgl-project/sglang/issues/19028 | functionstackx | 2026‑02‑19 22:04:14 | 在 ROCm nightly 镜像 `rocm/sgl-dev:v0.5.8.post1-rocm720-mi35x-20260219` 中，`transformers` 版本不兼容导致 GLM5 运行失败，需在镜像构建脚本中固定 `transformers` 版本。 |
| **[Bug] Missing saving kv for LLaDA2** | https://github.com/sgl-project/sglang/issues/19019 | DarkSharpness | 2026‑02‑19 14:58:26 | 当 `enable_fused_set_kv_buffer=False` 时，`flashinfer` 后端跳过 `save_kv_cache`，导致 LLaDA2 测试失败。建议在对应代码块中移除 `if` 条件或补全 KV 保存逻辑。 |
| **[Bug] cutlass_fp4_group_mm does not support SM 110 (Jetson AGX Thor)** | https://github.com/sgl-project/sglang/issues/18994 | WiwilZ | 2026‑02‑19 03:06:38 | Jetson AGX Thor（SM110）缺少对 `cutlass_fp4_group_mm` 的调度分支，导致 kernel 启动失败。需要在 `sgl-kernel/csrc/moe/nvfp4_blockwise_moe.cu` 中加入 `sm_version == 110` 的分支。 |
| **[Feature] Support NVFP4 kernels for SM110 (Jetson AGX Thor)** | https://github.com/sgl-project/sglang/issues/18993 | wiwi-z | 2026‑02‑19 02:57:09 | 与上条 bug 对应的功能需求：在 Jetson AGX Thor 上启用 NVFP4 Tensor Core 支持，扩展 `nvfp4_blockwise_moe.cu` 的调度逻辑。 |

> **说明**：所有 Issue 均已提供完整 `body` 内容，未出现 `body_truncated`。若后续需要进一步跟进，请关注对应 Issue 编号。
### vllm-project/vllm
# vLLM Development Update - February 19, 2026

## Notable Commits

### MoE Configuration Improvements
- Enhanced user experience with more descriptive reasons in `is_supported_config` for Mixture of Experts ([f72061a](https://github.com/vllm-project/vllm/commit/f72061a19ae7fbb7f193c31f0abea355fab41892))
- Modified files across MoE implementation layers including `flashinfer_trtllm_moe.py`, `modular_kernel.py`, and `flashinfer_fp4_moe.py`

### Major Bug Fix for Basic Models
- Fixed issues in basic models test with significant changes to configuration and CUDA platform support ([662205d](https://github.com/vllm-project/vllm/commit/662205d34eb1bb42228768d7a69a1ac4abf38c89))
- Updates across multiple executor components and test files with 174 additions and 220 deletions

### Qwen3.5 Hopper Kernel Fix
- Resolved cutlass FP8 kernel issues on Hopper architecture specifically for Qwen3.5 models ([4fb8bee](https://github.com/vllm-project/vllm/commit/4fb8beefaa8b2c4bd2cd3b336b01ff006dc98bdc))
- Modified `flashinfer_utils.py` to fix compatibility issues

### ROCm Infrastructure Updates
- Fixed beam search determinism issues for AMD GPUs ([f6220f9](https://github.com/vllm-project/vllm/commit/f6220f98779463705e578562f307b2becea8b8b3))
- Removed blocking labels from MI355 until stable infrastructure is ready ([2df2bb2](https://github.com/vllm-project/vllm/commit/2df2bb27b0fd624d8abd0fda3f8c337f1e8c60fc))

### Model-Specific Fixes
- Fixed Qwen3.5 kv-scale weight remapping ([6fff24f](https://github.com/vllm-project/vllm/commit/6fff24f30fe2554f43871978ef59feaa87f245c0))
- Resolved DeepSeek V3 weight loading issue ([7f51e93](https://github.com/vllm-project/vllm/commit/7f51e93864709e436fab21c3f4103c49d198f999))
- Fixed Voxtral Realtime engine crash on empty multimodal embeddings ([f75b61a](https://github.com/vllm-project/vllm/commit/f75b61a9e9dfc15d7821b35b1f88f9482805202a))

### Infrastructure Improvements
- Deprecated test-pipeline.yaml configuration ([b6101d3](https://github.com/vllm-project/vllm/commit/b6101d384db5709b4422ebd05fe84f0891ff63ce))
- Fixed reasoning tokens for text-based parsers in Responses API ([9681068](https://github.com/vllm-project/vllm/commit/9681068cf995af3cf651f1150fbabd8604537660))

## Active Issues

### Embedding Compatibility
- vLLM embeddings don't match Hugging Face sentence-transformer outputs for Qwen3-Embedding-0.6B model ([#34910](https://github.com/vllm-project/vllm/issues/34910))

### Qwen3.5 FP8 Deployment Issues
- Fused linear layer sharding incompatibility with TP=4 ([#34893](https://github.com/vllm-project/vllm/issues/34893))
- FP8 accuracy degradation with FlashInfer CUTLASS MoE backend ([#34892](https://github.com/vllm-project/vllm/issues/34892))
- Device multicasting support errors ([#34891](https://github.com/vllm-project/vllm/issues/34891))

### Memory Management
- OOM during mxfp4 weight swizzle with sleep mode enabled ([#34877](https://github.com/vllm-project/vllm/issues/34877))
- Deepseek V3.1 NVFP4 weight loading failures ([#34869](https://github.com/vllm-project/vllm/issues/34869))

### Documentation Needs
- Speculators documentation requires updates with MTP details and performance benchmarks ([#34901](https://github.com/vllm-project/vllm/issues/34901))
### NVIDIA/cutile-python
昨日无更新。

## 总结
各大项目持续优化对新硬件（如 Hopper、ROCm）的支持，特别是在 FP8 和混合精度推理方面存在较多待解决的兼容性问题。FlashInfer 在 TRTLLM-Gen MLA 方面有积极进展但仍有 kernel 缺失；SGLang 正在增强 diffusion pipeline 的稳定性和效率；vLLM 则聚焦于模型特定的工程细节修正与部署稳定性提升。建议关注各项目中关于 cutlass、nvfp4 及 MoE 相关的 issue 进展。
