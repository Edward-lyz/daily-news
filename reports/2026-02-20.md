ä»Šæ—¥çš„ AI Infra çš„æ–°é—»å¦‚ä¸‹ã€‚

## æ‘˜è¦
æœ¬æœŸæ›´æ–°èšç„¦ **ç®—å­ç²¾åº¦æ‰©å±•**ï¼ˆå¦‚ Cutlass SM120 FP8 MMA æŠ¥é”™ã€FlashInfer æ–°å¢ FP4 GEMMã€vLLM æ”¯æŒ ROCm ä¸Šçš„ bitsandbytes é‡åŒ–ï¼‰ï¼Œä»¥åŠ **æ¶æ„é€‚é…ä¸ä¼˜åŒ–**ï¼ˆFlashâ€‘Attention é‡æ„æ¶æ„æšä¸¾ã€SGLang é‡æ„ benchmark ä¸ CUDAâ€‘graph åç§°ã€cutlass ä¸ flashinfer çš„ SM100/SM103 ä¼˜åŒ–ï¼‰ã€‚

**CI ä¸æµ‹è¯•** æ–¹é¢ï¼ŒFlashInfer ä¸ SGLang å‡åŠ å…¥å®¹å™¨æ¸…ç†ä¸ nightly runner æ¸…ç†æ­¥éª¤ï¼ŒvLLM ä¿®å¤äº† gRPC ç‰ˆæœ¬å†²çªå¹¶ç§»é™¤å¤±æ•ˆçš„é›†æˆæµ‹è¯•ã€‚ 

**å¼€å‘è€…ä½“éªŒ** é€šè¿‡æ–°å¢ issueâ€‘claim å·¥ä½œæµï¼ˆFlashInferï¼‰å’Œ BlockRestriction æ¥å£ï¼ˆcutileâ€‘pythonï¼‰æå‡åä½œæ•ˆç‡ï¼›SGLang å¼•å…¥ perâ€‘tokenâ€‘groupâ€‘quant 8bit JIT å†…æ ¸å¹¶éšåå›æ»šï¼Œå±•ç¤ºäº†å¿«é€Ÿå®éªŒ-å›æ»šçš„è¿­ä»£èŠ‚å¥ã€‚ 

**Bug ä¿®å¤** åŒ…æ‹¬ FlashInfer çš„ chunk_end è®¡ç®—ã€SGLang çš„ KVâ€‘cache è§„æ¨¡é—®é¢˜ã€vLLM çš„ KV åŠ è½½ç­–ç•¥é»˜è®¤æ”¹ä¸º failï¼Œä»¥åŠå¤šé¡¹è·¨æ¶æ„çš„å¯„å­˜å™¨/æŒ‡ä»¤é”™è¯¯ä¿®æ­£ã€‚

## å…·ä½“å†…å®¹åˆ†æ
### deepseek-ai/DeepGEMM
æ˜¨æ—¥æ— æ›´æ–°ã€‚
### deepseek-ai/FlashMLA
æ˜¨æ—¥æ— æ›´æ–°ã€‚
### NVIDIA/cutlass
æäº¤  
- æš‚æ—  PR æˆ– commitï¼ˆè¯¥ä»“åº“åœ¨æœ¬æ¬¡æ›´æ–°ä¸­æœªå‡ºç°ä»£ç æäº¤ï¼‰ã€‚

Issues  
- **CuTe DSL: FP8 MMA segfaults on SM120 â€” MmaAtomSM80Type missing kind::f8f6f4 lowering**  
  - ä½œè€…: blake-snc  
  - åˆ›å»ºæ—¶é—´: 2026â€‘02â€‘20 23:57:41â€¯UTC  
  - é“¾æ¥: https://github.com/NVIDIA/cutlass/issues/3044  
  - ç®€è¦: åœ¨ SM120ï¼ˆRTXâ€¯5090ã€GB10/DGXâ€¯Sparkã€`sm_121a`ï¼‰ä¸Šä½¿ç”¨ `MmaAtomSM80Type.get()` è¿›è¡Œ FP8 MMAï¼ˆ`Float8E4M3FN`ï¼Œå½¢çŠ¶ `(16, 8, 32)`ï¼‰æ—¶ï¼ŒMLIRâ†’PTX ç¼–è¯‘é˜¶æ®µå‡ºç°æ®µé”™è¯¯ã€‚æ ¹å› æ˜¯ MLIR åç«¯ç¼ºå°‘å¯¹ SM120 `mma.sync.aligned.kind::f8f6f4.m16n8k32` æŒ‡ä»¤å˜ä½“çš„ lowering æ”¯æŒã€‚  

> ç›®å‰å°šæœªæäº¤å¯¹åº”çš„ PRï¼Œåç»­ä¿®å¤é¢„è®¡ä¼šåœ¨æ–° PR ä¸­æä¾›ã€‚
### flashinfer-ai/flashinfer
## æäº¤

- **feat: cute dsl mmfp4 for blackwell**  
  PR: [#2540](https://github.com/flashinfer-ai/flashinfer/pull/2540)  
  - æ–°å¢ `cute-dsl` åç«¯æ”¯æŒ FP4 GEMMã€‚  
  - `benchmarks/routines/gemm.py`ï¼šå°† `cute-dsl` åŠ å…¥åç«¯é€‰é¡¹ã€è‡ªåŠ¨è°ƒä¼˜åˆ—è¡¨ã€‚  
  - `flashinfer/gemm/gemm_base.py`ï¼šåœ¨å‡½æ•°ç­¾åä¸­åŠ å…¥ `"cute-dsl"`ï¼Œæ–°å¢ `enable_pdl` å‚æ•°ï¼ˆæœªä½¿ç”¨ï¼‰ã€‚  
  - æ–°å¢ä¸¤å¥— kernel å®ç°ï¼š`flashinfer/gemm/kernels/dense_blockscaled_gemm_sm100.py`ã€`..._sm103.py`ï¼ˆé’ˆå¯¹ SM100/SM103ï¼‰ã€‚  
  - `tests/gemm/test_mm_fp4.py`ï¼šåŠ å…¥ `cute-dsl` å‚æ•°åŒ–ï¼Œæ·»åŠ é’ˆå¯¹è¯¥åç«¯çš„å‰ç½®æ¡ä»¶æ£€æŸ¥ã€‚  

- **tests: add bias testing to nvfp4 moe**  
  PR: [#2585](https://github.com/flashinfer-ai/flashinfer/pull/2585)  
  - åœ¨ `tests/moe/test_trtllm_gen_fused_moe.py` ä¸­åŠ å…¥ `gemm1_bias`ã€`gemm2_bias` å‚æ•°ä¼ é€’ï¼ŒéªŒè¯å¸¦ bias çš„ NVFP4 MoE è®¡ç®—æ­£ç¡®æ€§ã€‚  

- **ci: fix H100 cleanup**  
  PR: [#2590](https://github.com/flashinfer-ai/flashinfer/pull/2590)  
  - CI æ¸…ç†è„šæœ¬æ”¹ä¸ºä»…åœæ­¢å½“å‰ runner å¯åŠ¨çš„å®¹å™¨ï¼Œé˜²æ­¢è¯¯æ€å…±äº«èŠ‚ç‚¹ä¸Šçš„å…¶ä»–ä»»åŠ¡ã€‚  
  - å¢åŠ  `docker container prune`ï¼Œåˆ é™¤ä¸å¿…è¦çš„ GPU ä¿¡æ¯è¾“å‡ºã€‚  

- **ci: add cleanup step to nightly release selfâ€‘hosted runner jobs**  
  PR: [#2510](https://github.com/flashinfer-ai/flashinfer/pull/2510)  
  - åœ¨ nightlyâ€‘release å·¥ä½œæµçš„æ¯ä¸ª job å¼€å¤´åŠ å…¥ç»Ÿä¸€çš„å·¥ä½œåŒºä¸ Docker æ¸…ç†æ­¥éª¤ï¼Œç¡®ä¿ç£ç›˜ä¸é•œåƒç©ºé—´å……è¶³ã€‚  

- **Add issue selfâ€‘claim workflow for external contributors**  
  PR: [#2586](https://github.com/flashinfer-ai/flashinfer/pull/2586)  
  - æ–°å¢ `.github/workflows/issue-claim.yml`ï¼Œé€šè¿‡åœ¨ issue è¯„è®ºä¸­ä½¿ç”¨ `!claim` æˆ– `!assign @user` å®ç°è‡ªåŠ©è®¤é¢†ä¸ç®¡ç†å‘˜æŒ‡æ´¾ã€‚  

- **test: Skip test_decode_delta_rule.py**  
  PR: [#2600](https://github.com/flashinfer-ai/flashinfer/pull/2600)  
  - ä¸º `tests/gdn/test_decode_delta_rule.py` æ·»åŠ  `pytest.mark.skip`ï¼Œæš‚æ—¶è·³è¿‡è¯¥æµ‹è¯•ä»¥é¿å… CI å¤±ç¨³ã€‚  

- **[bugfix] Correct chunk_end calculation in multiâ€‘CTA collaboration when max_len > length**  
  Commit: `ace3d154`  
  URL: <https://github.com/flashinfer-ai/flashinfer/commit/ace3d154f96ddadfa642f4570107515ca680f711>  
  - ä¿®å¤ `include/flashinfer/topk.cuh` ä¸­ `chunk_end` è®¡ç®—é”™è¯¯å¯¼è‡´çš„è¶Šç•Œé—®é¢˜ã€‚  
  - å¼•å…¥ `actual_chunk_size` å˜é‡ï¼Œç¡®ä¿åœ¨ `chunk_start >= length` æ—¶å¾ªç¯ä¸æ‰§è¡Œã€‚  

- **tests/utils/test_topk.py å‚æ•°æ‰©å±•**ï¼ˆåŒä¸Š bugfixï¼‰  
  - å°† `max_len` å‚æ•°åŒ–ä¸º `[4096, 131072]`ï¼Œè¦†ç›–æ›´é•¿åºåˆ—çš„ pageâ€‘table è½¬æ¢æµ‹è¯•ã€‚  

## Issues

- **test self claim**  
  URL: <https://github.com/flashinfer-ai/flashinfer/issues/2606>  
  - è®¨è®ºæ–°å»ºçš„ issueâ€‘claim å·¥ä½œæµä½¿ç”¨ç»†èŠ‚ä¸æƒé™éªŒè¯ã€‚  

- **[Bug] BF16 top_k returns incorrect values on long sequences when forcing filtered algorithm**  
  URL: <https://github.com/flashinfer-ai/flashinfer/issues/2604>  
  - åœ¨ `FLASHINFER_TOPK_ALGO=filtered` æ—¶ï¼ŒBF16 é•¿åºåˆ—çš„ `top_k` ç»“æœä¸æ­£ç¡®ï¼›`auto` ä¸ `multi_cta` æ­£å¸¸ã€‚  

- **[Bug] test_decode_delta_rule fails on GB200/GB300 (not B200)**  
  URL: <https://github.com/flashinfer-ai/flashinfer/issues/2601>  
  - `test_decode_delta_rule` åœ¨ Blackwell GB200/GB300 æœºå™¨ä¸Šå¤±è´¥ï¼Œè€Œåœ¨ B200 ä¸Šé€šè¿‡ï¼Œéœ€å®šä½å¹³å°ç‰¹å¼‚æ€§å›å½’ã€‚
### Dao-AILab/flash-attention
## Flash-Attention Updates (2026-02-20)

### Architecture Handling Improvements
- **[463623e](https://github.com/Dao-AILab/flash-attention/commit/463623e34b53daf6c6ac5da6692f06bf87a1b4a8)**: Updated architecture handling in `flash_fwd_sm100.py` to use `BaseDSL._get_dsl().get_arch_enum()` instead of hardcoded values. Removed `arch` parameter from `__init__` and updated imports.

### Dependency Update
- **[fe878cc](https://github.com/Dao-AILab/flash-attention/commit/fe878cc7ca17fda23e4fea574d1304bd729ea983)**: Updated quack dependency to version 0.2.10 in `pyproject.toml`.

### SM100 Optimizations
- **[8e0b5d7](https://github.com/Dao-AILab/flash-attention/commit/8e0b5d750abdf7e3a6fad27b1cbb5933b3364221)**: Disabled ex2 emulation for SM103 in `flash_fwd_sm100.py`. Added `self.enable_e2e` flag to control this behavior.

### API Modernization
- **[884f72d](https://github.com/Dao-AILab/flash-attention/commit/884f72db2577e3647815f1edef5e6e1b4bab89aa)**: Updated backward postprocessing in `flash_bwd_postprocess.py` to use `cute.arch.fence_view_async_shared()` API. Expanded architecture support to include 11.x (Blackwell).

### Compute Capability Refactoring
- **[05eea8b](https://github.com/Dao-AILab/flash-attention/commit/05eea8b43862f9a40cd429274f500af4d229b378)**: Refactored from `compute_capability` to `arch` terminology across multiple files. Updated `_get_device_capability()` to `_get_device_arch()` which returns `major * 10 + minor`.

### Backward Pass Enhancement
- **[710d3cc](https://github.com/Dao-AILab/flash-attention/commit/710d3cc239eb5171e8b87bcde9e51349d4affe8b)**: Implemented 2 CTA (Thread Block) configuration for SM100 backward pass. Added 1171 lines and removed 416 lines across multiple files including `flash_bwd_sm100.py`.

### Register Variable Fix
- **[6079a9b](https://github.com/Dao-AILab/flash-attention/commit/6079a9bf4cfd7af8e7586afea6c49a97ebddf46e)**: Fixed number of register variables in `flash_bwd_sm100.py`.

### Community Request
- **[Issue #2267](https://github.com/Dao-AILab/flash-attention/issues/2267)**: User requests update to fit torch-2.10 compatibility.
### sgl-project/sglang
æäº¤ï¼š
- Revert "[jit kernel] Support per_token_group_quant_8bit jit kernel" (#19131) [`4653939`](https://github.com/sgl-project/sglang/commit/4653939cdafac898ac1cd680f1efa4bb697631a1)
- å—å½±å“æ–‡ä»¶: python/sglang/jit_kernel/benchmark/bench_per_token_group_quant_8bit.py, python/sglang/jit_kernel/csrc/gemm/per_token_group_quant_8bit.cuh, python/sglang/jit_kernel/per_token_group_quant_8bit.py, python/sglang/jit_kernel/tests/test_per_token_group_quant_8bit.py, python/sglang/jit_kernel/utils.py, python/sglang/srt/layers/quantization/fp8_kernel.py
- [Benchmark] Remove re-exports from bench_serving.py (#19130) [`1f2da82`](https://github.com/sgl-project/sglang/commit/1f2da824ddc7d8b93093bda07373d98cfb07229f)
- å—å½±å“æ–‡ä»¶: benchmark/hicache/bench_long_context.py, benchmark/hicache/bench_mix.py, benchmark/hicache/bench_multiturn.py, benchmark/hicache/bench_serving.py, benchmark/hicache/data_processing.py, benchmark/lora/lora_bench.py, python/sglang/bench_offline_throughput.py, python/sglang/bench_serving.py, python/sglang/test/bench_one_batch_server_internal.py, python/sglang/test/kits/cache_hit_kit.py, scripts/playground/bench_speculative.py, scripts/playground/replay_request_dump.py, test/manual/hicache/test_disaggregation_hicache.py, test/registered/bench_fn/test_bench_serving_functionality.py, test/registered/hicache/test_hicache_storage_file_backend.py, test/registered/hicache/test_hicache_variants.py
- [Refactor] Benchmark Phase 1: extract utils and datasets from bench_serving (#19077) [`f158869`](https://github.com/sgl-project/sglang/commit/f158869c2cc3e4915669bc1e2a94049b06264723)
- å—å½±å“æ–‡ä»¶: python/sglang/bench_serving.py, python/sglang/benchmark/__init__.py, python/sglang/benchmark/datasets/__init__.py, python/sglang/benchmark/datasets/common.py, python/sglang/benchmark/datasets/custom.py, python/sglang/benchmark/datasets/generated_shared_prefix.py, python/sglang/benchmark/datasets/image.py, python/sglang/benchmark/datasets/mmmu.py, python/sglang/benchmark/datasets/mooncake.py, python/sglang/benchmark/datasets/openai_dataset.py, python/sglang/benchmark/datasets/random.py, python/sglang/benchmark/datasets/sharegpt.py, python/sglang/benchmark/utils.py
- Tiny rename `cuda_graph` tests to `piecewise_cuda_graph` (#19128) [`3fb457b`](https://github.com/sgl-project/sglang/commit/3fb457b1036a07957ddeed70662afb0802ed0e42)
- å—å½±å“æ–‡ä»¶: test/registered/piecewise_cuda_graph/test_piecewise_cuda_graph_2_gpu.py, test/registered/piecewise_cuda_graph/test_piecewise_cuda_graph_large_1_gpu.py, test/registered/piecewise_cuda_graph/test_piecewise_cuda_graph_small_1_gpu.py
- [FlashInfer] Switch FlashInfer allreduce fusion to unified API (#18341) [`7b0fb43`](https://github.com/sgl-project/sglang/commit/7b0fb43c7a7e5a6d8584e9ebdd5bc6b12e2f0fcd)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/layers/flashinfer_comm_fusion.py
- [sgl] view could hold the memory too long and introduced large memory (#19109) [`bf36aa4`](https://github.com/sgl-project/sglang/commit/bf36aa4c31173125a031f3c3135ddbafaee8e60f)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/managers/scheduler_output_processor_mixin.py
- fix KimiK2Detector regex patterns with re.DOTALL (#19120) [`677b66a`](https://github.com/sgl-project/sglang/commit/677b66af805d93f7a26e9cf96b7b243eb1beaee1)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/function_call/kimik2_detector.py
- fix tool handling in OpenAIServingChat (#18996) [`4a362a0`](https://github.com/sgl-project/sglang/commit/4a362a0e041a4b7524f1b40272ad8e44606c9b3d)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/entrypoints/openai/serving_chat.py, test/registered/openai_server/basic/test_serving_chat.py
- [Diffusion] Restruct and clean Diffusion rotary embedding (#19064) [`66497ab`](https://github.com/sgl-project/sglang/commit/66497ab0aa167a05a73b04955d49ee1dd468e600)
- å—å½±å“æ–‡ä»¶: python/sglang/multimodal_gen/runtime/layers/rotary_embedding.py, python/sglang/multimodal_gen/runtime/layers/rotary_embedding/__init__.py, python/sglang/multimodal_gen/runtime/layers/rotary_embedding/_base.py, python/sglang/multimodal_gen/runtime/layers/rotary_embedding/_factory.py, python/sglang/multimodal_gen/runtime/layers/rotary_embedding/_mrope.py, python/sglang/multimodal_gen/runtime/layers/rotary_embedding/_utils.py
- [Feature] rewrite rope kernel; remove flashinfer dependencies (#18844) [`d8d0208`](https://github.com/sgl-project/sglang/commit/d8d0208c6381312e56f7baf72441803509b605bf)
- å—å½±å“æ–‡ä»¶: python/sglang/jit_kernel/benchmark/bench_rope.py, python/sglang/jit_kernel/csrc/elementwise/rope.cuh, python/sglang/jit_kernel/include/sgl_kernel/utils.cuh, python/sglang/jit_kernel/rope.py, python/sglang/jit_kernel/tests/test_rope.py, python/sglang/srt/layers/rotary_embedding.py, python/sglang/srt/models/gpt_oss.py, python/sglang/srt/models/llada2.py, python/sglang/srt/models/utils.py
- [diffusion] feat: support passing component path via server args (#19108) [`6503f94`](https://github.com/sgl-project/sglang/commit/6503f94211a4317786f4c6d2418ddcdcd4b20c8e)
- å—å½±å“æ–‡ä»¶: docs/diffusion/api/cli.md, python/sglang/cli/generate.py, python/sglang/multimodal_gen/runtime/entrypoints/cli/cli_types.py, python/sglang/multimodal_gen/runtime/entrypoints/cli/generate.py, python/sglang/multimodal_gen/runtime/entrypoints/cli/main.py, python/sglang/multimodal_gen/runtime/loader/component_loaders/adapter_loader.py, python/sglang/multimodal_gen/runtime/loader/component_loaders/bridge_loader.py, python/sglang/multimodal_gen/runtime/loader/component_loaders/scheduler_loader.py, python/sglang/multimodal_gen/runtime/loader/component_loaders/text_encoder_loader.py, python/sglang/multimodal_gen/runtime/loader/component_loaders/transformer_loader.py, python/sglang/multimodal_gen/runtime/loader/component_loaders/vae_loader.py, python/sglang/multimodal_gen/runtime/loader/component_loaders/vocoder_loader.py, python/sglang/multimodal_gen/runtime/pipelines/diffusers_pipeline.py, python/sglang/multimodal_gen/runtime/pipelines_core/composed_pipeline_base.py, python/sglang/multimodal_gen/runtime/server_args.py, python/sglang/multimodal_gen/runtime/utils/hf_diffusers_utils.py
- æ–‡ä»¶åˆ—è¡¨å·²æˆªæ–­ã€‚
- Tiny update pull-requests permission of release-branch-cut.yml (#19121) [`c36a10a`](https://github.com/sgl-project/sglang/commit/c36a10aabb39363df3d8b83750bb5f0cc2edb2d5)
- å—å½±å“æ–‡ä»¶: .github/workflows/release-branch-cut.yml
- [FEAT] Add Anthropic compatible API endpoint (#18630) [`cc45167`](https://github.com/sgl-project/sglang/commit/cc451671b5b69c64c25e22a0b9f02f80e56bb960)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/entrypoints/anthropic/__init__.py, python/sglang/srt/entrypoints/anthropic/protocol.py, python/sglang/srt/entrypoints/anthropic/serving.py, python/sglang/srt/entrypoints/http_server.py, test/manual/vlm/test_anthropic_vision.py, test/registered/openai_server/basic/test_anthropic_server.py, test/registered/openai_server/function_call/test_anthropic_tool_use.py
- [diffusion] refactor: reduce redundancy and improve stage api (#19060) [`b89ca65`](https://github.com/sgl-project/sglang/commit/b89ca65789129567321a865165fae5bd56a8acda)
- å—å½±å“æ–‡ä»¶: docs/diffusion/support_new_models.md, python/sglang/multimodal_gen/runtime/pipelines/comfyui_flux_pipeline.py, python/sglang/multimodal_gen/runtime/pipelines/comfyui_qwen_image_pipeline.py, python/sglang/multimodal_gen/runtime/pipelines/comfyui_zimage_pipeline.py, python/sglang/multimodal_gen/runtime/pipelines/diffusers_pipeline.py, python/sglang/multimodal_gen/runtime/pipelines/flux.py, python/sglang/multimodal_gen/runtime/pipelines/flux_2.py, python/sglang/multimodal_gen/runtime/pipelines/glm_image.py, python/sglang/multimodal_gen/runtime/pipelines/hunyuan_pipeline.py, python/sglang/multimodal_gen/runtime/pipelines/ltx_2_pipeline.py, python/sglang/multimodal_gen/runtime/pipelines/mova_pipeline.py, python/sglang/multimodal_gen/runtime/pipelines/qwen_image.py, python/sglang/multimodal_gen/runtime/pipelines/wan_causal_dmd_pipeline.py, python/sglang/multimodal_gen/runtime/pipelines/wan_dmd_pipeline.py, python/sglang/multimodal_gen/runtime/pipelines/wan_i2v_dmd_pipeline.py, python/sglang/multimodal_gen/runtime/pipelines/wan_i2v_pipeline.py
- æ–‡ä»¶åˆ—è¡¨å·²æˆªæ–­ã€‚
- [Quantization] Support config.json quantization_config format, fix exclude_modules matching, and fix KV cache scale loading for Nemotron (#18546) [`33c33a7`](https://github.com/sgl-project/sglang/commit/33c33a7de9bb15c5708f6ae9d78c5d72d74b0fa4)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/layers/quantization/modelopt_quant.py, python/sglang/srt/model_loader/weight_utils.py, python/sglang/srt/models/nemotron_h.py
- Remove error dllm and diffusion doc in basic_useage (#19105) [`e239f8a`](https://github.com/sgl-project/sglang/commit/e239f8aa85f7f2e9ad9831ec6bec3cb5ec96d1cd)
- å—å½±å“æ–‡ä»¶: docs/basic_usage/diffusion.md, docs/basic_usage/diffusion_llms.md, docs/index.rst
- Fix bug in symm mem pre-allocation default (#19082) [`51b3ed0`](https://github.com/sgl-project/sglang/commit/51b3ed02ca544e58b05bcbfd261dfb5c7d355b8d)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/server_args.py
- [DSv32] Fix MTP and CP compatability (#19062) [`afd91e8`](https://github.com/sgl-project/sglang/commit/afd91e8782c0852d6038fd42df090c0de45cea1a)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/models/deepseek_nextn.py
- [Auto Sync] Update batch_invariant_ops.py (20260221) (#19098) [`463baaf`](https://github.com/sgl-project/sglang/commit/463baafe10454a198c14a73d6641d46bbece7c30)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/batch_invariant_ops/batch_invariant_ops.py
- [Auto Sync] Update bench_one_batch_server_internal.py (20260221) (#19097) [`2928dfb`](https://github.com/sgl-project/sglang/commit/2928dfb8fae9a0a1985166b34d4cbcce31ae45b3)
- å—å½±å“æ–‡ä»¶: python/sglang/test/bench_one_batch_server_internal.py
- Refactor graph input buffers (#18991) [`84c67c8`](https://github.com/sgl-project/sglang/commit/84c67c8be0a13bdf9749f674f0501461770fb4f7)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/model_executor/cuda_graph_runner.py, python/sglang/srt/model_executor/input_buffers.py, python/sglang/srt/model_executor/model_runner.py, python/sglang/srt/model_executor/piecewise_cuda_graph_runner.py, python/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py, python/sglang/srt/speculative/eagle_draft_extend_cuda_graph_runner.py, python/sglang/srt/speculative/multi_layer_eagle_draft_extend_cuda_graph_runner.py, python/sglang/srt/speculative/multi_layer_eagle_worker_v2.py
- Upd: CODEOWNERS (#19055) [`b2573fe`](https://github.com/sgl-project/sglang/commit/b2573fe4267ae3a6e3cda521fad15e8efb07dc0c)
- å—å½±å“æ–‡ä»¶: .github/CODEOWNERS
- [GPT-OSS] support fp8 online quantization for gpt-oss bf16 (#18988) [`4bffd3a`](https://github.com/sgl-project/sglang/commit/4bffd3a2323a332506ce16f0fbd2ce7e96db2204)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/layers/quantization/fp8.py, python/sglang/srt/server_args.py
- Add generated-shared-prefix dataset in bench_one_batch (#18986) [`96bae23`](https://github.com/sgl-project/sglang/commit/96bae2355e7010773729d4ca5d2b84e4621ab658)
- å—å½±å“æ–‡ä»¶: python/sglang/test/bench_one_batch_server_internal.py
- [feat] feat: support swa in trtllm_mha (#18970) [`ab18734`](https://github.com/sgl-project/sglang/commit/ab18734375accf3dbcd043f5b6244d4082ee84f5)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/layers/attention/trtllm_mha_backend.py
- [AMD] support two batch overlapping for mori ep (#17953) [`fbb6098`](https://github.com/sgl-project/sglang/commit/fbb60984872ee69d868b595f81c0155b15601de5)
- å—å½±å“æ–‡ä»¶: docs/advanced_features/server_arguments.md, python/sglang/srt/batch_overlap/operations_strategy.py, python/sglang/srt/batch_overlap/two_batch_overlap.py, python/sglang/srt/layers/attention/aiter_backend.py, python/sglang/srt/layers/moe/ep_moe/layer.py, python/sglang/srt/layers/moe/fused_moe_triton/layer.py, python/sglang/srt/layers/moe/token_dispatcher/__init__.py, python/sglang/srt/layers/moe/token_dispatcher/moriep.py, python/sglang/srt/models/deepseek_v2.py, python/sglang/srt/server_args.py, python/sglang/test/bench_one_batch_server_internal.py
- Fix adjust_num_token_non_padded_for_attn_tp returning CPU tensor (#19051) [`38ee749`](https://github.com/sgl-project/sglang/commit/38ee749dd90cdab572d393bb856430ef92c981d2)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/model_executor/forward_batch_info.py
- [Fix] Run FlashInfer autotune on non-default stream for NCCL 2.29+ compatibility (#18987) [`3358ba8`](https://github.com/sgl-project/sglang/commit/3358ba894588f25da97c1c29fea9e32569669d60)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/distributed/device_communicators/pynccl_allocator.py, python/sglang/srt/model_executor/model_runner.py
- [Fix] DO NOT skip save_kv_cache for dllm (#19020) [`5285240`](https://github.com/sgl-project/sglang/commit/52852404c869d7be291b386073baca850f69aaef)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/layers/attention/flashinfer_backend.py
- Fix NSA FP8 KV cache path for both-trtllm MHA one-shot (#18931) [`f23a23c`](https://github.com/sgl-project/sglang/commit/f23a23cc05fca67d296426e4acb9ce69524f26e4)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/models/deepseek_common/attention_forward_methods/forward_mha.py
- [diffusion] feat: support nunchaku for Z-Image-Turbo and flux.1 (int4) (#18959) [`8d789b5`](https://github.com/sgl-project/sglang/commit/8d789b5c3d7ed3f585dd27c6052b2e12fb5b96d0)
- å—å½±å“æ–‡ä»¶: python/sglang/multimodal_gen/configs/models/dits/flux.py, python/sglang/multimodal_gen/runtime/layers/quantization/configs/nunchaku_config.py, python/sglang/multimodal_gen/runtime/loader/component_loaders/transformer_loader.py, python/sglang/multimodal_gen/runtime/loader/fsdp_load.py, python/sglang/multimodal_gen/runtime/loader/utils.py, python/sglang/multimodal_gen/runtime/models/dits/base.py, python/sglang/multimodal_gen/runtime/models/dits/flux.py, python/sglang/multimodal_gen/runtime/models/dits/qwen_image.py, python/sglang/multimodal_gen/runtime/models/dits/zimage.py
- [jit kernel] Support per_token_group_quant_8bit jit kernel (#18905) [`7d95344`](https://github.com/sgl-project/sglang/commit/7d953440ec965032b6b5af386d56c427ae4e6da1)
- å—å½±å“æ–‡ä»¶: python/sglang/jit_kernel/benchmark/bench_per_token_group_quant_8bit.py, python/sglang/jit_kernel/csrc/gemm/per_token_group_quant_8bit.cuh, python/sglang/jit_kernel/per_token_group_quant_8bit.py, python/sglang/jit_kernel/tests/test_per_token_group_quant_8bit.py, python/sglang/jit_kernel/utils.py, python/sglang/srt/layers/quantization/fp8_kernel.py
Issuesï¼š
- [Feature] Parallelize module loading to speed up server launch and refit for Diffusion models (https://github.com/sgl-project/sglang/issues/19092)
- Issue å†…å®¹å·²æˆªæ–­ã€‚
- [Feature] Support offload and wake up of SGLang Diffusion (https://github.com/sgl-project/sglang/issues/19090)
- Issue å†…å®¹å·²æˆªæ–­ã€‚
- [Feature] Detailed Break down of time spend on Launching SGLang Diffusion (https://github.com/sgl-project/sglang/issues/19087)
- [Bug] Deepseek 3.2 nvfp4 specv2 nsa-decode-backend=trttlm kernel crash (https://github.com/sgl-project/sglang/issues/19081)
- Issue å†…å®¹å·²æˆªæ–­ã€‚
- [Bug] [Diffusion] Text to image model, --prompt-file-path feature not working (https://github.com/sgl-project/sglang/issues/19063)
- Issue å†…å®¹å·²æˆªæ–­ã€‚
- GLM-OCR (zai-org/GLM-OCR) multimodal completely broken in 0.5.8.post1 (https://github.com/sgl-project/sglang/issues/19048)
- Issue å†…å®¹å·²æˆªæ–­ã€‚
- [Bug][Qwen3.5] Tracking: Qwen3.5 model issues with PD disaggregation (https://github.com/sgl-project/sglang/issues/19045)
- Issue å†…å®¹å·²æˆªæ–­ã€‚
- DeepEP MoE A2A backend: crash on partial CUDA P2P topologies (deep_ep.cpp:200); add startup peer-access preflight (https://github.com/sgl-project/sglang/issues/19033)
- Issue å†…å®¹å·²æˆªæ–­ã€‚
### vllm-project/vllm
# vLLM Project Updates - 2026-02-20

## Notable Commits

### Model Runner V2
- Added Eagle3 support without CUDA graph ([#35029](https://github.com/vllm-project/vllm/pull/35029))
- Modified `vllm/v1/worker/gpu/model_runner.py` and added new files in `vllm/v1/worker/gpu/spec_decode/eagle/`

### Frontend Enhancements
- Added automatic language detection for Whisper transcription ([#34342](https://github.com/vllm-project/vllm/pull/34342))
- Added support for multimodal inputs for late-interaction scoring (ColQwen3) ([#34574](https://github.com/vllm-project/vllm/pull/34574))
- Added Qwen3-ASR realtime streaming support ([#34613](https://github.com/vllm-project/vllm/pull/34613))

### ROCm Improvements
- Enabled bitsandbytes quantization support on ROCm ([#34688](https://github.com/vllm-project/vllm/pull/34688))
- Optimized ROCM_AITER_FA spec decode eagle performance ([#34541](https://github.com/vllm-project/vllm/pull/34541))
- Fixed aiter paged_attention_v1 decode for sliding window and head_size < 64 ([#34570](https://github.com/vllm-project/vllm/pull/34570))

### Core Optimizations
- Optimized sample_recovered_tokens kernel ([#34974](https://github.com/vllm-project/vllm/pull/34974))
- Added per-block extra_keys to KV events ([#33304](https://github.com/vllm-project/vllm/pull/33304))
- Changed kv_load_failure_policy default from "recompute" to "fail" ([#34896](https://github.com/vllm-project/vllm/pull/34896))

### Build & CI
- Fixed gRPC version mismatch ([#35013](https://github.com/vllm-project/vllm/pull/35013))
- Added opentelemetry libs to default vllm build ([#34466](https://github.com/vllm-project/vllm/pull/34466))
- Removed failing prime-rl integration test ([#34843](https://github.com/vllm-project/vllm/pull/34843))

## Open Issues

### CI Failures
- Maverick model QKV weight shape mismatch during load_weights with expert parallelism ([#34995](https://github.com/vllm-project/vllm/issues/34995))
- GDN attention backend assertion failure with MTP speculative decoding ([#34993](https://github.com/vllm-project/vllm/issues/34993))
- Qwen3-Next-FP8 failure ([#34989](https://github.com/vllm-project/vllm/issues/34989))
- FlashInfer attn-fp4 fused kernel performs worse than unfused ([#34988](https://github.com/vllm-project/vllm/issues/34988))

### Feature Requests
- Infrastructure improvements for ROCm CI ([#34994](https://github.com/vllm-project/vllm/issues/34994))
- RFC: Adding Support for Single Batch Overlap (SBO) With FlashInfer DeepEP LL NVFP4 ([#34963](https://github.com/vllm-project/vllm/issues/34963))
- Compare outputs in test_functionalization.py ([#34996](https://github.com/vllm-project/vllm/issues/34996)) (good first issue)
### NVIDIA/cutile-python
æäº¤ï¼š
- Introduce BlockRestriction interface for restricting which operations are allowed inside a block [`61d6060`](https://github.com/NVIDIA/cutile-python/commit/61d6060eca8c046f480e1e97f0cf000e5ae12385)
- å—å½±å“æ–‡ä»¶: src/cuda/tile/_ir/ir.py, src/cuda/tile/_ir/ops.py, test/test_scan.py
- Fix helper function with while loop and return [`6653a56`](https://github.com/NVIDIA/cutile-python/commit/6653a56ea236005bff57bdb26dac41709e824247)
- å—å½±å“æ–‡ä»¶: changelog.d/while-loop-return.md, src/cuda/tile/_passes/ast2hir.py, test/test_helper_function.py
- Add ct.static_iter() & tuple concatenation [`94f7824`](https://github.com/NVIDIA/cutile-python/commit/94f7824480646a9bb26d6e42304390fbc749ef8f)
- å—å½±å“æ–‡ä»¶: changelog.d/static-iter.md, docs/source/operations.rst, src/cuda/tile/__init__.py, src/cuda/tile/_dispatch_mode.py, src/cuda/tile/_ir/hir.py, src/cuda/tile/_ir/ops.py, src/cuda/tile/_passes/ast2hir.py, src/cuda/tile/_stub.py, test/test_static_iter.py, test/test_tuple.py
- Use dataclasses for Operations [`f0be470`](https://github.com/NVIDIA/cutile-python/commit/f0be470fd103f7df31bb3846a5c2a7cb877398dd)
- å—å½±å“æ–‡ä»¶: src/cuda/tile/_ir/ir.py, src/cuda/tile/_ir/ops.py, src/cuda/tile/_passes/dce.py, src/cuda/tile/_passes/loop_split.py, src/cuda/tile/_passes/rewrite_patterns.py, src/cuda/tile/_passes/token_order.py
- Don't rely on co_varnames to get the list of locals [`e1854cc`](https://github.com/NVIDIA/cutile-python/commit/e1854cc5f386e9d454108e9223c2da931623bc52)
- å—å½±å“æ–‡ä»¶: src/cuda/tile/_passes/ast2hir.py, test/test_ast_util.py, test/test_static_eval.py

## æ€»ç»“
ğŸ”­ **å±•æœ›**ï¼šå…³æ³¨ Cutlass å¯¹ SM120 FP8 MMA çš„åç»­ PRã€FlashInfer å¯¹ BF16/FP4 topâ€‘k ä¸ NVFP4 MoE çš„ bug ä¿®å¤è¿›åº¦ï¼Œä»¥åŠ vLLM åœ¨ ROCm ä¸ Eagle3 ä¸Šçš„æ€§èƒ½å›å½’ã€‚ğŸš§ **å¾…å…³æ³¨**ï¼šSGLang å¤§è§„æ¨¡ benchmark é‡æ„åæ˜¯å¦ä¼šå½±å“ CI ç¨³å®šæ€§ï¼›FlashInfer ä¸ Flashâ€‘Attention çš„ SM100/SM103 ä¼˜åŒ–æ˜¯å¦ä¼šåœ¨æ–°ä¸€ä»£ GPUï¼ˆå¦‚ Blackwellï¼‰ä¸Šäº§ç”Ÿå…¼å®¹æ€§é—®é¢˜ã€‚
