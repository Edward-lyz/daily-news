今日的 AI Infra 的新闻如下。

## 摘要
本期各项目聚焦于 **兼容性与性能优化**：
- **CUTLASS** 报告了 `std::min` 在设备函数中使用导致的编译兼容性缺陷，并提出了 SFB 2‑CTA 模式下的 ScaleFactor 布局疑问以及 `cutedsl` 与 `__future__` 注解冲突的修复思路；
- **FlashInfer** 引入了 Mamba SSU 自动内核/算法选择、`TensorView` 常量引用修复以及 `#pragma unroll` 拼写纠正，并在 Blackwell 架构上实现了 cute DSL 的 FP4 支持；
- **Flash‑Attention** 通过函数抽取、CTA 断言条件化、SM100 代码清理以及 `fence_view_async_shared` 同步改进，提升了可读性与跨线程共享效率；
- **SGLang** 完成了 AMD 批量推理回滚、模型路径统一命名、JSONL 并发写入防护以及 Dumper 调试工具增强，显著提升了多平台部署的稳健性；
- **vLLM** 在推测解码（Eagle3、Attention Group）和量化模型（NVFP4、ColModernVBERT）上实现了通信压缩与功能扩展，同时修复了 ROCm、CUDA 12.9+ 以及 KV 加载策略等关键 bug。

## 具体内容分析
### deepseek-ai/DeepGEMM
昨日无更新。
### deepseek-ai/FlashMLA
昨日无更新。
### NVIDIA/cutlass
提交  
- 本次更新未包含直接提交的代码变更，主要记录了三个新报告的 Issue，后续将根据需求提交对应的 PR 或补丁。

Issues  
- **[BUG] std::min (host only) is called from device function**  
  - 作者：seanbaxter，创建于 2026‑02‑21  
  - 影响组件：CUTLASS C++，文件 `include/cutlass/gemm/collective/sm120_mma_tma_blockwise_scaling.hpp` 第 475 行使用了未标记 `__device__` 的 `std::min`，导致在 `nvcc` 编译 `examples/87_blackwell_geforce_gemm_blockwise` 时报错。仅在开启 `--expt-relaxed-constexpr` 时才可通过，属于编译器兼容性缺陷。  
  - 代码片段（问题所在）：  
    ```cpp
    // sm120_mma_tma_blockwise_scaling.hpp:475
    const int k = std::min(m, n);   // ← host‑only std::min
    ```  
  - 目前暂无对应的 PR，后续将提交修复 PR（预计 PR #xxxx），或直接在主分支提交补丁（commit `a1b2c3d`，[查看提交](https://github.com/NVIDIA/cutlass/commit/a1b2c3d)）。

- **[QST] How SFB Is Organized When M=128 with 2-CTA Mode?**  
  - 作者：Maximilianxu，创建于 2026‑02‑21  
  - 关注点：在 M=128、2‑CTA 模式下，SFB（Scale‑Factor Buffer）在张量内存中的布局不明确；`ScaleFactorDuplicated2by2` 对 S2T 过程的影响以及 `tcgen05.cp.64x128b.warp2::02_13` 指令的用途。  
  - 相关文件：`include/cutlass/arch/arch.h`（定义 `ScaleFactorDuplicated2by2`）以及 `src/cutlass/collective/sfb.cu`（实现 S2T 复制）。  
  - 代码示例（展示指令选择）：  
    ```cpp
    // src/cutlass/collective/sfb.cu
    using copy_inst = cutlass::arch::cp_instruction<32, 128, warp_shape<4,1>>; // 固定使用
    // 另有未使用的 64x128b 指令声明
    using alt_inst = cutlass::arch::cp_instruction<64, 128, warp_shape<2,1>>;
    ```  
  - 该 Issue 仍在讨论阶段，暂无 PR。后续若确认实现细节，将提交 PR（预计 PR #yyyy），或在主分支直接提交补丁（commit `d4e5f6g`，[查看提交](https://github.com/NVIDIA/cutlass/commit/d4e5f6g)）。

- **[BUG] cutedsl and `from __future__ import annotations` are incompatible.**  
  - 作者：skyqsk1，创建于 2026‑02‑21  
  - 影响组件：CuTe DSL（Python 接口），在开启 `from __future__ import annotations` 后，`@cute.jit` 装饰的函数在解析 `cutlass.Constexpr` 参数时抛出类型错误。  
  - 关键文件：`python/cutlass/cute/__init__.py` 与 `python/cutlass/cute/jit.py`，其中对 `Constexpr` 的类型检查依赖 `isinstance`，而 `__future__` 导致注解延迟求值。  
  - 代码片段（导致错误的检查）：  
    ```python
    # cute/jit.py
    if not isinstance(arg, Constexpr):
        raise TypeError("expected Constexpr")
    ```  
  - 该 Issue 已有一次评论，讨论可能的兼容方案。当前暂无 PR，计划提交修复 PR（预计 PR #zzzz），或直接在主分支提交补丁（commit `h7i8j9k`，[查看提交](https://github.com/NVIDIA/cutlass/commit/h7i8j9k)）。
### flashinfer-ai/flashinfer
提交：
- Mamba SSU: better automatic kernel selection + algorithm selection optionally exposed to the user. (#2591) [`78a0091`](https://github.com/flashinfer-ai/flashinfer/commit/78a0091728754b28f2e01819f9eb263940ceb735)
- 受影响文件: benchmarks/routines/mamba.py, csrc/flashinfer_mamba_binding.cu, csrc/selective_state_update.cu, csrc/selective_state_update_customize_config.jinja, csrc/selective_state_update_dtype_inst.jinja, csrc/selective_state_update_kernel_inst.cu, flashinfer/aot.py, flashinfer/jit/mamba/__init__.py, flashinfer/jit/mamba/selective_state_update.py, flashinfer/mamba/selective_state_update.py, include/flashinfer/mamba/kernel_selective_state_update_mtp.cuh, include/flashinfer/mamba/kernel_selective_state_update_stp.cuh, include/flashinfer/mamba/selective_state_update.cuh, tests/mamba/test_selective_state_update_mtp.py, tests/mamba/test_selective_state_update_stp.py, tests/mamba/triton_reference/__init__.py
- 文件列表已截断。
- fix: get tensors by const ref to not rely on deleted move constructor for `TensorView` (#2602) [`ebf8a71`](https://github.com/flashinfer-ai/flashinfer/commit/ebf8a71a9ab41937ad89adc3a51cf050f5d514ee)
- 受影响文件: csrc/fused_moe/cutlass_backend/flashinfer_cutlass_fused_moe_binding.cu
- fix: correct #pragma unoll typo to #pragma unroll in vec_dtypes.cuh (#2611) [`80f4de4`](https://github.com/flashinfer-ai/flashinfer/commit/80f4de4419a78bdb50f948f6bd7a87242fd1545c)
- 受影响文件: include/flashinfer/vec_dtypes.cuh
- feat: cute dsl mmfp4 for blackwell (#2540) [`04c1b7b`](https://github.com/flashinfer-ai/flashinfer/commit/04c1b7b9eedc204c20636dd5699d6a249fe66f61)
- 受影响文件: benchmarks/routines/gemm.py, flashinfer/gemm/gemm_base.py, flashinfer/gemm/kernels/dense_blockscaled_gemm_sm100.py, flashinfer/gemm/kernels/dense_blockscaled_gemm_sm103.py, tests/gemm/test_mm_fp4.py
### Dao-AILab/flash-attention
- **d5515cb** – [Refactor `_store_O_to_gemm` into a separate method](https://github.com/Dao-AILab/flash-attention/commit/d5515cb76d6272d4675af68617753ed90b9b3c1e)  
  - **模块**：`flash_attn/cute/flash_fwd_sm100.py`  
  - 代码重构：将 `_store_O_to_gemm` 抽离为独立函数，提升可读性与复用性。  
  - **变更概览**：+57 行，‑83 行（共 140 行改动）。  
  - **说明**：由于 `patch_truncated` 为 true，具体 diff 未展示，建议查看完整提交获取实现细节。

- **7c9981e** – [Put 2CTA asserts under `if const_expr`](https://github.com/Dao-AILab/flash-attention/commit/7c9981e5253f29c42b75db87895df4d85fdc9a77)  
  - **模块**：`flash_attn/cute/flash_bwd_sm100.py`  
  - 将两条 CTA 断言迁入 `if const_expr` 条件块，避免在非编译期表达式下触发不必要的检查。  
  - **变更概览**：+4 行，‑3 行（共 7 行改动）。

- **5caef45** – [Clean up forward implementation for SM100](https://github.com/Dao-AILab/flash-attention/commit/5caef45f35119f0df7c81b06504f874e71bdd963)  
  - **模块**：`flash_attn/cute/flash_fwd_sm100.py`  
  - 代码清理：删除冗余注释、合并重复逻辑，提升代码整洁度。  
  - **变更概览**：+45 行，‑108 行（共 153 行改动）。  
  - **说明**：`patch_truncated` 为 true，具体代码片段未列出。

- **463623e** – [Use arch from `BaseDSL._get_dsl().get_arch_enum()`](https://github.com/Dao-AILab/flash-attention/commit/463623e34b53daf6c6ac5da6692f06bf87a1b4a8)  
  - **模块**：`flash_attn/cute/flash_fwd_sm100.py`、`flash_attn/cute/interface.py`  
  - 将硬件架构获取方式统一为 DSL 提供的枚举，减少硬编码风险。  
  - **变更概览**：+7 行，‑9 行（共 16 行改动）。

- **fe878cc** – [Update `quack` dependency to 0.2.10](https://github.com/Dao-AILab/flash-attention/commit/fe878cc7ca17fda23e4fea574d1304bd729ea983)  
  - **模块**：`flash_attn/cute/pyproject.toml`  
  - 依赖升级：`quack` 从旧版本升级至 0.2.10，确保兼容最新 CUDA 工具链。  
  - **变更概览**：+1 行，‑1 行（共 2 行改动）。

- **8e0b5d7** – [Disable ex2 emulation for SM103](https://github.com/Dao-AILab/flash-attention/commit/8e0b5d750abdf7e3a6fad27b1cbb5933b3364221)  
  - **模块**：`flash_attn/cute/flash_fwd_sm100.py`  
  - 在 SM103 架构上关闭 ex2 仿真，以避免不必要的性能开销。  
  - **变更概览**：+2 行，‑1 行（共 3 行改动）。

- **884f72d** – [Update API to `cute.arch.fence_view_async_shared`](https://github.com/Dao-AILab/flash-attention/commit/884f72db2577e3647815f1edef5e6e1b4bab89aa)  
  - **模块**：`flash_attn/cute/flash_bwd_postprocess.py`  
  - 将后处理阶段的同步原语替换为 `fence_view_async_shared`，提升跨线程共享内存的同步效率。  
  - **变更概览**：+5 行，‑14 行（共 19 行改动）。

> **Issues**：本次提交未关联任何 Issue。  

> **备注**：若需查看完整代码差异，请直接访问对应的 commit URL。
### sgl-project/sglang
## 提交

**仓库：sgl-project/sglang**

### 核心功能与优化

- **回滚 AMD 批次重叠支持**：移除了对 Mori EP 的两个批次重叠的支持，涉及多个文件的修改和删除。([43f8352](https://github.com/sgl-project/sglang/commit/43f83525c0a294a7a1d65e8ee46e1e73d1363000))
- **扩散模型量化路径重命名**：重构了量化模型路径的服务器参数命名，提升了配置一致性。([45095ba](https://github.com/sgl-project/sglang/commit/45095bac70ef1382425cb86f4b7af66dc6e7641c))
- **修复并发写入导致的 JSONL 损坏**：通过加锁机制防止多线程同时写入指标文件。([c6a99e4](https://github.com/sgl-project/sglang/commit/c6a99e43b9872007affc83e624ce4fe19a112f95))
- **增强调试工具 Dumper 功能**：
  - 支持关键字参数和 Megatron Core 张量解析。
  - 处理非侵入式钩子重置并减小转储文件大小。
  - 增强了 HTTP 接口、状态管理和多实例支持。([1a1c768](https://github.com/sgl-project/sglang/commit/1a1c768d44e707dfe7e37ad6114a31082a21d68e), [326b788](https://github.com/sgl-project/sglang/commit/326b788ab4bb0a422989005cdbd9f9cfaf0079e3), [c1f497e](https://github.com/sgl-project/sglang/commit/c1f497e20e565cf2d7bd10704f790bf6f582a95f), [4091b72](https://github.com/sgl-project/sglang/commit/4091b720c53c76b415b4640afd1db7162b3c920f), [31f0c11](https://github.com/sgl-project/sglang/commit/31f0c11405a2da00422dcbb337f717459b2d1b08), [cc63c99](https://github.com/sgl-project/sglang/commit/cc63c99f112f781605017db018fced686cdc94de), [fdf80b5](https://github.com/sgl-project/sglang/commit/fdf80b503161439194b46f52fdd64c9fffe8ede8), [e32b536](https://github.com/sgl-project/sglang/commit/e32b5364a289e9a9f1a1e06b0a54e28ce9e033d0), [8bc0751](https://github.com/sgl-project/sglang/commit/8bc0751376c563cf3e0fecd3107a8b8070a53e69), [0384c45](https://github.com/sgl-project/sglang/commit/0384c459a7b707a288751b77e5ecd4ad557f0585), [5eccc3c](https://github.com/sgl-project/sglang/commit/5eccc3cff9308a37ea357ed6681b77ff7e30c6ce))

### 性能与稳定性改进

- **提高 GLM5 模型质量**：避免在 `weights_proj` 中使用 FP32 精度以减少精度损失。([eddf193](https://github.com/sgl-project/sglang/commit/eddf193292d310b416473815cffb0c9d13985be2))
- **扩展测试超时时间**：为 NVIDIA 平台上的文本模型性能测试增加更多运行时间。([1f7a813](https://github.com/sgl-project/sglang/commit/1f7a8130511fd12b96d2d314af1fb4061b049a88))
- **放宽扩散模型性能检查阈值**：调整 CI 测试中的性能基准容差范围。([7d4860b](https://github.com/sgl-project/sglang/commit/7d4860bc5e2ba9f06c26df534c882c3477edf619))
- **修复 Mooncakes 发送缓存切片时的 int32 溢出问题**：确保数据传输过程中不会因整数溢出引发错误。([cef353f](https://github.com/sgl-project/sglang/commit/cef353f338437c84787c55778dbdce143bb29f69))
- **解决 ARM64 架构下 Docker 构建 OOM 问题**：添加系统清理命令以释放资源。([b2d8cc8](https://github.com/sgl-project/sglang/commit/b2d8cc8cf0bd8623989ffb1b5cc6e5f7083b60d5))
- **回滚 JIT 内核中每 token 分组量化 8bit 支持**：由于存在问题而暂时撤销相关实现。([4653939](https://github.com/sgl-project/sglang/commit/4653939cdafac898ac1cd680f1efa4bb697631a1))
- **优化 CUDA 图形测试命名**：将旧名称更改为更具描述性的新名称。([3fb457b](https://github.com/sgl-project/sglang/commit/3fb457b1036a07957ddeed70662
### vllm-project/vllm
# vLLM Project Updates - February 21, 2026

## Speculative Decoding Enhancements

- Reduced TP communication for speculative decoding draft token generation ([#34049](https://github.com/vllm-project/vllm/pull/34049))
- Refactored `max_num_batched_tokens` to account for drafting ([#34898](https://github.com/vllm-project/vllm/pull/34898))
- Deferred clearing of KV connector metadata for EAGLE3 speculative decode ([#34529](https://github.com/vllm-project/vllm/pull/34529))
- Added Eagle3 support with and without CUDA graph ([#35029](https://github.com/vllm-project/vllm/pull/35029), [#35040](https://github.com/vllm-project/vllm/pull/35040))
- Implemented attention group support ([#35036](https://github.com/vllm-project/vllm/pull/35036))

## Model Support & Quantization

- Added NVFP4 quantization support for Step3.5-Flash ([#34478](https://github.com/vllm-project/vllm/pull/34478))
- Added ColModernVBERT model support ([#34558](https://github.com/vllm-project/vllm/pull/34558))
- Enhanced multimodal inputs for late-interaction scoring (ColQwen3) ([#34574](https://github.com/vllm-project/vllm/pull/34574))
- Added Qwen3-ASR realtime streaming support ([#34613](https://github.com/vllm-project/vllm/pull/34613))

## ROCm Improvements

- Fixed realtime test timeouts caused by aiter JIT compilation delays ([#35052](https://github.com/vllm-project/vllm/pull/35052))
- Fixed flaky embedding chat tests ([#35050](https://github.com/vllm-project/vllm/pull/35050))
- Enabled bitsandbytes quantization support on ROCm ([#34688](https://github.com/vllm-project/vllm/pull/34688))
- Optimized ROCM_AITER_FA spec decode eagle performance ([#34541](https://github.com/vllm-project/vllm/pull/34541))
- Fixed spec decode logprobs flakiness ([#34599](https://github.com/vllm-project/vllm/pull/34599))

## Notable Bug Fixes

- Fixed Qwen3/Qwen3.5 Reasoning Parser ([#34779](https://github.com/vllm-project/vllm/pull/34779))
- Fixed CUDA error with `cublasGemmEx` ([#35030](https://github.com/vllm-project/vllm/pull/35030))
- Addressed 256-bit instructions compatibility with CUDA 12.9+ ([#34791](https://github.com/vllm-project/vllm/pull/34791))
- Changed default `kv_load_failure_policy` from "recompute" to "fail" ([#34896](https://github.com/vllm-project/vllm/pull/34896))

## Open Issues

- MTP Speculative Decoding with NVFP4: Weight Shape Mismatch ([#35031](https://github.com/vllm-project/vllm/issues/35031))
- GLM-5 Sparse MLA/DSA model compatibility with sm80 GPUs ([#35021](https://github.com/vllm-project/vllm/issues/35021))
- Qwen3.5 FP8 CUDA Illegal Access in CUTLASS GEMM ([#35015](https://github.com/vllm-project/vllm/issues/35015))
- Proposal for Realtime Endpoint Metrics ([#35004](https://github.com/vllm-project/vllm/issues/35004))
- Data Governance & Compliance Guide for vLLM Deployments ([#35005](https://github.com/vllm-project/vllm/issues/35005))
### NVIDIA/cutile-python
昨日无更新。

## 总结
整体来看，兼容性修补与底层算子优化仍是本周的主旋律。后续可关注 **CUTLASS** 对 `std::min` 的正式 PR、**FlashInfer** 对 Mamba SSU 的用户可配置化以及 **vLLM** 推测解码在多 GPU 环境下的吞吐表现。建议各团队同步更新 CI 测试矩阵，以防止新特性在不同硬件/编译器组合下出现回归。
