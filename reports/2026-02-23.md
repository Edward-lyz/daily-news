今日的 AI Infra 的新闻如下。

## 摘要
模型摘要不可用，以下为原始数据整理。

## 具体内容分析
### deepseek-ai/DeepGEMM
昨日无更新。
### deepseek-ai/FlashMLA
昨日无更新。
### NVIDIA/cutlass
**提交**  
暂无关联的 Pull Request。

**Issues**  

- **[QST] cute: complement result looking weid**  
  - 链接: https://github.com/NVIDIA/cutlass/issues/3058  
  - 作者: JINO-ROHIT  
  - 创建时间: 2026‑02‑23 17:57:30 UTC  
  - 评论数: 2  
  - 摘要: 用户在使用 `make_layout` 与 `complement` 时，期望得到 `(2:1)`，但实际输出为 `(2,1):(_1,8)`，请求解释该结果的含义。

- **[BUG] nvpf4 tensor creation looks incorrect**  
  - 链接: https://github.com/NVIDIA/cutlass/issues/3057  
  - 作者: whatdhack  
  - 创建时间: 2026‑02‑23 16:59:07 UTC  
  - 评论数: 0  
  - 摘要: 在 `cutlass/examples/python/CuTeDSL/blackwell/grouped_blockscaled_gemm.py` 中，`create_tensors_abc_for_all_groups()` 与 `create_tensor_and_stride()` 对 `cutlass.Float4E2M1FN` 与 `torch.float4_e2m1fn_x2` 的 4‑bit 打包实现缺失，导致张量尺寸计算错误。

- **[QST] Unexpected Slower Performance in C++ CuTe (77_blackwell_fmha) vs Python CuTe DSL (fmha.py) on Jetson Thor**  
  - 链接: https://github.com/NVIDIA/cutlass/issues/3056  
  - 作者: RRoboter  
  - 创建时间: 2026‑02‑23 16:01:27 UTC  
  - 评论数: 0  
  - 摘要: 在 Jetson Thor 上，C++ 示例 `77_blackwell_fmha` 的循环计数（922 724 cycles）显著快于 Python DSL 实现 `fmha.py`（1 110 707 cycles），并且前者出现更高的 Local Memory Spill，需进一步分析实现差异。

- **[QST] How to multiply binary matrices with CUTLASS Python?**  
  - 链接: https://github.com/NVIDIA/cutlass/issues/3054  
  - 作者: apolotsk  
  - 创建时间: 2026‑02‑23 10:39:11 UTC  
  - 评论数: 0  
  - 摘要: 用户询问在 CUTLASS Python 中如何实现二进制矩阵乘法（`b1 ^ b1 + s32 => {s32, b1}`），并提供了对应的 C++ 示例代码，期待获得 Python API 或示例。
### flashinfer-ai/flashinfer
## 提交

**flashinfer-ai/flashinfer**

- **GDN BF16 容差并行归约改进**
  - 修改了 `flashinfer/gdn_kernels/gdn_decode_bf16_state.py`，引入 SM90+ 兼容的标量 FP32 FMA 操作，并为未来 SM100+ 架构预留优化空间。
  - 调整了测试文件 `tests/gdn/test_decode_delta_rule.py` 中的 KV 状态比较容差值，以适应 BF16 并行归约中的 ULP 误差。[PR #2610](https://github.com/flashinfer-ai/flashinfer/pull/2610)
- **采样模块中 log2f 性能说明文档更新**
  - 在 `include/flashinfer/sampling.cuh` 中添加注释解释为何使用 `log2f` 而非 `logf`：前者映射到单条 PTX LG2 指令，在 NVIDIA GPU 上更高效（3 次操作 vs 4 次）。[PR #2609](https://github.com/flashinfer-ai/flashinfer/pull/2609)
- **编辑模式安装依赖要求文档补充**
  - 更新 `CONTRIBUTING.md`、`README.md` 和 `docs/installation.rst`，明确指出在使用 `--no-build-isolation` 时需确保 `setuptools>=77`，避免因旧版本导致构建失败。[PR #2541](https://github.com/flashinfer-ai/flashinfer/pull/2541)
- **TRT-LLM MoE 接口新增 do_finalize 参数支持**
  - 修改 `csrc/trtllm_fused_moe_kernel_launcher.cu` 和 `flashinfer/fused_moe/core.py`，增加对 `do_finalize` 参数的支持，允许外部控制是否执行最终输出张量分配。[PR #2548](https://github.com/flashinfer-ai/flashinfer/pull/2548)

---

## Issues

- **[功能请求] 支持 BF16 矩阵乘法返回 FP32 输出类型**
  - 当前 `mm_bf16` 只支持 `bf16` 和 `fp16` 输出格式，用户希望扩展至 `fp32` 以提高精度。[Issue #2624](https://github.com/flashinfer-ai/flashinfer/issues/2624)
- **[Bug 报告] gdn_prefill_sm90 内核挂起问题**
  - 使用 Qwen 3.5 397B 模型测试时发现 SM90 版本的 GDN Prefill 内核出现挂起现象，初步怀疑与 barrier 同步有关。[Issue #2623](https://github.com/flashinfer-ai/flashinfer/issues/2623)
- **[功能请求] 启用 sm103 cute_dsl mm_fp4 内核**
  - 目前该内核由于需要 CUDA 13.1 和 Cute DSL 4.4.0 尚未启用，等待新容器环境验证后上线。[Issue #2621](https://github.com/flashinfer-ai/flashinfer/issues/2621)
- **[功能请求] 自动调优器支持缓存配置结果**
  - 希望 FlashInfer 的自动调优器能够将最优策略持久化存储并在后续运行中加载，减少冷启动开销。[Issue #2620](https://github.com/flashinfer-ai/flashinfer/issues/2620)
### Dao-AILab/flash-attention
提交：
- [Cute] Handle window_size=(-1, -1) for non-local attention (#2251) [`5678dd9`](https://github.com/Dao-AILab/flash-attention/commit/5678dd909aca97f925957dab716022046fb1e44f)
- 受影响文件: flash_attn/cute/interface.py
- [Fwd,Sm100] Use pipeline abstraction for loading Q and KV [`a595ceb`](https://github.com/Dao-AILab/flash-attention/commit/a595cebb74d784ce0b25ce8d48f58dbc201926a9)
- 受影响文件: flash_attn/cute/flash_fwd_sm100.py, flash_attn/cute/pipeline.py
- [Fwd,Sm100] Use position_independent for sO, more clean up [`8d9e28b`](https://github.com/Dao-AILab/flash-attention/commit/8d9e28bda911ece3ef79b269d8c47fd19d26a909)
- 受影响文件: flash_attn/cute/blackwell_helpers.py, flash_attn/cute/flash_fwd_sm100.py, flash_attn/cute/mma_sm100_desc.py
- [DSL] Don't need to parse swizzle from str anymore [`9136b0c`](https://github.com/Dao-AILab/flash-attention/commit/9136b0c202113803e84c8a9ab15798020c045743)
- 受影响文件: flash_attn/cute/blackwell_helpers.py, flash_attn/cute/mma_sm100_desc.py, flash_attn/cute/utils.py
- [Fwd,Sm100] Use pipeline_kv in load_KV instead of raw mbarrier [`6287355`](https://github.com/Dao-AILab/flash-attention/commit/628735565a38a3eb4a9ff085708f83d963bb102a)
- 受影响文件: flash_attn/cute/flash_fwd_sm100.py
### sgl-project/sglang
- **9fac90d** – *Tiny enhance the dp attention load balance benchmark*  
  [Commit](https://github.com/sgl-project/sglang/commit/9fac90d85d9507d1f63cbc6192da5a560b00241a)  
  - 在 `python/sglang/test/test_utils.py` 中为 `get_benchmark_args` 增加 `max_concurrency` 参数。  
    ```python
    def get_benchmark_args(..., max_concurrency=None):
        ...
        max_concurrency=max_concurrency,
    ```  
  - `test/registered/distributed/test_disaggregation_dp_attention.py` 新增 `test_bench_serving` 用例并把 CI 预估时间从 155 min 调整至 580 min。

- **672b666** – *add new ci user*  
  [Commit](https://github.com/sgl-project/sglang/commit/672b66605727133e1136d0736b81dcffaf3b0c05)  
  - 在 `.github/CI_PERMISSIONS.json` 中为用户 **narutolhy** 添加 CI 权限（可标记、重跑、无冷却）。

- **2aa3fe3** – *fix the teardown output of disaggregation test*  
  [Commit](https://github.com/sgl-project/sglang/commit/2aa3fe394e2debaf7f1aafc3933f5f6321d0ac59)  
  - `python/sglang/test/test_utils.py` 中 `popen_with_error_check` 改为不捕获 stdout/stderr，直接 `process.wait()`，并把线程设为 daemon。  
    ```python
    process = subprocess.Popen(command, stdout=None, stderr=None)
    process.wait()
    ```

- **2274bfeb** – *Support query dp rank from bootstrap server*  
  [Commit](https://github.com/sgl-project/sglang/commit/2274bfebb194f83ac1f2b69e100a27c4a27d94d2)  
  - 文档 `docs/advanced_features/pd_disaggregation.md` 新增一行说明 `SGLANG_DISAGGREGATION_THREAD_POOL_SIZE`。  
  - 大幅改动 `python/sglang/srt/disaggregation/*`，实现从 bootstrap server 查询 DP rank 的逻辑（代码量 +321 行，删除 -139 行）。

- **2cdde5d** – *Migrate AWQ marlin repack kernel to JIT*  
  [Commit](https://github.com/sgl-project/sglang/commit/2cdde5d4abb07d31b741bb78e4178aceb7ac44fb)  
  - 新增 JIT 编译支持：`awq_dequantize.py`, `awq_marlin_repack.py` 以及对应 C++ CUDA 实现 `awq_dequantize.cuh`, `awq_marlin_repack.cuh`。  
  - 添加基准测试 `bench_awq_*` 与单元测试 `test_awq_*`，共计 1337 行新增。

- **e0e0cad** – *Match rotary_embedding module name style*  
  [Commit](https://github.com/sgl-project/sglang/commit/e0e0cad6bc4327613e805543b28f53165ab1750c)  
  - 调整 `python/sglang/multimodal_gen/runtime/layers/rotary_embedding/` 包内文件命名，使其统一为 `__init__.py`, `base.py`, `factory.py` 等。

- **2717393** – *Split rotary_embedding.py into a modular package*  
  [Commit](https://github.com/sgl-project/sglang/commit/2717393681a8e8df355786f274a2e853abda2d9f)  
  - 将单文件 `rotary_embedding.py` 拆分为完整包，新增 9 个模块（`base.py`, `factory.py`, `mrope.py`, `rope_variant.py` 等），代码行数增加 3572 行，删除旧文件 3766 行。

- **27d394a** – *bump sgl-kernel-npu version*  
  [Commit](https://github.com/sgl-project/sglang/commit/27d394a6a51f4a3fa168bf7c63f51a48204dcb52)  
  - 更新 NPU CI Dockerfile，升级 `sgl-kernel-npu` 至 `2026.02.01.post2`。

- **d80c884** – *Use single mma warp group for short q_len in FA*  
  [Commit](https://github.com/sgl-project/sglang/commit/d80c884a279bd7fdb8fd1e908bf36b07c268a04e)  
  - 在 `sgl-kernel/CMakeLists.txt` 中加入编译选项，针对短查询长度使用单个 warp 组，提高解码吞吐。

- **6448de7** – *Reorganize topk logic*  
  [Commit](https://github.com/sgl-project/sglang/commit/6448de7e9690e5bf1df1828dd3e433c2fe1d728c)  
  - `python/sglang/srt/layers/moe/topk.py` 重构 Top‑K 逻辑，删除冗余代码，提升可读性。

- **4f3dc1e** – *Use unreg path for custom all-reduce during CUDA graph capture*  
  [Commit](https://github.com/sgl-project/sglang/commit/4f3dc1ef5b621a3d0f3827be7a42a368c4c48d35)  
  - 在 `custom_all_reduce.py` 中加入两行代码，切换到未注册路径以避免图捕获冲突。

- **c3c1532** – *Detect Flux2 custom VAE path from component_paths*  
  [Commit](https://github.com/sgl-project/sglang/commit/c3c15328207b798fbedf0f46d6c8466f7225b5c0)  
  - `pipeline_configs/base.py` 新增对 Flux2 VAE 自定义路径的自动检测。

- **6a999db** – *ENV flags tuning and cleanup for AMD*  
  [Commit](https://github.com/sgl-project/sglang/commit/6a999dbdf8d38dc09f8b7d971b9cbac680766852)  
  - 精简 `docker/rocm*.Dockerfile` 中的环境变量，仅保留必要标记。

- **42b1019** – *Fix bench_one_batch_server print statements*  
  [Commit](https://github.com/sgl-project/sglang/commit/42b1019881b228efbea55ac8101b17afb761c62d)  
  - 移除 `bench_one_batch_server_internal.py` 中的调试 `print`，保持日志整洁。

- **3224836** – *Install amdsmi for QuickReduce on rocm7.2*  
  [Commit](https://github.com/sgl-project/sglang/commit/3224836d8be3b3d95ae215f0a8da5dd5867559b7)  
  - 在 `docker/rocm720.Dockerfile` 中加入 `amdsmi` 包，以支持 QuickReduce 初始化。

- **2472e47** – *Revert graph input buffers refactor*  
  [Commit](https://github.com/sgl-project/sglang/commit/2472e47d731b8fe352aa5083926a6bf9a486328e)  
  - 恢复 `cuda_graph_runner.py`、`input_buffers.py` 等文件的旧实现，删除 700 行改动，保留 494 行新增以兼容旧版图执行。

- **fa80b9b** – *Skip some subtests for tool call parser*  
  [Commit](https://github.com/sgl-project/sglang/commit/fa80b9beba632a5be26c4e8c894d35c379386fda)  
  - 在 `tool_call_test_runner.py` 中标记部分子测试为跳过，缩短 CI 时长。

- **0e670c3** – *Enforce strict input_reference validation for T2V*  
  [Commit](https://github.com/sgl-project/sglang/commit/0e670c36d6f08eae207ecf9d4b83d7ca25c77d87)  
  - 更新 `sampling_params.py` 与 `video_api.py`，加入对 `input_reference` 的严格检查，防止非法输入导致运行错误。

---

- **[Tracking][Performance][AMD] Qwen3-Next Latency Optimization on AMD MI300X with 4‑GPU TP4 Deployment**  
  <https://github.com/sgl-project/sglang/issues/19196>  
  作者: **sunxxuns** · 创建于 2026‑02‑23 22:59  
  目标在 MI300X 上对 Qwen3‑Next‑80B‑A3B（MoE + GDN + Full‑Attention）进行 4‑GPU TP4 延迟优化。

- **[diffusion] OS support: Windows, MacOS**  
  <https://github.com/sgl-project/sglang/issues/19185>  
  作者: **mickqian** · 创建于 2026‑02‑23 14:52  
  提议为 Diffusion 功能添加 Windows 与 macOS 平台支持。

- **[Bug] [Diffusion] multimodal-gen-test-1-npu-a3 failed**  
  <https://github.com/sgl-project/sglang/issues/19182>  
  作者: **ping1jing2** · 创建于 2026‑02‑23 11:31  
  NPU 测试因缺失日志改动导致失败，需尽快修复。
### vllm-project/vllm
提交：
- [RL] Validation for pause_mode='keep' (#34992) [`596ed1f`](https://github.com/vllm-project/vllm/commit/596ed1f02ef2a068e459a019d7d3f54593cc7e9e)
- 受影响文件: .buildkite/test_areas/distributed.yaml, examples/offline_inference/new_weight_syncing/rlhf_async_new_apis.py
- [Misc] Monitor interface changes (#35113) [`b8d8b7e`](https://github.com/vllm-project/vllm/commit/b8d8b7e934a572717273c5f635a644774814869c)
- 受影响文件: .github/CODEOWNERS
- Enforce that `model` is the first positional arg when `--served-model-name` is used (#34973) [`28c5e69`](https://github.com/vllm-project/vllm/commit/28c5e69ba0c4ccc9b33e05bfb757052b68af4282)
- 受影响文件: tests/entrypoints/openai/test_cli_args.py, vllm/utils/argparse_utils.py
- Fix custom processors that use deleted import for Transformers v5 (#35101) [`864167d`](https://github.com/vllm-project/vllm/commit/864167d37690f0ac1b94ae9938fc0fdffbc51225)
- 受影响文件: vllm/transformers_utils/processor.py
- [Bugfix] Fix prefix caching for Mamba 'all' mode (Nemotron models) (#34874) [`a2ba6a5`](https://github.com/vllm-project/vllm/commit/a2ba6a52443f9823d55a7f779c48c5de98abbc80)
- 受影响文件: tests/v1/attention/test_mamba_update_block_table.py, vllm/v1/attention/backends/mamba_attn.py
- Use Xet high performance mode for Transformers v5 (#35098) [`c4f3869`](https://github.com/vllm-project/vllm/commit/c4f38696f759099c9295a8d65a25d4e3171d2737)
- 受影响文件: vllm/model_executor/model_loader/weight_utils.py
- [Bugfix] Fix MRotaryEmbedding missing `truncate` attr with YaRN scaling (#35080) [`a7f341c`](https://github.com/vllm-project/vllm/commit/a7f341c32326fe5463556c69cfa54d90281041c8)
- 受影响文件: vllm/model_executor/layers/rotary_embedding/mrope.py
- [CI] Skip Responses API (#34990) [`d13ece3`](https://github.com/vllm-project/vllm/commit/d13ece38d73adeafab8f9aa60c73a46ef741f6c8)
- 受影响文件: tests/entrypoints/openai/responses/test_harmony.py
- [Metrics] Add Prometheus counters for Model FLOPs Utilization (MFU) (#30950) [`5cc7c44`](https://github.com/vllm-project/vllm/commit/5cc7c4452e48b4492c47ff7e130751d7a786dbf9)
- 受影响文件: docs/mkdocs/hooks/generate_metrics.py, docs/usage/metrics.md, vllm/v1/metrics/loggers.py, vllm/v1/metrics/perf.py, vllm/v1/metrics/ray_wrappers.py
- [kv-cache, ct] Use compressed-tensors as a source of ground-truth for quant strategies (#34254) [`b95bb69`](https://github.com/vllm-project/vllm/commit/b95bb6927f7ea5f00f60b76032eb23b0128f3c48)
- 受影响文件: vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
- [Refactor] Decouple TimingContext from InputProcessingContext (#35083) [`3926454`](https://github.com/vllm-project/vllm/commit/392645454b34fec6ff2b690465b275e773e6af09)
- 受影响文件: tests/models/multimodal/processing/test_common.py, tests/models/multimodal/processing/test_gemma3.py, tests/models/multimodal/processing/test_glm4_1v.py, tests/models/multimodal/processing/test_h2ovl.py, tests/models/multimodal/processing/test_idefics3.py, tests/models/multimodal/processing/test_internvl.py, tests/models/multimodal/processing/test_llama4.py, tests/models/multimodal/processing/test_llava_next.py, tests/models/multimodal/processing/test_llava_onevision.py, tests/models/multimodal/processing/test_minimax_vl_01.py, tests/models/multimodal/processing/test_nemotron_vl.py, tests/models/multimodal/processing/test_phi3v.py, tests/models/multimodal/processing/test_phi4mm.py, tests/models/multimodal/processing/test_qwen2_vl.py, tests/models/multimodal/processing/test_qwen3_omni.py, tests/models/multimodal/processing/test_smolvlm.py
- 文件列表已截断。
- [Llama4,CI] Bring back Llama-4 bug fixes, and also fix Maverick tests (#35033) [`1e8438a`](https://github.com/vllm-project/vllm/commit/1e8438a89a6453a6b1ba28798bb9c51d6364ed96)
- 受影响文件: tests/models/multimodal/generation/test_maverick.py, vllm/model_executor/models/llama4.py
- [ModelBash][DSV3] Add TRTLLM DSV3 Router GEMM kernel (6% B1 Speedup) (#34302) [`8435b2e`](https://github.com/vllm-project/vllm/commit/8435b2e0492525b2ae7361b7f0081febb483bb34)
- 受影响文件: CMakeLists.txt, csrc/moe/dsv3_router_gemm_bf16_out.cu, csrc/moe/dsv3_router_gemm_entry.cu, csrc/moe/dsv3_router_gemm_float_out.cu, csrc/moe/dsv3_router_gemm_utils.h, csrc/moe/moe_ops.h, csrc/moe/torch_bindings.cpp, vllm/_custom_ops.py, vllm/model_executor/models/deepseek_v2.py
- [XPU] allow TORCH_SDPA/TRITON_ATTN as XPU vit Backend (#35010) [`b1b5e04`](https://github.com/vllm-project/vllm/commit/b1b5e045dfdcb4ca31931e05fafa8ea15823d740)
- 受影响文件: vllm/model_executor/layers/attention/mm_encoder_attention.py, vllm/platforms/xpu.py
- [ROCm][CI] Fix spec decode profile assertion and logprob test determinism (#35043) [`5f68464`](https://github.com/vllm-project/vllm/commit/5f68464f92af79b0b851edc7f2bba7c861069713)
- 受影响文件: tests/v1/sample/test_logprobs.py
- [CLEANING] Remove unused disable_by_batch_size from SpeculativeConfig (#35060) [`aa08a30`](https://github.com/vllm-project/vllm/commit/aa08a30fc90248006ce6202496926f074149b08c)
- 受影响文件: vllm/config/speculative.py
- [Refactor] Remove dead private func `_fp8_perm` and `_extract_mask_for_item` (#35068) [`7f40e9e`](https://github.com/vllm-project/vllm/commit/7f40e9e5164af6af34bcbd5945356d416e29e71a)
- 受影响文件: vllm/model_executor/layers/fused_moe/utils.py, vllm/model_executor/models/glmasr_utils.py
- Fix pipeline parallel with embed scaling in the Transformers modelling backend (#35094) [`103e614`](https://github.com/vllm-project/vllm/commit/103e614b1487fd58477bbe7354a3cb2e9162e388)
- 受影响文件: vllm/model_executor/models/transformers/base.py
- [Feature] Lazy import for the "mistral" tokenizer module. (#34651) [`54e2f83`](https://github.com/vllm-project/vllm/commit/54e2f83d0a82462e0128e5d852e3d46fbb566a7f)
- 受影响文件: tests/models/multimodal/processing/test_common.py, tests/reasoning/utils.py, vllm/entrypoints/llm.py, vllm/entrypoints/openai/chat_completion/serving.py, vllm/entrypoints/openai/engine/serving.py, vllm/entrypoints/pooling/score/serving.py, vllm/multimodal/processing/context.py, vllm/sampling_params.py, vllm/tokenizers/mistral.py, vllm/tool_parsers/hermes_tool_parser.py, vllm/tool_parsers/jamba_tool_parser.py, vllm/tool_parsers/mistral_tool_parser.py, vllm/utils/mistral.py, vllm/v1/structured_output/backend_xgrammar.py
- fix: Apply embedding_multiplier to inputs_embeds (#34813) [`e631f8e`](https://github.com/vllm-project/vllm/commit/e631f8e78ef78fe6cf13903e27c827b45b25a0d0)
- 受影响文件: vllm/model_executor/models/granitemoehybrid.py
- [BugFix]: Fix local mypy issues (#34739) [`e97c46a`](https://github.com/vllm-project/vllm/commit/e97c46a92ded70813806fd6f33cf8bd9fcf005bc)
- 受影响文件: vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py, vllm/entrypoints/openai/chat_completion/protocol.py, vllm/entrypoints/openai/completion/protocol.py, vllm/entrypoints/openai/responses/protocol.py, vllm/entrypoints/openai/responses/serving.py, vllm/sampling_params.py
- [Bugfix] Fix  kernel benchmark (#33752) [`7291d1b`](https://github.com/vllm-project/vllm/commit/7291d1b288558d48508e1a17c37b0aa170332264)
- 受影响文件: benchmarks/fused_kernels/layernorm_rms_benchmarks.py, benchmarks/kernels/benchmark_activation.py, benchmarks/kernels/benchmark_block_fp8_gemm.py, benchmarks/kernels/benchmark_fp8_gemm.py, benchmarks/kernels/benchmark_int8_gemm.py, benchmarks/kernels/benchmark_layernorm.py, benchmarks/kernels/benchmark_mrope.py, benchmarks/kernels/benchmark_mxfp4_qutlass.py, benchmarks/kernels/benchmark_nvfp4_gemm.py, benchmarks/kernels/benchmark_nvfp4_quant.py, benchmarks/kernels/benchmark_nvfp4_qutlass.py, benchmarks/kernels/benchmark_per_token_quant_fp8.py, benchmarks/kernels/benchmark_rope.py, vllm/benchmarks/lib/utils.py
- [Refactor] Simplify dummy data generation (#35025) [`987506b`](https://github.com/vllm-project/vllm/commit/987506bca63d95d9ff4ebb3a37e3f2447fa0757c)
- 受影响文件: docs/contributing/model/multimodal.md, tests/models/multimodal/processing/test_audioflamingo3.py, tests/models/multimodal/processing/test_common.py, tests/models/multimodal/processing/test_tensor_schema.py, vllm/config/multimodal.py, vllm/model_executor/models/aria.py, vllm/model_executor/models/audioflamingo3.py, vllm/model_executor/models/aya_vision.py, vllm/model_executor/models/bagel.py, vllm/model_executor/models/bee.py, vllm/model_executor/models/blip2.py, vllm/model_executor/models/chameleon.py, vllm/model_executor/models/clip.py, vllm/model_executor/models/cohere2_vision.py, vllm/model_executor/models/colmodernvbert.py, vllm/model_executor/models/deepseek_ocr.py
- 文件列表已截断。
- [Model Runner V2] Remove propose_draft method (#35070) [`c645e9a`](https://github.com/vllm-project/vllm/commit/c645e9a2144f7fede7222c0fc8937a93def04402)
- 受影响文件: vllm/v1/worker/gpu/model_runner.py
- [Model Runner V2][Minor] Remove redundant `do_spec_decode` field (#35039) [`944ffb5`](https://github.com/vllm-project/vllm/commit/944ffb59680c0210ec54ddb43a3c7ef015e1f842)
- 受影响文件: vllm/v1/worker/gpu/model_runner.py
Issues：
- [Performance]: Optimized Deployment Recipe for Qwen3.5 (https://github.com/vllm-project/vllm/issues/35154)
- Issue 内容已截断。
- [Feature]:  Support NVFP4 Checkpoint of Qwen3.5 (https://github.com/vllm-project/vllm/issues/35150)
- [Performance]: Optimize GDN Decode (https://github.com/vllm-project/vllm/issues/35149)
- [Feature]: Sequence Parallel Support for Model Runner V2 (https://github.com/vllm-project/vllm/issues/35141)
- [CI] Ultravox audio model HuggingFace reference produces invalid output with NaN logprobs (https://github.com/vllm-project/vllm/issues/35140)
- Issue 内容已截断。
- [Bug]: Qwen/Qwen3.5-397B-A17B-FP8 and Qwen/Qwen3.5-397B-A17B has accuracy issues when running with Flashinfer Attention backend on Blackwell. (https://github.com/vllm-project/vllm/issues/35138)
- Issue 内容已截断。
- [CI Failure]: Fusion E2E Quick (H100) `test_tp1_fp8_fusions` (https://github.com/vllm-project/vllm/issues/35134)
- Issue 内容已截断。
- [CI Failure]:  mi355_4: LoRA TP Test (Distributed) (https://github.com/vllm-project/vllm/issues/35133)
- Issue 内容已截断。
### NVIDIA/cutile-python
昨日无更新。

## 总结
- Diff 内容已截断以满足 prompt 预算。
- OpenRouter repo summarize failed for Dao-AILab/flash-attention: OpenRouter 429 Too Many Requests (z-ai/glm-4.5-air:free): {"error":{"message":"Provider returned error","code":429,"metadata":{"raw":"z-ai/glm-4.5-air:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations","provider_name":"Z.AI","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
- OpenRouter repo summarize failed for vllm-project/vllm: OpenRouter 429 Too Many Requests (z-ai/glm-4.5-air:free): {"error":{"message":"Provider returned error","code":429,"metadata":{"raw":"z-ai/glm-4.5-air:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations","provider_name":"Z.AI","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
- OpenRouter global summarize failed: OpenRouter 429 Too Many Requests (z-ai/glm-4.5-air:free): {"error":{"message":"Provider returned error","code":429,"metadata":{"raw":"z-ai/glm-4.5-air:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations","provider_name":"Z.AI","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
