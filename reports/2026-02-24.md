今日的 AI Infra 的新闻如下。

## 摘要
模型摘要不可用，以下为原始数据整理。

## 具体内容分析
### deepseek-ai/DeepGEMM
昨日无更新。
### deepseek-ai/FlashMLA
昨日无更新。
### NVIDIA/cutlass
## 提交

### NVIDIA/cutlass

**提交:** [0853d81](https://github.com/NVIDIA/cutlass/commit/0853d81d70912cf4245d23ed3b3aaaad15e31421) - "Revise README"  
**作者:** Haicheng Wu  
**日期:** 2026-02-24  
**文件:** README.md  
**变更:** 25 处 (7 新增, 18 删除)  
**代码片段:**
```diff
- * Advanced compiler control
- Use 'Advanced compiler control' for mixed input gemm examples for better performance.
- Advanced compiler control is an experimental feature of CUDA compiler...
+ * Use 'Advanced control file' for mixed input gemm examples for better performance.
+   - Advanced control file is an experimental feature of CUDA compiler...
```
*注意: 补丁被截断*

## 问题

### [BUG] 编译错误：在 C++20 和 MSVC 环境下包含 "cute/tensor.hpp" 时出错  
**仓库:** NVIDIA/cutlass  
**问题编号:** #3065  
**作者:** edgchen1  
**创建时间:** 2026-02-24  
**评论数:** 0  
**组件:** CUTLASS C++  
**问题描述:** 在使用 MSVC 编译 C++20 并包含 "cute/tensor.hpp" 时出现编译错误  
*注意: 问题正文被截断*

### [DOC] 文档问题：基础类型 -> 数值转换部分存在空代码块  
**仓库:** NVIDIA/cutlass  
**问题编号:** #3064  
**作者:** vidyasiv  
**创建时间:** 2026-02-24  
**评论数:** 0  
**问题描述:** 在 Fundamental Types -> Numeric Conversion 部分缺少代码块

### [BUG] Lambda 函数错误：cutedsl 4.4.0.dev1 版本问题  
**仓库:** NVIDIA/cutlass  
**问题编号:** #3061  
**作者:** Edenzzzz  
**创建时间:** 2026-02-24  
**评论数:** 2  
**组件:** CuTe DSL  
**问题描述:** Lambda 函数在 4.4.0.dev0 版本中正常工作，但在 dev1 版本中出错  
*注意: 问题正文被截断*
### flashinfer-ai/flashinfer
## 提交

**flashinfer-ai/flashinfer**

- **新增 TensorRT-LLM tinygemm2 的 BF16 RouterGEMM 支持**
  - 在 `csrc/tinygemm2.cu` 中添加了新的 CUDA 实现，支持 SM90+ 架构上的 BF16 小矩阵乘法（带 bias）。
  - 修改 `flashinfer/gemm/__init__.py` 和 `flashinfer/gemm/routergemm.py` 暴露新接口 `tinygemm_bf16`。
  - 新增 JIT 编译支持：`flashinfer/jit/tinygemm2.py`。
  - 添加测试用例：`tests/model_optimizations/test_tinygemm2.py`。
  - [Commit: 41545e5](https://github.com/flashinfer-ai/flashinfer/commit/41545e5bd8be737a3969cd467cd5d88d9e3b8dd0)

---

## Issues

- **FlashInfer 在单节点 TP>1 集群中使用 NV Fabric 时崩溃**
  - 影响版本 > 0.5.3。问题出现在 `mnnvl.py` 中的 `SymmDeviceMemory` 初始化过程。
  - 触发路径与 VLLM 的 AOT 编译和 AllReduceFusion 相关。
  - Issue: [#2633](https://github.com/flashinfer-ai/flashinfer/issues/2633)

- **OE2m1 + ForGen 缺少 VarSeqQ32/64/128 支持**
  - 关联 vLLM 和 TensorRT-LLM 的相关 issue。
  - Issue: [#2632](https://github.com/flashinfer-ai/flashinfer/issues/2632)
### Dao-AILab/flash-attention
昨日无更新。
### sgl-project/sglang
**2026‑02‑24 代码更新概览**

- **c193a52** – *AMD*：在 `deepseek_v2.py` 中引入 `get_is_capture_mode`，并对 AIter 后端的条件分支进行整理，去除冗余导入。  
  - 关键文件：`python/sglang/srt/models/deepseek_v2.py`（+13 / -9 行）  
  - 备注：部分 `if _use_aiter` 逻辑被重新排列，确保在非捕获模式下仍能正确加载 AIter ops。  

- **152560d** – *PCG*：新增 GPT‑OSS 模型的分段 CUDA 图测试。  
  - 关键文件：`test/registered/models/test_gpt_oss_models_pcg.py`（+72 行）  
  - 备注：注册 CUDA CI，设置 2‑GPU TP，开启分段图与推理解析器。  

- **ae6f6e1** – *Benchmark 重构*：将 `sample_random_requests` 移动至 `benchmark/datasets/random.py`，统一数据集接口。  
  - 关键文件：`benchmark/hicache/bench_mix.py`、`bench_multiturn.py`（各 -1 / +1 行）  

- **ae6f6e1**（续） – *Benchmark 数据集抽象*：在 `benchmark/datasets/common.py` 中新增抽象基类 `BaseDataset`，为所有数据集实现统一加载接口。  
  - 关键文件：`benchmark/datasets/common.py`（+16 行）  

- **ae6f6e1**（续） – *各子数据集实现*：为 `custom、generated_shared_prefix、image、mmmu、mooncake、openai_dataset、random、sharegpt` 添加对应 `@dataclass` 实现，继承 `BaseDataset` 并提供 `from_args` 与 `load` 方法。  
  - 关键文件示例：`benchmark/datasets/custom.py`（+42 行），`generated_shared_prefix.py`（+54 行），`image.py`（+49 行），`mmmu.py`（+28 行），`mooncake.py`（+41 行），`openai_dataset.py`（+28 行）  
  - 备注：若 `patch_truncated` 为 true，说明完整代码片段未展示，实际实现请参考仓库。  

- **65de904** – *CI*：新增 Docker 镜像重新标记工作流 `retag-docker.yml`（+30 行）。  

- **0c224c3** – *文档*：在 Sphinx 文档中嵌入 Release Lookup 工具，新增 `docs/references/release_lookup.rst`（+325 行），并在首页做微调（+1 行）。  

- **38f25e8** – *DeepSeek*：修正 README 中的链接错误（+1 / -1 行）。  

- **a064251** – *Bugfix*：在 `schedule_batch.py` 中去除多余的 `+1` 计数，防止请求尾部字符串错误（+9 / -7 行）。  

- **9bce3b0** – *Diffusion*：更新 NPU 性能基准文件 `perf_baselines_npu.json`（+59 / -59 行），保持基准对称。  

- **e6ad58e** – *Diffusion*：微调 WebUI 返回数据的逻辑（+1 行）。  

- **1efc33c** – *Diffusion*：统一注意力后端实现，删除冗余枚举定义（-4 / -5 / -66 行）。  

- **ea1bc1c** – *Diffusion*：在 `usp.py` 中优化 All‑to‑All 通信（+50 / -53 行）。  

- **6e54361** – *CUDA 图*：大幅重构输入缓冲区管理，新增共享缓冲池，实现更高效的图捕获与复用（`cuda_graph_runner.py` +203 / -6 行，`input_buffers.py` +46 / -199 行等）。  

- **9494676** – *环境变量*：将默认环境变量值设为 `None`，避免意外覆盖（+1 / -1 行）。  

- **fcfd964** – *Diffusion*：为 LTX‑2 模型添加 PR3 支持，更新相关配置与模型连接器（累计 +237 / -70 行）。  

- **4f25a48** – *NPU*：在 `xverse_moe` 上实现 NPU 专用 MoE 量化方法（+79 行）。  

- **5e80027** – *PD‑Disagg*：新增 `FAKE` 传输后端以支持更多场景（+3 行）。  

- **02a646b** – *CI 权限*：更新 CI 权限文件 `CI_PERMISSIONS.json`（+7 行）。  

- **d8ef33a** – *NPU 初始化*：优先调用 `init_npu_backend`，解决多种初始化错误（-5 行）。  

- **59e7ea9** – *Docker*：修正发布 Docker 工作流参数错误（+2 / -1 行）。  

- **feb041f** – *PD‑Disagg*：改进 `conn.py` 类型提示，统一解码/前填信息流（累计 +72 / -44 行）。  

- **edba96b** – *Docker*：将 ROCm Dockerfile 从 `setuptools‑rust` 迁移至 `maturin`（+3 / -3 行）。  

- **ea7ef63** – *PD‑Disagg*：抽取 KVManager 公共方法至 `CommonKVManager`，减少重复实现（+20 / -66 行）。  

- **8aeb16f** – *代码风格*：在 `serving_transcription.py` 中补充缺失的空行（+1 行）。  

- **581bf53** – *Whisper*：新增 Whisper 模型支持、音频转录端点 `/v1/audio/transcriptions`，并提供基准脚本与文档（累计 +1673 / -6 行）。  

- **3a11e7d** – *Docker*：修补 Docker 镜像构建脚本（+26 / -10 行）。  

- **d160c5b** – *PD‑Disagg*：统一 Prefill 信息流，使用 `PrefillServerInfo` 结构（+68 / -79 行）。  

---

### 近期 Issue 速览

- **[AMD] Aiter attention backends crash on hybrid Mamba+attention models**  
  <https://github.com/sgl-project/sglang/issues/19272>  
  *描述：AIter 在混合模型的第 0 层读取 `v_head_dim` 时抛出 `ValueError`，因层 ID 转换不当。*  

- **[Bug] Qwen3.5 + pddisagg: Error querying dp_ranks from bootstrap**  
  <https://github.com/sgl-project/sglang/issues/19271>  
  *描述：在分布式配置下查询 `dp_ranks` 失败，导致连接错误。*  

- **[Feature] Support other moe runners with flashinfer a2a backend**  
  <https://github.com/sgl-project/sglang/issues/19270>  
  *需求：让 `flashinfer_a2a` 与更多 MoE runner 后端兼容，以提升宽度扩展性能。*  

- **bug: mm_cache not working on qwens‑vl‑8b**  
  <https://github.com/sgl-project/sglang/issues/19267>  

- **[Bug] Anthropic API swallows thinking tokens and ignores thinking control parameters**  
  <https://github.com/sgl-project/sglang/issues/19257>  
  *描述：在 Anthropic 兼容的 `/v1/messages` 接口中，思考阶段的 token 被吞掉，控制参数失效。*  

- **[Bug] deepep auto mode + cuda graph will hang**  
  <https://github.com/sgl-project/sglang/issues/19252>  

- **Hardcoded src=0 in update_weights_from_distributed causes deadlock with multi‑GPU torchrun training**  
  <https://github.com/sgl-project/sglang/issues/19251>  

- **[Feature] Support OpenAI base64 audio API**  
  <https://github.com/sgl-project/sglang/issues/19235>  

---

> **提示**：若某提交的 `patch_truncated` 为 `true`，表示展示的代码片段不完整，请在对应 PR 页面查看完整 diff。若 Issue 的 `body_truncated` 为 `true`，请前往 Issue 链接获取完整描述。
### vllm-project/vllm
**提交概览**  

- **576fe50** – 添加 Nemotron FP8 Triton MoE 配置（`vllm/model_executor/layers/fused_moe/configs/E=128,N=1856,device_name=NVIDIA_H200,dtype=fp8_w8a8.json`）  
  - 新增 147 行 JSON 配置文件，用于在 H200 GPU 上以 fp8_w8a8 精度运行 Nemotron MoE。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/576fe50333a8a8fc91ee28595779452f3d997d32  

- **a0e50a4** – 将 `wvSplitKQ` 转换为 16×16 MFMA，为 mi4xx 做准备（`csrc/rocm/skinny_gemms.cu`）  
  - 代码改动 14 行新增、46 行删除，核心实现改为使用 MFMA 指令提升矩阵乘法效率。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/a0e50a4260d20d021d6caa137a078ae2d16a8f93  

- **9fa5b25** – 修复 DeepGEMM 稀疏注意力始终准备元数据的 bug（`vllm/v1/attention/backends/mla/indexer.py`）  
  - 小幅改动：新增 4 行、删除 2 行，确保仅在需要时创建稀疏索引。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/9fa5b25a238c08fae8acf507e5dbc923f5b2e5cb  

- **ea97750** – 修复分布式测试配置错误（`.buildkite/test_areas/distributed.yaml`）  
  - 仅修改 1 行，平衡 `distributed` 测试区域的 YAML 配置。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/ea97750414844743afb677c33eb4af2c9e34dc8d  

- **067c5d9** – 为 ROCm CI 添加 MI325 镜像并更新多项测试脚本（涉及 `.buildkite/*`、`docker/Dockerfile.rocm`、`tests/v1/kv_connector/unit/test_moriio_connector.py`）  
  - 主要改动：`run-amd-test.sh` 新增 237 行、删除 163 行；其他 YAML 与 Dockerfile 小幅增删。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/067c5d9ad1be71a36f7878631d97d7239dd097ab  

- **f5972a8** – 为 Nemotron‑H 添加 MTP 与 Mamba 推测解码支持（新增 `vllm/model_executor/models/nemotron_h_mtp.py`，共 503 行新增）  
  - 关键改动：实现 MTP（Mixture‑of‑Token‑Proposals）和 Mamba 结构的推测解码路径。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/f5972a872fa3fabd94b7a6c6f031f4b5bcee2b2d  

- **a9e15e0** – 将 MatthewBonanni 加入 CODEOWNERS（`.github/CODEOWNERS`）  
  - 仅修改 4 行添加/删除，更新代码所有权。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/a9e15e040de12f2aaa97c6c71d7b1540cbe2561f  

- **542ca66** – 恢复 CI 中的 OpenTelemetry 安装（`.buildkite/test-amd.yaml`、`.buildkite/test_areas/misc.yaml`）  
  - 新增 15 行，重新引入 OpenTelemetry 依赖。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/542ca66357e7128d43f67b2013c7adfce77829d1  

- **fc8456c** – 修正 CI 中 kernels 测试路径（`.buildkite/test_areas/kernels.yaml`）  
  - 仅 1 行改动，确保 kernels 测试被正确发现。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/fc8456c3367007ae6997f86aa903aaa936c05f99  

- **9ce8fad** – 使用 `itertools.islice` 优化结构化输出的 Python 切片（多文件 `vllm/reasoning/*`、`vllm/v1/structured_output/__init__.py`）  
  - 小幅改动：共计 23 行新增、16 行删除，提升切片性能。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/9ce8fad2a9fe010ede46b085f6cb4099c8ec402d  

- **c38b8d5** – 移除不使用的 `padding_index` 以兼容 Transformers v5（涉及多模型文件）  
  - 每个模型文件均删除 1 行 `padding_index` 定义，共计 14 行删除。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/c38b8d5a317da356353a0a9ae1ab87f612267fb3  

- **60da0e1** – 删除重复的 kernels 测试条目（`.buildkite/test_areas/kernels.yaml`）  
  - 删除 11 行，仅保留 1 行新增。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/60da0e1544086949e7926623cbb708f20c489268  

- **9609b1f** – 将 flashinfer `mm_mxfp8` 集成到 ModelOpt MXFP8（`vllm/model_executor/layers/quantization/modelopt.py`、`vllm/model_executor/layers/quantization/utils/mxfp8_utils.py`、`vllm/utils/flashinfer.py`）  
  - 关键改动：新增 230 行实现 MXFP8 乘法，删除 11 行旧实现。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/9609b1f18def2c55f95e2a354e7938efee457c38  

- **a0c7081** – 修复 flashinfer 自动调优器在 `trtllm_fp4_block_scale_moe` 场景下回退默认策略的 bug（`vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`）  
  - 仅 2 行改动，确保 fallback 正常工作。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/a0c70816956298f7dd1d0cf47cfa1a169a413692  

- **34ce0ff** – 为 s390x CPU 使用向量指令加速 Attention（`csrc/cpu/cpu_attn_vxe.hpp`、`csrc/cpu/generate_cpu_attn_dispatch.py` 等）  
  - 新增 386 行 SIMD 实现，整体提升约 5% 吞吐。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/34ce0ffd1f3c7b5e2bcc6073dd1525f266133755  

- **0de5333** – 修复 GLM4 解析器测试失败（`tests/tool_parsers/test_glm4_moe_tool_parser.py`）  
  - 关键改动：新增 67 行、删除 45 行，更新测试用例以匹配最新解析逻辑。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/0de53339894ef2cef20512e31b4b8e0d83dcb6de  

- **a87cc50** – 在注意力选择器中使用 per‑head 缩放因子（`vllm/model_executor/layers/attention/attention.py`、`vllm/v1/attention/backend.py` 等）  
  - 新增 9 行实现 per‑head scale，删除 8 行旧实现。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/a87cc508599d2257f39420536dd5bb52aaa557c3  

- **761e63e** – 前端始终传递 `supported_tasks` 进行校验（`vllm/entrypoints/openai/speech_to_text/speech_to_text.py`、`vllm/v1/engine/*`）  
  - 代码改动 41 行，提升 API 参数校验一致性。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/761e63e5418e9c3d5c0086eb91eace11d406e786  

- **d12d201** – 修复 FunASR 处理器测试失败（`vllm/transformers_utils/processors/funasr_processor.py`）  
  - 仅 4 行改动，调整异常处理逻辑。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/d12d20140949ada88546d3f13ae68d4cea070a7e  

- **b3ad37c** – 调整 GLM‑ASR 默认 dummy 音频大小（`tests/models/multimodal/processing/test_common.py`）  
  - 新增 10 行，修改默认音频尺寸以匹配最新模型。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/b3ad37c5db8240e79195f644a3148a989e8e9422  

- **14561fa** – 优化 pooling 模型的冗余拷贝，提升约 1.8% 吞吐（`vllm/v1/worker/gpu_model_runner.py`）  
  - 关键改动：删除 15 行拷贝，新增 51 行优化逻辑。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/14561fabfd39030168a3365327d921ae5c49bb0c  

- **c77f3e1** – 原子化保存 AOT 编译产物（`vllm/compilation/decorators.py`）  
  - 新增 5 行，确保并发写入安全。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/c77f3e1207d32d4101a202e92f5d684d1c6968e1  

- **012dee9** – 为 Llama‑4 Vision（mllama4）添加 LoRA Tower/Connector 支持（`vllm/model_executor/models/mllama4.py`）  
  - 新增 23 行 LoRA 适配代码。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/012dee92331c7f9477c90c3f3944600c1f03f38d  

- **f1c6645** – 使 Voxtral 编译友好（`vllm/model_executor/models/voxtral_realtime.py`）  
  - 新增 16 行，修复编译时的类型冲突。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/f1c664545b954e30ac4887e32ce8f73b39310a9a  

- **c870eb9** – 更新 LoRA 扩展 kernel 的 `block_n` 计算（`vllm/lora/ops/triton_ops/utils.py`）  
  - 仅 2 行改动，修正块大小计算错误。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/c870eb9e0f001ed5d07e6b6e2eb5e500a080a717  

- **6af03f2** – 重构 kernel 抽象目录（大量文件重命名与迁移）  
  - 关键改动：`vllm/model_executor/kernels/linear/mixed_precision/*` 迁移至新结构，新增 48 行 `__init__.py`，删除 210 行旧实现。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/6af03f2394b54e95440945d2659a8120288b21ea  

- **1a6cf39** – 移除 CI 配置中冗余的 OpenTelemetry pip 安装（`.buildkite/test-amd.yaml`、`.buildkite/test_areas/misc.yaml`）  
  - 删除 15 行，简化 CI 环境。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/1a6cf39dec663f2f7c6c68ca16d232addcc0c59a  

- **f91808a** – 为离线 LLM 添加音频分块支持（`vllm/multimodal/audio.py`、`vllm/entrypoints/openai/speech_to_text/speech_to_text.py` 等）  
  - 新增 366 行实现音频切片与流式处理。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/f91808ae0ddf750acfdeb351fa072c91d4d678fc  

- **33a0d43** – 在 Qwen3.5 中硬编码 `mlp.gate` 为不可量化（`vllm/model_executor/models/qwen3_next.py`）  
  - 仅 2 行改动，避免量化错误。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/33a0d43c7119269aa300d4341871dd46d52575d0  

- **80d93fd** – 在模型配置中缓存 `is_encoder_decoder` 标志（`vllm/config/model.py`）  
  - 新增 1 行读取并缓存该属性。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/80d93fd6daf60d497c55a09c6dcd5471081c5978  

- **ec85340** – 为 FP8 MoE 添加 bias 支持（`vllm/model_executor/layers/quantization/fp8.py`）  
  - 新增 54 行实现 bias 加法。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/ec85340531e83962877fa683e23f1d5d17cd786a  

- **2ff4e51** – 在 ROCm 上实现 AITER 融合 RoPE+KVCache（新增 `vllm/_aiter_ops.py`、`vllm/compilation/passes/fusion/rope_kvcache_fusion.py` 等）  
  - 关键改动：新增 1211 行实现融合逻辑，修改多个注意力后端以使用新算子。  
  - PR 链接: https://github.com/vllm-project/vllm/commit/2ff4e51152d8975303529731069033ba313e1c46  

**Issue 概览**  

- **Qwen3.5 dtype mismatch** – 在使用 `torch.compile` 时，DeltaNet 线性注意力层出现 float 与 half 的 dtype 不匹配导致崩溃。  
  - 关联 PR 暂未提供修复。  
  - Issue 链接: https://github.com/vllm-project/vllm/issues/35238  

- **mi355_1: Multi‑Modal Models Test (Extended) 1 失败** – `test_voxtral_realtime_forward` 与 `test_voxtral_realtime_generator` 在 AMD CI 上报错。  
  - 可能与最近对 Voxtral 的编译友好改动有关。  
  - Issue 链接: https://github.com/vllm-project/vllm/issues/35235  

- **mi355_1: Language Models Test (Extended Generation) 失败** – `test_models` 在 AMD CI 上因 skinny GEMM 引入的数值差异导致回归。  
  - 需关注 `csrc/rocm/skinny_gemms.cu` 的最新改动。  
  - Issue 链接: https://github.com/vllm-project/vllm/issues/35233  

- **DeepSeek 3.2 多流 indexer 性能提升建议** – 提议使用多 CUDA 流并行 `weights_proj` 与其他算子，参考 sglang 实现。  
  - 尚未有对应 PR。  
  - Issue 链接: https://github.com/vllm-project/vllm/issues
### NVIDIA/cutile-python
昨日无更新。

## 总结
- Diff 内容已截断以满足 prompt 预算。
- Issue 内容已截断以满足 prompt 预算。
- OpenRouter global summarize failed: OpenRouter 429 Too Many Requests (z-ai/glm-4.5-air:free): {"error":{"message":"Provider returned error","code":429,"metadata":{"raw":"z-ai/glm-4.5-air:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations","provider_name":"Z.AI","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
