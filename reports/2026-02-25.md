ä»Šæ—¥çš„ AI Infra çš„æ–°é—»å¦‚ä¸‹ã€‚

## æ‘˜è¦
æ¨¡å‹æ‘˜è¦ä¸å¯ç”¨ï¼Œä»¥ä¸‹ä¸ºåŸå§‹æ•°æ®æ•´ç†ã€‚

## å…·ä½“å†…å®¹åˆ†æ
### deepseek-ai/DeepGEMM
**æäº¤**  
- **çŸ­ SHA**: `35c4bc8`  
- **å®Œæ•´é“¾æ¥**: [https://github.com/deepseek-ai/DeepGEMM/commit/35c4bc87713726d048f65275f6f1b551a4e7a6dc](https://github.com/deepseek-ai/DeepGEMM/commit/35c4bc87713726d048f65275f6f1b551a4e7a6dc)  
- **å¯¹åº” PR**: [#238](https://github.com/deepseek-ai/DeepGEMM/pull/238)  

**å˜æ›´æ¦‚è¿°**  
- ä¿®å¤ `k_grouped_fp8_gemm_nt_contiguous` åœ¨ H100 ä¸Š n=768 æ—¶å´©æºƒçš„é—®é¢˜ã€‚  
- ä¸»è¦æ”¹åŠ¨ä½äº `deep_gemm/include/deep_gemm/impls/sm90_fp8_gemm_1d1d.cuh`ï¼Œæ–°å¢å˜é‡ `prefetched_next_group_idx` ç”¨äºè¿½è¸ªå·²é¢„å–çš„ç»„ç´¢å¼•ï¼Œå¹¶å¯¹è°ƒåº¦é€»è¾‘è¿›è¡Œç»†åŒ–ï¼Œä»¥ç¡®ä¿åœ¨åˆ‡æ¢ç»„æ—¶æ­£ç¡®å‡†å¤‡ TensorMapã€‚  

**å…³é”®ä»£ç ç‰‡æ®µ**  

```cpp
// åŸå§‹ä»£ç 
uint32_t last_group_idx = kNumGroups, sum_k = 0;

// æ›´æ–°å
uint32_t last_group_idx = kNumGroups;
uint32_t prefetched_next_group_idx = kNumGroups;  // Track which group was prefetched
```

åç»­åœ¨è°ƒåº¦å¾ªç¯ä¸­åŠ å…¥äº†å¯¹ `prefetched_next_group_idx` çš„æ£€æŸ¥ï¼Œç¡®ä¿å½“å‰ç»„ä¸å·²é¢„å–ç»„ä¸€è‡´ï¼Œè‹¥ä¸ä¸€è‡´åˆ™é‡æ–°å‡†å¤‡å¯¹åº”çš„ TensorMapï¼Œä»è€Œé¿å…äº†å´©æºƒã€‚  

**å…¶ä»–ä¿¡æ¯**  
- æœ¬æ¬¡æäº¤æœªå…³è”ä»»ä½• Issueã€‚  
- ç”±äº `patch_truncated` ä¸º `true`ï¼Œå®Œæ•´ diff æœªå±•ç¤ºï¼Œä»¥ä¸Šä»£ç ç‰‡æ®µä¸ºæ ¸å¿ƒä¿®æ”¹ç‚¹ã€‚  

---  
*DeepGEMM å›¢é˜Ÿ*
### deepseek-ai/FlashMLA
**æäº¤**  
ï¼ˆæœ¬æ¬¡ä»“åº“æš‚æ— ä»£ç æäº¤ï¼‰

**Issues**  

| æ ‡é¢˜ | é“¾æ¥ | ä½œè€… | åˆ›å»ºæ—¶é—´ | è¯„è®ºæ•° | è¯´æ˜ |
|------|------|------|----------|--------|------|
| [Question] Questions regarding INTERLEAVE vs. SW128 Layouts for SM90 Sparse Attention Decode | https://github.com/deepseek-ai/FlashMLA/issues/166 | pengwubj | 2026â€‘02â€‘25 07:27:54â€¯UTC | 0 | Issue å†…å®¹å·²è¢«æˆªæ–­ï¼Œå®Œæ•´æ­£æ–‡ä¸å¯å¾—ã€‚ |

> **å¤‡æ³¨**ï¼šå½“å‰æ²¡æœ‰å…³è”çš„ Pull Request æˆ– commitï¼Œæ•…æœªæä¾›å¯¹åº”é“¾æ¥ã€‚è‹¥åç»­å‡ºç°ä»£ç æ”¹åŠ¨ï¼Œè¯·åœ¨æ­¤è¡¨æ ¼ä¸­è¡¥å……ç›¸åº”çš„ PR é“¾æ¥æˆ– commit SHAï¼ˆé™„å¸¦ commit URLï¼‰ã€‚
### NVIDIA/cutlass
**æäº¤**  

| çŸ­ SHA | PR é“¾æ¥ | å½±å“æ¨¡å—/æ–‡ä»¶ | å…³é”®æ”¹åŠ¨ï¼ˆä»£ç ç‰‡æ®µï¼‰ |
|--------|--------|--------------|-------------------|
| `954503d` | ï¼ˆæ—  PRï¼‰<br>[commit](https://github.com/NVIDIA/cutlass/commit/954503d44ce9180a15ec4bd0bcfd1268f650e03d) | `python/cutlass_cppgen/__init__.py` | ```python\n- this.__version__ = '4.3.5'\n+ this.__version__ = '4.4.0'\n``` |
| `6c4200f` | ï¼ˆæ—  PRï¼‰<br>[commit](https://github.com/NVIDIA/cutlass/commit/6c4200f1bcb0db9cdc9e5156baf9029fbbb52a24) | `python/setup_pycute.py` | ```python\n- version='4.3.5',\n+ version='4.4.0',\n``` |
| `de93e8a` | ï¼ˆæ—  PRï¼‰<br>[commit](https://github.com/NVIDIA/cutlass/commit/de93e8a4ac3e1000b7021f3c682db95d492a6096) | `python/setup_library.py` | ```python\n- version='4.3.5',\n+ version='4.4.0',\n``` |
| `b92b9f0` | ï¼ˆæ—  PRï¼‰<br>[commit](https://github.com/NVIDIA/cutlass/commit/b92b9f0d37df5e3d0e34ca56ec26b1ee4a88bcef) | `python/setup_cutlass.py` | ```python\n- version='4.3.5',\n+ version='4.4.0',\n``` |
| `2aedca6` | ï¼ˆæ—  PRï¼‰<br>[commit](https://github.com/NVIDIA/cutlass/commit/2aedca6f5eea09a6928cbfc78de926a37d5f765d) | `include/cutlass/version.h` | ```c\n-#define CUTLASS_MINOR 3\n-#define CUTLASS_PATCH 5\n+#define CUTLASS_MINOR 4\n+#define CUTLASS_PATCH 0\n``` |
| `6450964` | ï¼ˆæ—  PRï¼‰<br>[commit](https://github.com/NVIDIA/cutlass/commit/6450964b57967e6364f289c34fd6e91c3cd81167) | `README.md` | ```diff\n+    - Flash Decoding with cluster reduction.\n``` |
| `284449f` | ï¼ˆæ—  PRï¼‰<br>[commit](https://github.com/NVIDIA/cutlass/commit/284449fa5b5c6cda147aafb8833cb0ab8886f273) | `CHANGELOG.md` | ```diff\n-* Advanced compiler control\n+* Use 'Advanced control file' for mixed input gemm examples for better performance.\n``` |

**Issues**  

- **[QST] Is `_cast` used anywhere in CuTeDSL?**  
  - é“¾æ¥: <https://github.com/NVIDIA/cutlass/issues/3068>  
  - æå‡ºè€…: Peter9606 (2026â€‘02â€‘25)  
  - è¯„è®ºæ•°: 1  
  - **ç®€è¦å›ç­”**: `_cast` ä»…åœ¨ CuTeDSL çš„å†…éƒ¨å®ç° `arith.py` ä¸­å®šä¹‰ï¼Œç”¨ä½œ MLIR ç”Ÿæˆé˜¶æ®µçš„è¾…åŠ©å‡½æ•°ã€‚å®ƒåœ¨å½“å‰ä»£ç åº“ä¸­æ²¡æœ‰è¢«å…¶ä»–æ¨¡å—ç›´æ¥è°ƒç”¨ï¼Œå±äºé¢„ç•™çš„å®éªŒæ€§ APIï¼Œæœªæ¥å¯èƒ½åœ¨ DSL æ‰©å±•æˆ–è‡ªå®šä¹‰ç®—å­ä¸­è¢«åˆ©ç”¨ã€‚è‹¥æ— æ˜ç¡®éœ€æ±‚ï¼Œå¯æš‚æ—¶å¿½ç•¥ã€‚
### flashinfer-ai/flashinfer
æäº¤ï¼š
- benchmark: Add MXFP4/MXFP8 quantization mode support to FP4 MoE benchmark (#2635) [`fcc47b8`](https://github.com/flashinfer-ai/flashinfer/commit/fcc47b821911dbaeebf497825f79489174c6a3e8)
- å—å½±å“æ–‡ä»¶: benchmarks/routines/flashinfer_benchmark_utils.py, benchmarks/routines/moe.py, benchmarks/routines/moe_utils.py
- feat: add is_sm12x_supported() helper for SM12x family detection (#2574) [`17770f5`](https://github.com/flashinfer-ai/flashinfer/commit/17770f5c7c27b531c04139e7688d439e949a0d8a)
- å—å½±å“æ–‡ä»¶: flashinfer/gemm/gemm_base.py, flashinfer/prefill.py, flashinfer/utils.py, tests/attention/test_fmha_v2_prefill_deepseek.py
- benchmark: Enable speculative decode microbenchmarking for paged decode (#2628) [`4e03158`](https://github.com/flashinfer-ai/flashinfer/commit/4e03158679963240147c99e34cc3fef055f9e871)
- å—å½±å“æ–‡ä»¶: benchmarks/README.md, benchmarks/routines/attention.py
- fix: add SM121 support to SM120 version guards (#2631) [`dd417a5`](https://github.com/flashinfer-ai/flashinfer/commit/dd417a50266bfda06c2ba977d01dafd2a85079ca)
- å—å½±å“æ–‡ä»¶: csrc/gemm_groupwise_sm120.cu, csrc/group_gemm_fp8_groupwise_sm120.cu, csrc/trtllm_fmha_v2_binding.cu, flashinfer/decode.py, flashinfer/gemm/gemm_base.py, flashinfer/jit/gemm/core.py, flashinfer/jit/gemm/cutlass/generate_kernels.py, flashinfer/mla.py, flashinfer/xqa.py, include/flashinfer/gemm/cutlass_gemm_configs.h, include/flashinfer/gemm/fp4_gemm_cutlass_template_sm120.h, include/flashinfer/gemm/fp4_gemm_template_sm120.h, include/flashinfer/gemm/gemm_groupwise_sm120.cuh, include/flashinfer/gemm/group_gemm_fp8_groupwise_sm120.cuh, include/flashinfer/trtllm/common.h, tests/attention/test_trtllm_gen_mla.py
- æ–‡ä»¶åˆ—è¡¨å·²æˆªæ–­ã€‚
- [bugfix] Fix FilteredTopK overflow correctness (#2605) [`82ff6a9`](https://github.com/flashinfer-ai/flashinfer/commit/82ff6a9cbceca308a59067f5a02f2865f9ed932c)
- å—å½±å“æ–‡ä»¶: include/flashinfer/topk.cuh, tests/utils/test_topk.py
- fix: cute dsl nvfp4 moe routing index error (#2629) [`9826c26`](https://github.com/flashinfer-ai/flashinfer/commit/9826c26e27315927704cd5d8344f940c7213b52a)
- å—å½±å“æ–‡ä»¶: csrc/moe_utils_binding.cu, csrc/trtllm_fused_moe_routing_deepseek.cu, csrc/trtllm_fused_moe_routing_llama4.cu, csrc/trtllm_fused_moe_routing_renormalize.cu, csrc/trtllm_fused_moe_runner.cu, include/flashinfer/trtllm/fused_moe/RoutingKernel.cuh, include/flashinfer/trtllm/fused_moe/RoutingKernel.h, tests/moe/test_cute_dsl_fused_moe.py
Issuesï¼š
- Decouple routing_runner and moe_runner in BF16/FP16 trtllm_fused_moe_kernel (https://github.com/flashinfer-ai/flashinfer/issues/2636)
### Dao-AILab/flash-attention
æäº¤  
- **0586d2e** â€“ guard `use_2cta_instrs` on sm90ï¼ˆReuben Sternï¼Œ2026â€‘02â€‘25ï¼‰  
  [æŸ¥çœ‹æäº¤](https://github.com/Dao-AILab/flash-attention/commit/0586d2e78aff94f53cca056744e9aa2e429c03e7)  

- **484a5dc** â€“ ä¿®æ­£ Cutlass é”™è¯¯å¤„ç†ï¼ˆankutalevï¼Œ2026â€‘02â€‘25ï¼‰  
  [æŸ¥çœ‹æäº¤](https://github.com/Dao-AILab/flash-attention/commit/484a5dc1b1058bcbe03b3aeb81334c49cfcf6ba3)  

- **76d7362** â€“ å‰å‘ SM100 è°ƒæ•´å¯„å­˜å™¨ä½¿ç”¨ï¼ˆTri Daoï¼Œ2026â€‘02â€‘25ï¼‰  
  [æŸ¥çœ‹æäº¤](https://github.com/Dao-AILab/flash-attention/commit/76d736217b1c8e9741df3626f631732632fdfc7e)  

- **e0bc9ca** â€“ å‰å‘ SM100 ä½¿ç”¨ pipeline æŠ½è±¡å®ç°æ ¡æ­£â€‘å°¾å£°ï¼ˆTri Daoï¼Œ2026â€‘02â€‘25ï¼‰  
  [æŸ¥çœ‹æäº¤](https://github.com/Dao-AILab/flash-attention/commit/e0bc9ca2435e9b797629c1a868f773648a05ebf6)  

- **405df75** â€“ å‰å‘ SM100 ä½¿ç”¨ pipeline æŠ½è±¡å®ç° softmaxâ€‘correction å±éšœï¼ˆTri Daoï¼Œ2026â€‘02â€‘25ï¼‰  
  [æŸ¥çœ‹æäº¤](https://github.com/Dao-AILab/flash-attention/commit/405df756051c7585022e7814a3d362557e505d8f)  

- **01a8b74** â€“ å‰å‘ SM100 ä½¿ç”¨ pipeline æŠ½è±¡å®ç° `S_full` ä¸ `P_full_O_rescaled`ï¼ˆTri Daoï¼Œ2026â€‘02â€‘25ï¼‰  
  [æŸ¥çœ‹æäº¤](https://github.com/Dao-AILab/flash-attention/commit/01a8b741b65d613ae51391a491c13a7aceea04a7)  

- **2c0f11e** â€“ å‰å‘ SM100 ä»… 1 çº¿ç¨‹/warp å‘ä¿¡å· `mbar_P_full_2`ï¼ˆTri Daoï¼Œ2026â€‘02â€‘23ï¼‰  
  [æŸ¥çœ‹æäº¤](https://github.com/Dao-AILab/flash-attention/commit/2c0f11e9f8db961edf16dc4cec3c33066f504fdf)  

- **156137b** â€“ ä¸º SM100 æ·»åŠ  `hdim=192, hdimv=128` çš„åå‘å®ç°ï¼ˆjayhshahï¼Œ2026â€‘02â€‘25ï¼‰  
  [æŸ¥çœ‹æäº¤](https://github.com/Dao-AILab/flash-attention/commit/156137b153ceaf2d0b078a505804f3626f7b395a)  

- **0ba6f22** â€“ æ–‡æ¡£åŠ å…¥ â€œğŸ¤ Kernelsâ€ ä½¿ç”¨è¯´æ˜ï¼ˆSayak Paulï¼Œ2026â€‘02â€‘25ï¼‰  
  [æŸ¥çœ‹æäº¤](https://github.com/Dao-AILab/flash-attention/commit/0ba6f226a0b95472876cd8facedcc7f2200425ce)  

---

æ–‡ä»¶æ”¹åŠ¨æ¦‚è§ˆï¼ˆdiff å·²æˆªæ–­ï¼Œä»…æä¾›æ–‡ä»¶è·¯å¾„ä¸æ¨¡å—ä¿¡æ¯ï¼‰  

| æ–‡ä»¶ | æ¨¡å—/è·¯å¾„ | è¯´æ˜ |
|------|-----------|------|
| `flash_attn/cute/interface.py` | interface | ä»…å¢åŠ  1 è¡Œ guard ä»£ç ï¼Œå…·ä½“å®ç°è¯·å‚è§æäº¤ã€‚ |
| `hopper/cuda_check.h` | hopper | æ–°å¢ 11 è¡Œé”™è¯¯æ£€æŸ¥å®ï¼Œç»†èŠ‚è§æäº¤ã€‚ |
| `hopper/flash_bwd_launch_template.h` | hopper | åˆ é™¤ 12 è¡Œã€æ·»åŠ  8 è¡Œæ¨¡æ¿ä»£ç ï¼Œæ¶‰åŠ backward launch å‚æ•°ã€‚ |
| `hopper/flash_fwd_combine_launch_template.h` | hopper | å°å¹…å¢åˆ ï¼ˆ2 è¡Œï¼‰ï¼Œç”¨äº forward combine launchã€‚ |
| `hopper/flash_fwd_launch_template.h` | hopper | å¯¹ forward launch å‚æ•°åšå¯¹ç§°å¢åˆ ï¼ˆ4 è¡Œï¼‰ã€‚ |
| `flash_attn/cute/flash_fwd_sm100.py` | flash_fwd_sm100 | å¤šæ¬¡å¤§å¹…æ”¹åŠ¨ï¼šå¯„å­˜å™¨è°ƒä¼˜ã€pipeline æŠ½è±¡ã€ä¿¡å·ä¼˜åŒ–ç­‰ï¼Œç´¯è®¡å¢åˆ çº¦ 200 è¡Œã€‚ |
| `flash_attn/cute/block_sparse_utils.py` | block_sparse_utils | å¼•å…¥ pipeline æŠ½è±¡ç›¸å…³å‡½æ•°ï¼Œå¢åˆ çº¦ 30 è¡Œã€‚ |
| `flash_attn/cute/pipeline.py` | pipeline | æ–°å¢çº¦ 110 è¡Œ pipeline æŠ½è±¡å®ç°ã€‚ |
| `flash_attn/cute/flash_bwd_postprocess.py` | flash_bwd_postprocess | æ·»åŠ  7 è¡Œåå¤„ç†ä»£ç ã€‚ |
| `flash_attn/cute/flash_bwd_preprocess.py` | flash_bwd_preprocess | æ·»åŠ  11 è¡Œå‰å¤„ç†ä»£ç ã€‚ |
| `flash_attn/cute/flash_bwd_sm100.py` | flash_bwd_sm100 | å¤§å¹…æ‰©å±•ï¼Œæ–°å¢çº¦ 664 è¡Œï¼Œåˆ é™¤ 268 è¡Œï¼Œå®ç° `hdim=192, hdimv=128` çš„ backwardã€‚ |
| `flash_attn/cute/tile_scheduler.py` | tile_scheduler | å°å¹…å¢åˆ ï¼ˆ22 è¡Œï¼‰ï¼Œä¼˜åŒ– tile è°ƒåº¦ã€‚ |
| `tests/cute/test_flash_attn.py` | tests | æ–°å¢ 8 è¡Œæµ‹è¯•ç”¨ä¾‹ã€‚ |
| `tests/cute/test_flash_attn_race_condition.py` | tests | æ–°å¢ 22 è¡Œç”¨äºæ£€æµ‹ race condition çš„æµ‹è¯•ã€‚ |
| `README.md` | æ–‡æ¡£ | å¢åŠ  â€œğŸ¤ Kernelsâ€ ä½¿ç”¨ç¤ºä¾‹ï¼Œ19 è¡Œè¯´æ˜æ–‡å­—ã€‚ |

> **æ³¨**ï¼šä¸Šè¿°æ–‡ä»¶çš„å…·ä½“ä»£ç æ”¹åŠ¨å›  `patch_truncated` ä¸º trueï¼Œæœªåœ¨æœ¬æ‘˜è¦ä¸­å±•å¼€ã€‚è‹¥éœ€æŸ¥çœ‹å®Œæ•´ diffï¼Œè¯·è®¿é—®å¯¹åº”çš„ commit é“¾æ¥ã€‚
### sgl-project/sglang
æäº¤ï¼š
- Flashinfer MOE FP8 support for Mistral Large 3. (#15422) [`3501904`](https://github.com/sgl-project/sglang/commit/350190487be4a4b0a173c05e51136c57ee137a28)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8_moe.py, python/sglang/srt/layers/quantization/utils.py, test/registered/8-gpu-models/test_mistral_large3.py
- [Logging] Guard `log_prefill_stats` against idle batches in disagg prefill (#19361) [`c60dcc4`](https://github.com/sgl-project/sglang/commit/c60dcc40bb241e5269c4bf33ae7a0495e7367f6c)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/disaggregation/prefill.py
- [Logging] Fix prefill side logging in pd disagg (#19350) [`08957c8`](https://github.com/sgl-project/sglang/commit/08957c88eab997a7a87f2c735cfdf8573102e6f8)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/disaggregation/prefill.py
- Revert "Fix HybridAttnBackend forward for linear attention" (#19356) [`306c552`](https://github.com/sgl-project/sglang/commit/306c55263901f2bc466b0d25b7f514c5cde3aada)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/layers/attention/hybrid_attn_backend.py
- Update (#19351) [`a0f3361`](https://github.com/sgl-project/sglang/commit/a0f33610237acc33200d69cb5e65e1de0b9b2eab)
- å—å½±å“æ–‡ä»¶: .github/CODEOWNERS
- [AMD] Support Qwen3-Coder-Next on AMD platform (#18355) [`b2c46fc`](https://github.com/sgl-project/sglang/commit/b2c46fc60b14904857a37a02ceb633c898b23a63)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/layers/attention/aiter_backend.py, python/sglang/srt/models/qwen3_next.py
- fix: add --cuda-graph-max-bs to DSV3 FA3 FP8 KV cache test (#19307) [`cc1ca61`](https://github.com/sgl-project/sglang/commit/cc1ca61c81fb2133ab134fd4294d11828fa89a9e)
- å—å½±å“æ–‡ä»¶: test/registered/mla/test_mla_deepseek_v3.py
- [diffusion] Clean code (#19325) [`0217e82`](https://github.com/sgl-project/sglang/commit/0217e82a08932c5cadf92d49b3e6192cb16fed8a)
- å—å½±å“æ–‡ä»¶: python/sglang/multimodal_gen/runtime/distributed/parallel_state.py, python/sglang/multimodal_gen/runtime/layers/attention/turbo_layer.py, python/sglang/multimodal_gen/runtime/pipelines/diffusers_pipeline.py, python/sglang/multimodal_gen/runtime/pipelines_core/lora_pipeline.py, python/sglang/multimodal_gen/runtime/platforms/interface.py, python/sglang/multimodal_gen/runtime/utils/layerwise_offload.py, python/sglang/multimodal_gen/runtime/utils/perf_logger.py, python/sglang/multimodal_gen/test/scripts/gen_perf_baselines.py
- Revert "bugfix: prioritize init_npu_backend to fix various initialization bugs" (#19343) [`2fb2394`](https://github.com/sgl-project/sglang/commit/2fb239450eef00e5cfd2e9458bbe6515f17a049c)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/model_executor/model_runner.py, python/sglang/srt/server_args.py
- refactor linear attention backend (#18622) [`c7c4a1c`](https://github.com/sgl-project/sglang/commit/c7c4a1cbbd6c1d5cebe7ad0c6d3586110bd5338c)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/environ.py, python/sglang/srt/layers/attention/attention_registry.py, python/sglang/srt/layers/attention/hybrid_linear_attn_backend.py, python/sglang/srt/layers/attention/linear/__init__.py, python/sglang/srt/layers/attention/linear/gdn_backend.py, python/sglang/srt/layers/attention/linear/kda_backend.py, python/sglang/srt/layers/attention/linear/kernels/__init__.py, python/sglang/srt/layers/attention/linear/kernels/gdn_cutedsl.py, python/sglang/srt/layers/attention/linear/kernels/gdn_triton.py, python/sglang/srt/layers/attention/linear/kernels/kda_triton.py, python/sglang/srt/layers/attention/linear/kernels/kernel_backend.py, python/sglang/srt/layers/attention/linear/lightning_backend.py, python/sglang/srt/layers/attention/linear/utils.py, python/sglang/srt/server_args.py
- [diffusion] logging: improve logging (#19312) [`471acd9`](https://github.com/sgl-project/sglang/commit/471acd98b9ebabc7d912f5bf2463cc7ec62e914c)
- å—å½±å“æ–‡ä»¶: python/sglang/cli/utils.py, python/sglang/multimodal_gen/registry.py, python/sglang/multimodal_gen/runtime/loader/component_loaders/component_loader.py, python/sglang/multimodal_gen/runtime/pipelines_core/schedule_batch.py, python/sglang/multimodal_gen/runtime/utils/hf_diffusers_utils.py
- [Ascend ] Add qwen3.5 122B/35B/27B deployment examples on doc (#19339) [`56891e4`](https://github.com/sgl-project/sglang/commit/56891e46bc1ac4f75e8e541b61810f777cda8209)
- å—å½±å“æ–‡ä»¶: docs/platforms/ascend_npu_glm5_examples.md, docs/platforms/ascend_npu_qwen3_5_examples.md
- [diffusion] improve: improve fuse_scale_shift_kernel with non-blocking op (#18710) [`59b9d1e`](https://github.com/sgl-project/sglang/commit/59b9d1e86db0a5ad8d73bd77c9050021bbfa7021)
- å—å½±å“æ–‡ä»¶: python/sglang/jit_kernel/diffusion/triton/scale_shift.py
- Fix HybridAttnBackend forward for linear attention (#19006) [`c144e55`](https://github.com/sgl-project/sglang/commit/c144e554622efe1e286926bc09fe74a191146ae5)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/layers/attention/hybrid_attn_backend.py
- fix(dense): fix Qwen3.5 dense model precision bug in TP_SIZE>1 (#19070) [`d38c0e5`](https://github.com/sgl-project/sglang/commit/d38c0e537d95bfb78486c1185f68c90046ce0cc9)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/models/qwen3_5.py
- [NPU] Fix a corner case where FusedMoE.top_k is not explicitly declared (#19287) [`cdc4111`](https://github.com/sgl-project/sglang/commit/cdc411160b08b038b0608736c919a40ecc91b225)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/layers/quantization/unquant.py
- [diffusion] chore: enable sequence shard for wan by default (#19311) [`9840cd3`](https://github.com/sgl-project/sglang/commit/9840cd3f68073a8eaa07ab7303d41211981ca7d2)
- å—å½±å“æ–‡ä»¶: python/sglang/multimodal_gen/configs/sample/sampling_params.py, python/sglang/multimodal_gen/configs/sample/wan.py
- [AMD] add testcases for Qwen3 235b Instruct 2507 models (#18805) [`307a2b7`](https://github.com/sgl-project/sglang/commit/307a2b73edd84e66128c92576fb5817535e370d2)
- å—å½±å“æ–‡ä»¶: .github/workflows/nightly-test-amd-rocm720.yml, .github/workflows/nightly-test-amd.yml, test/registered/amd/test_qwen3_instruct.py, test/registered/amd/test_qwen3_instruct_fp8.py, test/registered/amd/test_qwen3_instruct_mxfp4.py
- [AMD][with CI Fix] support two batch overlapping for mori ep (#19216) [`60eeef7`](https://github.com/sgl-project/sglang/commit/60eeef73701a5e5d79b28bd4b9b6d552c576854f)
- å—å½±å“æ–‡ä»¶: docs/advanced_features/server_arguments.md, python/sglang/srt/batch_overlap/operations.py, python/sglang/srt/batch_overlap/operations_strategy.py, python/sglang/srt/batch_overlap/two_batch_overlap.py, python/sglang/srt/layers/attention/aiter_backend.py, python/sglang/srt/layers/moe/ep_moe/layer.py, python/sglang/srt/layers/moe/fused_moe_triton/layer.py, python/sglang/srt/layers/moe/token_dispatcher/__init__.py, python/sglang/srt/layers/moe/token_dispatcher/moriep.py, python/sglang/srt/models/deepseek_v2.py, python/sglang/srt/server_args.py, python/sglang/test/bench_one_batch_server_internal.py
- [diffusion] fix: fix bugs to let LTX-2 pipeline support latest Sglang Args pipelines (#19295) [`c4ef338`](https://github.com/sgl-project/sglang/commit/c4ef33862b30c6c41eeaefd1fdd4739243a838a6)
- å—å½±å“æ–‡ä»¶: python/sglang/multimodal_gen/configs/pipeline_configs/base.py, python/sglang/multimodal_gen/configs/pipeline_configs/ltx_2.py, python/sglang/multimodal_gen/runtime/pipelines_core/stages/input_validation.py
- Fix `trtllm_mha` fp8 SWA KV index translation (#19107) [`671b595`](https://github.com/sgl-project/sglang/commit/671b59557045f13c13650c29b4e56a02874f7fcd)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/layers/attention/trtllm_mha_backend.py
- [Misc] Normalize `--host` parameter to use plain hostname without scheme (#19309) [`a55f658`](https://github.com/sgl-project/sglang/commit/a55f6588350c754cfc7ff0a5a0f8b75a5fde6dfa)
- å—å½±å“æ–‡ä»¶: benchmark/deepseek_v3/README.md, docs/developer_guide/evaluating_new_models.md, examples/runtime/multimodal/llama3_llava_server.py, examples/runtime/multimodal/pixtral_server.py, examples/runtime/multimodal/qwen_llava_server.py, python/sglang/test/few_shot_gsm8k.py, python/sglang/test/test_utils.py, python/sglang/utils.py
- [Fix][Qwen3.5] Fix KV cache slice transfer for GQA models with replicated KV heads (#19086) [`f75abb4`](https://github.com/sgl-project/sglang/commit/f75abb4521cfa5a58a9876b3d71c5788302f751d)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/disaggregation/base/conn.py, python/sglang/srt/disaggregation/mooncake/conn.py, python/sglang/srt/disaggregation/prefill.py
- [HiCache] Support heterogeneous tp for hicache storage  (#18541) [`d40cb2f`](https://github.com/sgl-project/sglang/commit/d40cb2f72551c4a597108dda16507e8188b666b7)
- å—å½±å“æ–‡ä»¶: docs/advanced_features/hicache_best_practices.md, python/sglang/srt/managers/cache_controller.py, python/sglang/srt/mem_cache/hicache_storage.py, python/sglang/srt/mem_cache/memory_pool_host.py, python/sglang/srt/mem_cache/storage/mooncake_store/mooncake_store.py
- refactor: extract device-to-backend mapping into get_default_distributed_backend (#19202) [`3d879c6`](https://github.com/sgl-project/sglang/commit/3d879c69e9e95a4c977caca3ef4908cc3bb6603e)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/disaggregation/encode_server.py, python/sglang/srt/distributed/parallel_state.py, python/sglang/srt/model_executor/model_runner.py
- [NPU] bugfix for model Qwen3-Coder-Next at weight shape transpose for npu. (#18700) [`d0bb140`](https://github.com/sgl-project/sglang/commit/d0bb140034894da4390f048d424be53ff76d8f2d)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/hardware_backend/npu/quantization/fused_moe_method_npu.py, python/sglang/srt/layers/attention/hybrid_linear_attn_backend.py
- Perf/fuse mamba state scatter mtp verify (#18088) [`a1b39c1`](https://github.com/sgl-project/sglang/commit/a1b39c1c26d41203bcce61be35c2676c5c646929)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/layers/attention/hybrid_linear_attn_backend.py, python/sglang/srt/layers/attention/mamba/mamba_state_scatter_triton.py, test/unit/test_mamba_state_scatter_triton.py
- [SKILL] Better claude skills for sgl-kernel and jit-kernel (#19302) [`ab7071b`](https://github.com/sgl-project/sglang/commit/ab7071b54558e6966eff9281874eb329ed39bc2c)
- å—å½±å“æ–‡ä»¶: .claude/skills/add-jit-kernel/SKILL.md, .claude/skills/add-sgl-kernel/SKILL.md
- [Fix] Kimi K2.5 support pp (#18434) [`4a3a787`](https://github.com/sgl-project/sglang/commit/4a3a787f1e1ff3d1a5efd9eb0b7ffa00eb53d9a8)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/models/deepseek_v2.py, python/sglang/srt/models/kimi_k25.py
- Fix comment for tp_rank calculation in dp_attention (#19306) [`8d9ee66`](https://github.com/sgl-project/sglang/commit/8d9ee6669e1e51b232c10a1868113042ddc55740)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/layers/dp_attention.py
- [AMD] Support --enable-aiter-allreduce-fusion on AMD GPUs (#13747) [`17b0aff`](https://github.com/sgl-project/sglang/commit/17b0affbdf3b532f816a644194168e004cf4fe4d)
- å—å½±å“æ–‡ä»¶: benchmark/kernels/all_reduce/benchmark_fused_ar_rms_amd.py, docs/advanced_features/server_arguments.md, python/sglang/srt/distributed/communication_op.py, python/sglang/srt/distributed/parallel_state.py, python/sglang/srt/layers/communicator.py, python/sglang/srt/layers/layernorm.py, python/sglang/srt/server_args.py, test/registered/ops/test_aiter_allreduce_fusion_amd.py, test/run_suite.py
- [Qwen3.5] Raise Exception when radix_cache and extra_buffer are enabled at the same time (#19169) [`73fe389`](https://github.com/sgl-project/sglang/commit/73fe389dd11472fcd41f401382c0763d4d1db664)
- å—å½±å“æ–‡ä»¶: python/sglang/srt/server_args.py
Issuesï¼š
- [Bug] Qwen3 235b illegal memory access (https://github.com/sgl-project/sglang/issues/19365)
- Issue å†…å®¹å·²æˆªæ–­ã€‚
- [Bug] PD Disaggregation: Mooncake NVLink transport fails with "Requested address not found" causing cascading KV transfer failures on Qwen3.5 with Concurrency>=512 (https://github.com/sgl-project/sglang/issues/19355)
- Issue å†…å®¹å·²æˆªæ–­ã€‚
- [Feature] Refactor common test entries of SGLang Diffusion (https://github.com/sgl-project/sglang/issues/19352)
- Zero-Overhead Batch Scheduler fail to overlap CPU scheduling with GPU computation as claimed (https://github.com/sgl-project/sglang/issues/19347)
- Issue å†…å®¹å·²æˆªæ–­ã€‚
- [Bug] Image generation without `base_64` return unexpected `400` (https://github.com/sgl-project/sglang/issues/19346)
- Issue å†…å®¹å·²æˆªæ–­ã€‚
- OpenAI-compatible tool-call stream can emit malformed JSON args (GLM-5 via NIM) (https://github.com/sgl-project/sglang/issues/19345)
- [Bug] Incorrect versioning in nightly containers. (https://github.com/sgl-project/sglang/issues/19344)
- Issue å†…å®¹å·²æˆªæ–­ã€‚
- [Feature]Does sglang currently support a W8A8_int model with mixed precision, where some layers use int8 and others use bf16? (https://github.com/sgl-project/sglang/issues/19342)
### vllm-project/vllm
æäº¤ï¼š
- [ROCm][CI] Amending deletion of AMD mirror (#35322) [`ed42507`](https://github.com/vllm-project/vllm/commit/ed42507f6d6e326663997da5cca6991da5d8a23f)
- å—å½±å“æ–‡ä»¶: .buildkite/test_areas/entrypoints.yaml
- [ROCm][CI] Extending attention backend coverage for Eagle spec decode tests (#35265) [`9571e99`](https://github.com/vllm-project/vllm/commit/9571e999451a468423e97cd7e3f36e9d27a098cb)
- å—å½±å“æ–‡ä»¶: .buildkite/test_areas/engine.yaml, tests/utils.py, tests/v1/e2e/test_async_scheduling.py, tests/v1/e2e/test_spec_decode.py
- fix(mxfp4): Disable monolithic path for TRITON backend with EP (#34270) [`c97234c`](https://github.com/vllm-project/vllm/commit/c97234c08b42326cf1e5ef024d9ac8441e0848b1)
- å—å½±å“æ–‡ä»¶: tests/kernels/quantization/test_mxfp4_triton_ep.py, vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py
- [CI][AMD][BugFix] Add  torch.cuda.set_device to test_punica_ops so punica kernels execute on same device as tensor (#34985) [`b188bab`](https://github.com/vllm-project/vllm/commit/b188bab4417f7f94df0c4e84399d163e6c0db316)
- å—å½±å“æ–‡ä»¶: tests/lora/test_punica_ops.py
- Revert "[Misc] Enable weights loading tracking for quantized models" (#35309) [`15d76f7`](https://github.com/vllm-project/vllm/commit/15d76f74e2fdb12a95ea00f0ca283acf6219a2b7)
- å—å½±å“æ–‡ä»¶: vllm/model_executor/model_loader/default_loader.py
- [ROCm][CI] Disable skinny GEMMs in multimodal tests to fix non-deterministic results (#35049) [`8fd6975`](https://github.com/vllm-project/vllm/commit/8fd69754798c50b7b07938451bd97b1e66765927)
- å—å½±å“æ–‡ä»¶: tests/models/multimodal/conftest.py
- [Bugfix] Fix Harmony preamble visibility in Responses API (#32114) [`5d18bf8`](https://github.com/vllm-project/vllm/commit/5d18bf8b32837275d7656e2ae8b5c684274234d2)
- å—å½±å“æ–‡ä»¶: tests/entrypoints/openai/parser/test_harmony_utils.py, tests/entrypoints/openai/responses/test_harmony.py, tests/entrypoints/openai/responses/test_mcp_tools.py, tests/entrypoints/openai/test_serving_chat_stream_harmony.py, tests/entrypoints/openai/test_serving_responses.py, tests/entrypoints/test_context.py, vllm/entrypoints/openai/chat_completion/stream_harmony.py, vllm/entrypoints/openai/parser/harmony_utils.py, vllm/entrypoints/openai/responses/context.py, vllm/entrypoints/openai/responses/streaming_events.py
- [Bugfix] Gracefully disable AllReduceFusionPass on GPUs without multicast support (#35085) [`0788ff0`](https://github.com/vllm-project/vllm/commit/0788ff0a153c6eb6436743d717b31c863c987761)
- å—å½±å“æ–‡ä»¶: vllm/compilation/passes/fusion/allreduce_rms_fusion.py
- [XPU]Fix for Qwen-OMNI crash (#35249) [`d72b0be`](https://github.com/vllm-project/vllm/commit/d72b0be33cdd561e557df1ce5350a14451b9af13)
- å—å½±å“æ–‡ä»¶: vllm/_xpu_ops.py
- [Misc][LoRA] Increase max vocab size limit to 258048 in logits processor (#34773) [`42489e4`](https://github.com/vllm-project/vllm/commit/42489e43c2718674828ece00eefc0f11088e801d)
- å—å½±å“æ–‡ä»¶: tests/lora/conftest.py, tests/lora/test_layers.py, vllm/lora/layers/logits_processor.py
- [Bugfix] Fix step3p5 reasoning with interleaved thinking (#34211) [`af5e6af`](https://github.com/vllm-project/vllm/commit/af5e6afa0af28b75409104906fe47ac9e8c03cf1)
- å—å½±å“æ–‡ä»¶: tests/reasoning/test_step3p5_reasoning_parser.py, vllm/reasoning/step3p5_reasoning_parser.py
- [Tests] Add GSM8k check to SpecDec E2E tests (#34772) [`ee59a7c`](https://github.com/vllm-project/vllm/commit/ee59a7c61574485cf4ddbc6037ba557941be5c56)
- å—å½±å“æ–‡ä»¶: tests/evals/gsm8k/gsm8k_eval.py, tests/v1/e2e/test_spec_decode.py
- Doc link typo (#35281) [`709eadb`](https://github.com/vllm-project/vllm/commit/709eadbb0bff8fa00a22a3ade30327fecd61be4b)
- å—å½±å“æ–‡ä»¶: docs/design/moe_kernel_features.md
- Fix custom processors that use deleted behaviour for Transformers v5 (#35107) [`90fc7f9`](https://github.com/vllm-project/vllm/commit/90fc7f91097de57ad17887577a0da2a95c28c418)
- å—å½±å“æ–‡ä»¶: vllm/transformers_utils/processor.py
- [Bugfix][CPU] Fix basic unit tests failing in CPU platforms (#34677) [`675ec59`](https://github.com/vllm-project/vllm/commit/675ec59aa94301989c3c174b3b910338c2d51ff4)
- å—å½±å“æ–‡ä»¶: tests/test_config.py
- [Doc] Suggest "--managed-python" flag when installing python using uv (#33069) [`80e60a6`](https://github.com/vllm-project/vllm/commit/80e60a61338fe7b001b81e33968584cc9fa96982)
- å—å½±å“æ–‡ä»¶: docs/getting_started/installation/python_env_setup.inc.md
- [DOC][BugFix] Specfiy build dependency installation (#34513) [`26e722f`](https://github.com/vllm-project/vllm/commit/26e722f9068699c18538eff44c9827561f3f90ab)
- å—å½±å“æ–‡ä»¶: docs/contributing/README.md
- [Docs]Fix documentation formatting in architecture overview (#34679) [`2c619e5`](https://github.com/vllm-project/vllm/commit/2c619e5e3f3b5712073546d10b10f1a2f00ce5a4)
- å—å½±å“æ–‡ä»¶: docs/design/arch_overview.md
- docs: document committer proposal process in governance (#35225) [`8a685be`](https://github.com/vllm-project/vllm/commit/8a685be8d9867bcfd114b073fb7f623888dc94ca)
- å—å½±å“æ–‡ä»¶: docs/governance/process.md
- [Perf] Add opt-in SM100 Oink RMSNorm custom-op path (#31828) [`2465071`](https://github.com/vllm-project/vllm/commit/24650715105a26b41c88f4164777b5e1a0f3200b)
- å—å½±å“æ–‡ä»¶: tests/model_executor/test_oink_integration.py, vllm/_oink_ops.py, vllm/envs.py, vllm/model_executor/layers/layernorm.py
-     [Perf] Optimize FP8 gemm of sm120. (#34424) [`cd43673`](https://github.com/vllm-project/vllm/commit/cd4367366814f1d1404e263e621a2b15c117eaf6)
- å—å½±å“æ–‡ä»¶: csrc/quantization/w8a8/cutlass/c3x/scaled_mm_sm120_fp8_dispatch.cuh
- [XPU]Support CUDAGraph on XPU Platform (#34482) [`35d44b4`](https://github.com/vllm-project/vllm/commit/35d44b45570396b28ce94186b2438dbc608fd6c0)
- å—å½±å“æ–‡ä»¶: vllm/platforms/xpu.py, vllm/utils/torch_utils.py, vllm/v1/worker/xpu_model_runner.py
- [Platform] Add current_platform.num_compute_units interface (#35042) [`8ad54a9`](https://github.com/vllm-project/vllm/commit/8ad54a991b9f49a002cb3a9912f05a25b9f7588f)
- å—å½±å“æ–‡ä»¶: tests/kernels/attention/test_cutlass_mla_decode.py, tests/kernels/quantization/test_allspark_gemm.py, tests/kernels/quantization/test_rocm_skinny_gemms.py, vllm/model_executor/kernels/linear/mixed_precision/allspark.py, vllm/model_executor/kernels/linear/scaled_mm/rocm.py, vllm/model_executor/layers/batch_invariant.py, vllm/model_executor/layers/fla/ops/layernorm_guard.py, vllm/model_executor/layers/quantization/utils/marlin_utils.py, vllm/model_executor/layers/utils.py, vllm/model_executor/warmup/deep_gemm_warmup.py, vllm/platforms/cuda.py, vllm/platforms/interface.py, vllm/platforms/rocm.py, vllm/platforms/xpu.py, vllm/utils/platform_utils.py, vllm/v1/attention/backends/mla/cutlass_mla.py
- æ–‡ä»¶åˆ—è¡¨å·²æˆªæ–­ã€‚
- remove cuda check in `top_k_top_p_triton` kernel (#35011) [`92510ed`](https://github.com/vllm-project/vllm/commit/92510edc325c855848653c8864f0805eb7fbb022)
- å—å½±å“æ–‡ä»¶: vllm/v1/sample/ops/topk_topp_sampler.py, vllm/v1/sample/ops/topk_topp_triton.py
- [Misc] Add shard_id validation for MergedColumnLinear (#35055) [`a6c1375`](https://github.com/vllm-project/vllm/commit/a6c137521cf7218cc2da5f56aa3e68ad96aa76b1)
- å—å½±å“æ–‡ä»¶: vllm/model_executor/layers/linear.py
- [Misc] Enable weights loading tracking for quantized models (#35074) [`4572a06`](https://github.com/vllm-project/vllm/commit/4572a06afe96d0a6d5d3efacf130c71505dd2bc9)
- å—å½±å“æ–‡ä»¶: vllm/model_executor/model_loader/default_loader.py
- [compile] Improve error message during artifacts load failure. (#35115) [`5cc29cf`](https://github.com/vllm-project/vllm/commit/5cc29cfb8bdf1fadf3abb57556966785a8501285)
- å—å½±å“æ–‡ä»¶: vllm/compilation/decorators.py
- [Linear Attention] fix bug for linear attention + prefix caching + reset_prefix_cache (#35157) [`8fae54f`](https://github.com/vllm-project/vllm/commit/8fae54faff485e446dc8d1a700417f07659ef89e)
- å—å½±å“æ–‡ä»¶: tests/v1/worker/test_mamba_utils.py, vllm/v1/worker/mamba_utils.py
- Remove requirement to use `--hf-overrides` for `DeepseekVLV2ForCausalLM` (#35203) [`f796757`](https://github.com/vllm-project/vllm/commit/f7967577f5563b45b1ad5b6e0fae5b639af17e28)
- å—å½±å“æ–‡ä»¶: docs/models/supported_models.md, tests/models/registry.py, tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh, tests/v1/kv_connector/nixl_integration/run_edge_case_test.sh, vllm/transformers_utils/configs/deepseek_vl2.py
- [Bugfix] Fix AttributeError when passing StructuredOutputsParams to CompletionRequest (#35237) [`af770b8`](https://github.com/vllm-project/vllm/commit/af770b8e7bf77539fcbc81a9ff1974f12cdf87ff)
- å—å½±å“æ–‡ä»¶: vllm/entrypoints/openai/chat_completion/protocol.py, vllm/entrypoints/openai/completion/protocol.py
- [Responses][CI] Filter negative token IDs in schema fuzz test to avoid 500 errors (#35231) [`2ff3e43`](https://github.com/vllm-project/vllm/commit/2ff3e436ade14b005abe6423752966319c365eb2)
- å—å½±å“æ–‡ä»¶: tests/entrypoints/openai/test_completion_error.py, vllm/entrypoints/openai/completion/protocol.py
- [FIX] fused moe with lora shared expert dual stream (1.07x otps) (#34933) [`c2c4c46`](https://github.com/vllm-project/vllm/commit/c2c4c4611a6d92f3006dfdedf8598a70d39002b3)
- å—å½±å“æ–‡ä»¶: vllm/lora/layers/fused_moe.py
Issuesï¼š
- [CI] AttributeError in SMControlContextManager: torch.cuda.current_device() returns int, not device object (https://github.com/vllm-project/vllm/issues/35337)
- Issue å†…å®¹å·²æˆªæ–­ã€‚
- [Refactor]: Make SSM backends use the null block (0) for padded requests instead of -1 (https://github.com/vllm-project/vllm/issues/35336)
- Issue å†…å®¹å·²æˆªæ–­ã€‚
- [Feature]: Performance Optimization for Model Runner V2 (https://github.com/vllm-project/vllm/issues/35335)
- [Feature]: Profiler num_steps (https://github.com/vllm-project/vllm/issues/35332)
- [Bug]: quantization="mxfp4" produces incorrect results / hangs for MoE models at tensor_parallel_size=1 (https://github.com/vllm-project/vllm/issues/35329)
- Issue å†…å®¹å·²æˆªæ–­ã€‚
- [Bug]: FusedMoE.weight_loader MXFP4 branch crashes on standard HuggingFace MoE checkpoints (per-expert 2-D weights) (https://github.com/vllm-project/vllm/issues/35324)
- Issue å†…å®¹å·²æˆªæ–­ã€‚
- [Feature]: Encoder self-attention for RocmAttentionImpl (https://github.com/vllm-project/vllm/issues/35321)
- Issue å†…å®¹å·²æˆªæ–­ã€‚
- [Bug]: Multi-Node inference with PP > 1 crashes after processing completions request with non-None `logprobs` parameter. (https://github.com/vllm-project/vllm/issues/35319)
- Issue å†…å®¹å·²æˆªæ–­ã€‚
### NVIDIA/cutile-python
æ˜¨æ—¥æ— æ›´æ–°ã€‚

## æ€»ç»“
- Diff å†…å®¹å·²æˆªæ–­ä»¥æ»¡è¶³ prompt é¢„ç®—ã€‚
- Issue å†…å®¹å·²æˆªæ–­ä»¥æ»¡è¶³ prompt é¢„ç®—ã€‚
- OpenRouter repo summarize failed for flashinfer-ai/flashinfer: OpenRouter 429 Too Many Requests (z-ai/glm-4.5-air:free): {"error":{"message":"Provider returned error","code":429,"metadata":{"raw":"z-ai/glm-4.5-air:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations","provider_name":"Z.AI","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
- OpenRouter repo summarize failed for sgl-project/sglang: OpenRouter 429 Too Many Requests (z-ai/glm-4.5-air:free): {"error":{"message":"Provider returned error","code":429,"metadata":{"raw":"z-ai/glm-4.5-air:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations","provider_name":"Z.AI","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
- OpenRouter repo summarize failed for vllm-project/vllm: OpenRouter 429 Too Many Requests (z-ai/glm-4.5-air:free): {"error":{"message":"Provider returned error","code":429,"metadata":{"raw":"z-ai/glm-4.5-air:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations","provider_name":"Z.AI","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
- OpenRouter global summarize failed: OpenRouter 429 Too Many Requests (z-ai/glm-4.5-air:free): {"error":{"message":"Provider returned error","code":429,"metadata":{"raw":"z-ai/glm-4.5-air:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations","provider_name":"Z.AI","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
