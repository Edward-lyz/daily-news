今日的 AI Infra 的新闻如下。

## 摘要
## 本周关键更新概览

- **NVIDIA CUTLASS** 发布 v4.4.1，重点修复了 Blackwell 上的 TMA 描述符 bug、aarch64 TVM‑FFI 崩溃以及 `mma.sync.aligned.m16n8k16` 寄存器索引错误，提升了跨平台稳定性。文档同步更新，删除 10k 行冗余 DSL 示例，社区提出的 2D Tile‑to‑global TMA 存储需求已登记待实现。

- **flashinfer‑ai/flashinfer** 主要围绕 **MLA、GDN、MoE** 进行功能完善：
  - 支持 GLM‑5 `qk_nope_head_dim=192`，扩展了多模态注意力的维度上限；
  - 优化 GDN 解码内存访问顺序，显著提升大批量解码吞吐；
  - 修复 `codeowners_analyzer.py` 中的用户名大小写冲突；
  - 报告 XQA 在 SM120 上 `head_dim=256` 且 `page_size<64` 时的崩溃，提示后续需要更细粒度的页面管理。

- **Dao‑AILab/flash‑attention** 在 **SM100** 代码路径上完成一系列管道化与张量分配改进：
  - 引入 `tScS` 分区用于 `score_mod`，并在 `s0_s1_sequence` 中使用管道抽象提升并行度；
  - `split_P_arrive` 参数化、`TmemAllocator` 与 `mbar_P_full_2`/`O_full` 的管道抽象化，进一步降低显存碎片；
  - 新增 `return_lse` 接口，方便上层获取对数软最大值。
  - 仍有 `test_flash_attn_kvcache` 在 B200 GPU 上挂起的稳定性问题待排查。

- **sgl‑project/sglang** 近期提交聚焦 **多硬件适配、模型兼容与性能调优**：
  - 完成 LoRA 更新 CI 关闭、nvfp4 权重更新、VLM 输入格式改进等日常维护；
  - 在 AMD 平台上启用 cudagraph、移除冗余 H2D 操作、统一 NSA 后端调度，提升跨平台一致性；
  - 引入 **Parakeet** 编码器支持 Nemotron‑Nano‑VL 音视频特性；
  - 大幅优化 **FlashInfer‑TRTLLM MoE**、**GDN**、**Diffusion** 关键路径的内存与算子调度；
  - 多项 Bug（如 FLUX.2‑klein‑9B 多 GPU 图像编辑、KeyError、ValueError）已提交或在追踪中。

- **vllm‑project/vllm** 近期提交覆盖 **后端统一、权重同步、模型扩展与安全性**：
  - 在 ROCm + AITER 环境实现 encoder/encoder‑decoder 统一后端，解决跨平台注意力实现差异；
  - 引入基于 IPC 的 **WeightTransferEngine**，支持 RLHF 场景下的高效权重迁移；
  - 修复 GPT‑OSS、MTP、DP‑padding、pytree 缓存等多处批处理与并行逻辑错误，提升推理一致性；
  - 新增 **Parakeet** 音视频模型、`AXK1` HuggingFace 模型、Helion HOP pattern 匹配等功能；
  - 加入多项安全/可维护性改进：编译器锁、CODEOWNERS 更新、异常改为 `ValueError`、RMSNormGated 参数补全等。

## 影响评估
- **跨平台兼容性**：CUTLASS、vllm 与 sglang 的 ROCm/Amd 适配显著降低了在非‑NVIDIA 环境的部署门槛；
- **大模型推理性能**：Flash‑Attention 与 FlashInfer 的内存布局、管道抽象以及 GDN 优化，预计在 2‑4× 大模型（如 Qwen‑3‑VL、Gemma‑3‑N）上带来显著吞吐提升；
- **多模态/音视频**：Parakeet、Qwen‑3‑Omni、FlashInfer‑TRTLLM MoE 等新特性为音视频生成与跨模态检索提供了更高效的算子实现；
- **可靠性**：大量 bug 修复（如 XQA 崩溃、DP‑padding 误触、pytree 缓存冲突）提升了生产环境的稳定性；
- **可维护性**：文档同步、CODEOWNERS 完备、编译器锁等措施降低了社区贡献的回归风险。

## 具体内容分析
### deepseek-ai/DeepGEMM
昨日无更新。
### deepseek-ai/FlashMLA
昨日无更新。
### NVIDIA/cutlass
# NVIDIA CUTLASS Update (2026-02-26)

## Version Update
- **v4.4.1 released** - Bug fixes and improvements ([PR #3079](https://github.com/NVIDIA/cutlass/pull/3079))
  - Fixed segfault issue with tvm-ffi on aarch64
  - Fixed driver TMA descriptor related bug on Blackwell for tensors with <128KB allocation
  - Updated version numbers across all Python packages and documentation

## Bug Fixes
- Fixed register index bug in `mma.sync.aligned.m16n8k16` ([commit edf2f82](https://github.com/NVIDIA/cutlass/commit/edf2f82c0025b98b48977cb83db746769721b4ee))
- Fixed typos in tutorial comments ([commit c651d66](https://github.com/NVIDIA/cutlass/commit/c651d660d22bed49ffc1330feeb3e02a9f0a6b2c))
- Fixed debug print statements in sgemm examples ([commit 7934535](https://github.com/NVIDIA/cutlass/commit/79345359a790c18893f8368c4493d33d9ed314c5))

## Documentation Improvements
- Corrected Mxf4 format references in Blackwell documentation ([commit 518327d](https://github.com/NVIDIA/cutlass/commit/518327d6317756f02cacdc0433d48ac57d328e99))
- Fixed shape example in CuTe layout algebra documentation ([commit de67bb7](https://github.com/NVIDIA/cutlass/commit/de67bb7a42f8bb6dbe898df5c4e3a0f3b0551aac))
- Fixed documentation typos in host tensor planar complex header ([commit 8b9b3d7](https://github.com/NVIDIA/cutlass/commit/8b9b3d78dfedc707e92b00dd0c9007238d4e597c))
- Fixed FP8 warpgroup MMA operation documentation ([commit fc5bbc2](https://github.com/NVIDIA/cutlass/commit/fc5bbc2dabadb80ff30fa7e5bb3318ac1a017d2f))

## Code Cleanup
- Removed redundant DSL examples ([commit c213bfd](https://github.com/NVIDIA/cutlass/commit/c213bfdfc1f4ffb156c69d51d07efcf7f367f2fb), [commit 057635d](https://github.com/NVIDIA/cutlass/commit/057635de5c493a34e9913e2c5e836bdd98c58861))
  - Removed 3 redundant examples totaling over 10,000 lines of code

## Community Issue
- **Documentation Request**: User requesting documentation on storing 2D tiles from accumulator registers to global memory using TMA on SM90A architecture ([Issue #3076](https://github.com/NVIDIA/cutlass/issues/3076))
### flashinfer-ai/flashinfer
## 仓库更新

### 修复提交
- **修复 trtllm_mxint4_block_scale_moe 单元测试索引问题** (#2627)  
  作者: Jimmy Zhou  
  修改文件: `tests/moe/test_trtllm_gen_fused_moe.py`  
  代码变更: 将 `return output.to(torch.float)` 修改为 `return output[0].to(torch.float)`  
  [查看提交](https://github.com/flashinfer-ai/flashinfer/commit/a337e42e6ae91f512ff066bc09180a1ffa74ba88)

- **支持 GLM-5 的 qk_nope_head_dim=192 配置** (#2607)  
  作者: Rain Jiang  
  修改文件: `flashinfer/mla.py` 和 `tests/attention/test_trtllm_gen_mla.py`  
  代码变更: 更新验证逻辑以允许 qk_nope_head_dim 为 128 或 192（之前仅支持 128）  
  [查看提交](https://github.com/flashinfer-ai/flashinfer/commit/ad94692d6b911af9498415d2faa946fbb3bba882)

- **优化 GDN 解码预转置内核性能** (#2588)  
  作者: ameynaik-hub  
  修改文件: `flashinfer/gdn_decode.py`  
  代码变更: 重新排序内存访问操作以提高所有批次大小的性能  
  [查看提交](https://github.com/flashinfer-ai/flashinfer/commit/1589ebb405b437dab5a3b0ca3b77097da0959071)

- **修复 codeowners_analyzer.py 中重复用户名 bug** (#2637)  
  作者: Scott Ricketts  
  修改文件: `scripts/codeowner_analyzer.py`  
  代码变更: 实现区分大小写的 GitHub 用户名去重逻辑  
  [查看提交](https://github.com/flashinfer-ai/flashinfer/commit/f852eb6cd24cd9e17dc254a13a80701c7d25ee58)

### 问题报告
- **XQA 测试在 headdim=256 且 pagesize<64 时失败** (#2638)  
  作者: samuellees  
  创建时间: 2026-02-26T07:33:35Z  
  问题描述: 在 SM120 架构上，当 head_dim=256 且 page_size<64 时 XQA 测试失败  
  注意: 问题正文被截断  
  [查看问题](https://github.com/flashinfer-ai/flashinfer/issues/2638)
### Dao-AILab/flash-attention
## Flash-Attention 更新摘要 (2026-02-26)

### 提交记录

1. **修复掩模模块错误 (#2276)** - Reuben Stern
   - 修复了 `flash_attn/cute/block_sparse_utils.py` 中的管道阶段错误
   - 在 `flash_attn/cute/mask.py` 中添加了 tScS 分区修复
   - 更新了测试用例，将 `_compute_capability` 参数改为 `_arch`

2. **[Fwd,Sm100] 修复 tScS 分区用于 score_mod** - Tri Dao
   - 在 `flash_attn/cute/flash_fwd_sm100.py` 中添加了 tScS 分区修复代码

3. **[Fwd,Sm100] 为 s0_s1_sequence 使用管道抽象** - Tri Dao
   - 重构了 `flash_attn/cute/flash_fwd_sm100.py` 中的代码以使用管道抽象

4. **[Fwd,Sm100] 将 split_P_arrive 设为可调参数** - Tri Dao
   - 在 `flash_attn/cute/blackwell_helpers.py` 和 `flash_attn/cute/flash_fwd_sm100.py` 中实现
   - 使 split_P_arrive 成为可调参数以提高灵活性

5. **[Fwd,Sm100] 使用 TmemAllocator** - Tri Dao
   - 在 `flash_attn/cute/flash_fwd_sm100.py` 中集成了 TmemAllocator

6. **[Fwd,Sm100] 为 mbar_P_full_2 使用管道抽象** - Tri Dao
   - 在 `flash_attn/cute/flash_fwd_sm100.py` 和 `flash_attn/cute/pipeline.py` 中添加了管道抽象

7. **[Fwd,Sm100] 为 O_full 使用管道抽象** - Tri Dao
   - 在 `flash_attn/cute/flash_fwd_sm100.py` 中添加了 O_full 的管道抽象

8. **[cute] 添加 return_lse 功能 (#2271)** - Erik Wijmans
   - 在 `flash_attn/cute/interface.py` 中添加了 return_lse 功能

### 问题报告

1. **[Cute][Test] 单元测试 `test_flash_attn_kvcache` 在 B200 上挂起**
   - 问题描述：在 B200 GPU 上运行特定测试用例时，测试挂起
   - 复现命令：`pytest -x 'tests/cute/test_flash_attn.py::test_flash_attn_kvcache[1-128-64-False-False-False-1-0.0-True-False-False-False-False-False-False-mha-dtype0]'`
   - 症状：GPU sm_util 100%，功耗保持空闲水平，cpu-side 在 cudaStreamSynchronize 处卡住
### sgl-project/sglang
提交：
- [CI] Disable test_lora_update: Nutanix LoRA adapter removed from HuggingFace (#19527) [`2bd2b60`](https://github.com/sgl-project/sglang/commit/2bd2b60b5cdff59972318eb55f3deac8098cfc09)
- 受影响文件: test/registered/lora/test_lora_update.py
- Fix nvfp4 weight update (#18085) [`9469ad0`](https://github.com/sgl-project/sglang/commit/9469ad089b670a3980f5bbebbf35d0f598e8f44d)
- 受影响文件: python/sglang/srt/layers/moe/moe_runner/flashinfer_trtllm.py, python/sglang/srt/layers/quantization/modelopt_quant.py, python/sglang/srt/layers/utils/common.py
- Fix nightly VLM accuracy: gemma3n TP fixes + removal, latency thresholds (#19401) [`6ca7da3`](https://github.com/sgl-project/sglang/commit/6ca7da3e7c1abb2975afd3f52a990cd14a51f3bb)
- 受影响文件: python/sglang/srt/models/gemma3n_causal.py, python/sglang/srt/models/gemma3n_mm.py, test/registered/eval/test_vlms_mmmu_eval.py
- CI: use 'sglang serve' in CI tests (#18597) [`e6da514`](https://github.com/sgl-project/sglang/commit/e6da514c2c016d2fb269c4b2bf16d105dc18df90)
- 受影响文件: python/sglang/cli/utils.py, python/sglang/multimodal_gen/utils.py, python/sglang/test/test_utils.py
- [3/n] deepseek_v2.py Refactor: Migrate MLA forward method in deepseek_v2.py (#19122) [`776709e`](https://github.com/sgl-project/sglang/commit/776709efe89390599302a244f6ab1626f02ba9ff)
- 受影响文件: python/sglang/srt/models/deepseek_common/attention_backend_handler.py, python/sglang/srt/models/deepseek_common/attention_forward_methods/__init__.py, python/sglang/srt/models/deepseek_common/attention_forward_methods/forward_methods.py, python/sglang/srt/models/deepseek_common/attention_forward_methods/forward_mla.py, python/sglang/srt/models/deepseek_common/attention_forward_methods/forward_mla_fused_rope_cpu.py, python/sglang/srt/models/deepseek_common/attention_forward_methods/forward_mla_fused_rope_rocm.py, python/sglang/srt/models/deepseek_v2.py, test/srt/cpu/test_qkv_proj_with_rope.py, test/srt/cpu/test_rope.py
- [AMD] Enable cudagraph for aiter nsa backend and add aiter impl for nsa pr… (#18526) [`7e46aaf`](https://github.com/sgl-project/sglang/commit/7e46aafebb6dded04f6ba345a8e203049618704a)
- 受影响文件: python/sglang/srt/layers/attention/nsa/triton_kernel.py, python/sglang/srt/layers/attention/nsa_backend.py
- [HiCache] refactor page_first_direct io kernel (#18113) [`36dc973`](https://github.com/sgl-project/sglang/commit/36dc973cbfb9c35688cc2799a61591d80ce25d13)
- 受影响文件: sgl-kernel/csrc/kvcacheio/transfer.cu, sgl-kernel/tests/test_kvcacheio.py
- Fix BatchMLAPagedAttentionWrapper query/qo_inptr mismatch for EAGLE (#15601) [`1b75d0d`](https://github.com/sgl-project/sglang/commit/1b75d0d1a979a324a24daf6a10fd8491b579f1c9)
- 受影响文件: python/sglang/srt/layers/attention/trtllm_mla_backend.py
- Fix HiCacheNixl TypeError: mem_pool_host passed as file_path (#19517) [`6a1480c`](https://github.com/sgl-project/sglang/commit/6a1480ce45aa06c1f712ba4f3169f835a799ec0c)
- 受影响文件: python/sglang/srt/mem_cache/storage/backend_factory.py
- Remove gpt-oss hybrid swa gate for trtllm_mha (#19079) [`35ef38c`](https://github.com/sgl-project/sglang/commit/35ef38c61b2de3ce905836f18cfffd80e56c0a00)
- 受影响文件: python/sglang/srt/server_args.py
- [AMD] Fix AMD CI test of TestToolChoiceLfm2Moe (#19113) [`1b79934`](https://github.com/sgl-project/sglang/commit/1b79934d347ff1799fe651ee08d1d7a8cff3001f)
- 受影响文件: .github/workflows/pr-test-amd-rocm720.yml, .github/workflows/pr-test-amd.yml, python/sglang/srt/layers/attention/aiter_backend.py, python/sglang/srt/layers/attention/mamba/causal_conv1d.py, scripts/ci/utils/slash_command_handler.py, test/registered/lora/test_multi_lora_backend.py, test/registered/openai_server/function_call/test_tool_choice.py, test/run_suite.py
- Allow PR authors to use /rerun-failed-ci on their own PRs (#19496) [`2c856c6`](https://github.com/sgl-project/sglang/commit/2c856c6d2766e8b88f4e1ed2b29ceeb3dcbe790b)
- 受影响文件: docs/developer_guide/contribution_guide.md, scripts/ci/utils/slash_command_handler.py
- [diffusion] fix: MulAdd 4D path (shift indexing) (#18673) [`fe4bc8e`](https://github.com/sgl-project/sglang/commit/fe4bc8ebd5210817bcd1dcd80789e1158bbd5739)
- 受影响文件: python/sglang/jit_kernel/diffusion/triton/scale_shift.py
- [Diffusion] [NPU] [CI] fix CI performance (#19486) [`b1249ac`](https://github.com/sgl-project/sglang/commit/b1249ac909297cb94e0b969d3abeb89229792038)
- 受影响文件: python/sglang/multimodal_gen/runtime/platforms/npu.py, python/sglang/multimodal_gen/test/server/ascend/perf_baselines_npu.json
- [Qwen3-Next] Support gdn fused_rms_norm_gated (#19434) [`d2885a9`](https://github.com/sgl-project/sglang/commit/d2885a9094c3847769923322e51a85841a552662)
- 受影响文件: python/sglang/srt/layers/attention/fla/fused_norm_gate.py, python/sglang/srt/layers/attention/fla/kda.py, python/sglang/srt/models/kimi_linear.py, python/sglang/srt/models/qwen3_next.py
- [diffusion] fix: Support default response_format=url in /v1/images/generations to avoid 400 errors when response_format is omitted (#19360) [`ca5f2e2`](https://github.com/sgl-project/sglang/commit/ca5f2e2ed13c03e5e6f76b4c4e5428a45839268d)
- 受影响文件: docs/diffusion/api/openai_api.md, python/sglang/multimodal_gen/runtime/entrypoints/openai/image_api.py
- [PD-Disagg][Fix] Remove 'test_external_dp_routing' from Rust Router constructor parameters. (#19492) [`98e433e`](https://github.com/sgl-project/sglang/commit/98e433e305c54f1ccf7704049fa6cb60e5190939)
- 受影响文件: sgl-model-gateway/bindings/python/src/sglang_router/router.py
- [AMD] [MiniMax-M2.5 Day 0] Add MiniMax-M2.5 nightly accuracy test (#19443) [`403195d`](https://github.com/sgl-project/sglang/commit/403195d59de0db4fe37d0b730e9cf6c2a9d9749c)
- 受影响文件: .github/workflows/nightly-test-amd-rocm720.yml, .github/workflows/nightly-test-amd.yml, docs/basic_usage/minimax_m2.md, docs/supported_models/text_generation/generative_models.md, test/registered/amd/accuracy/mi30x/test_minimax_m25_eval_amd.py, test/registered/amd/accuracy/mi35x/test_minimax_m25_eval_mi35x.py
- [AMD] remove redundancy H2D op in aiter attention backend (#19416) [`f69ca93`](https://github.com/sgl-project/sglang/commit/f69ca93d496d96ce881a11b6992601cf0c18bd81)
- 受影响文件: python/sglang/srt/layers/attention/aiter_backend.py
- [cache] add conservative estimation (#19482) [`07da4be`](https://github.com/sgl-project/sglang/commit/07da4bed7b31cdb28f582bbfddd852373def51f1)
- 受影响文件: python/sglang/srt/managers/schedule_policy.py
- [AMD] Use `tilelang` as default NSA attention backend dispatch on AMD Instinct (#18319) [`9496bbd`](https://github.com/sgl-project/sglang/commit/9496bbd7b11c7ce7ac96b339c7a1fb23d0406545)
- 受影响文件: python/sglang/srt/layers/attention/nsa_backend.py, python/sglang/srt/server_args.py
- [AMD] Merge Dockerfiles for ROCm (#19203) [`9b2fbf7`](https://github.com/sgl-project/sglang/commit/9b2fbf7e6ad4730c0f6abf49a307fa2fecc4952a)
- 受影响文件: .github/workflows/pr-test-amd-rocm720.yml, .github/workflows/release-docker-amd-rocm720-nightly.yml, .github/workflows/release-docker-amd.yml, docker/rocm.Dockerfile, docker/rocm720.Dockerfile, scripts/ci/amd/amd_ci_start_container.sh
-  [4/N] (Elastic EP) Back up Expert Weights in DRAM (#17374) [`43fade5`](https://github.com/sgl-project/sglang/commit/43fade5f690e42ff8e1e78dfbe29ddc8fa8006fb)
- 受影响文件: docs/advanced_features/server_arguments.md, python/sglang/srt/elastic_ep/elastic_ep.py, python/sglang/srt/elastic_ep/expert_backup_client.py, python/sglang/srt/elastic_ep/expert_backup_manager.py, python/sglang/srt/entrypoints/engine.py, python/sglang/srt/environ.py, python/sglang/srt/managers/io_struct.py, python/sglang/srt/managers/scheduler.py, python/sglang/srt/model_executor/model_runner.py, python/sglang/srt/server_args.py, test/manual/ep/test_mooncake_expert_backup.py
- [NPU]kimi k2 thinking bugfix (#19387) [`eef44ec`](https://github.com/sgl-project/sglang/commit/eef44ec916fe24119e68117b2deeea3b03e7a88f)
- 受影响文件: python/sglang/srt/layers/moe/fused_moe_triton/layer.py, python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16_moe.py
- Add torch.compile support for qwen3-next on CPU (#12444) [`4f0f6cd`](https://github.com/sgl-project/sglang/commit/4f0f6cd9d0259fb5081ecc565dad2bb47643e8fa)
- 受影响文件: python/sglang/srt/layers/attention/hybrid_linear_attn_backend.py, python/sglang/srt/layers/attention/intel_amx_backend.py, python/sglang/srt/mem_cache/memory_pool.py, python/sglang/srt/model_executor/cpu_graph_runner.py, sgl-kernel/csrc/cpu/torch_extension_cpu.cpp
- llama4 npu adapt (#17123) [`bc91904`](https://github.com/sgl-project/sglang/commit/bc9190435b3fabd3b3c578e6675071f9387efe66)
- 受影响文件: python/sglang/srt/models/llama4.py, python/sglang/srt/server_args.py, test/registered/ascend/llm_models/test_ascend_llama4_scount_17b_16e.py
- [vlm][internVL] Support processor and embedding inputs for InternVL (#19127) [`f0c2089`](https://github.com/sgl-project/sglang/commit/f0c208959794abcd603c6495e6829b2ff6a06a46)
- 受影响文件: python/sglang/srt/models/internvl.py, python/sglang/srt/multimodal/processors/base_processor.py, python/sglang/srt/multimodal/processors/internvl.py, test/registered/vlm/test_vlm_input_format.py
- [diffusion] model: Fix a performance bug in the Wan model about usp (#19340) [`8a56cc5`](https://github.com/sgl-project/sglang/commit/8a56cc5836ac5eedab7c33f8aa4f8d6f24b660b7)
- 受影响文件: python/sglang/multimodal_gen/runtime/layers/attention/layer.py, python/sglang/multimodal_gen/runtime/layers/usp.py, python/sglang/multimodal_gen/runtime/models/bridges/mova_dual_tower.py, python/sglang/multimodal_gen/runtime/models/dits/wanvideo.py
- [Perf] Eliminate the slice op for Flashinfer `trtllm_fp4_block_scale_moe` (#15731) [`af3eccc`](https://github.com/sgl-project/sglang/commit/af3eccc9abf7bea2fcb278f63949c682094a0ac1)
- 受影响文件: python/sglang/srt/layers/quantization/mxfp4.py
- fix qwen3_vl visual module loading (#19333) [`d566816`](https://github.com/sgl-project/sglang/commit/d566816d838ce92d3ae044209f7d67eaa58ce74a)
- 受影响文件: python/sglang/srt/models/qwen3_vl.py
- [Diffusion] Add SGL-D diffusion efficient kernel skills (#19473) [`fe971b6`](https://github.com/sgl-project/sglang/commit/fe971b620a5fcd0c9a0f02eb4fcea6ad55506168)
- 受影响文件: .claude/skills/diffusion/use-efficient-diffusion-kernels.md
- Fix missing StandardCombineInput import in BF16 flashinfer_trtllm MoE (#19400) [`52c8a36`](https://github.com/sgl-project/sglang/commit/52c8a3632a97157ec99403b302d58670d8ff3bab)
- 受影响文件: python/sglang/srt/layers/moe/moe_runner/flashinfer_trtllm.py
Issues：
- [Bug] [Diffusion] FLUX.2-klein-9B image editing produces corrupted output with multi-GPU (`--num-gpus 8`) (https://github.com/sgl-project/sglang/issues/19449)
- Issue 内容已截断。
- [Feature] Sglang FA4 Refactor (https://github.com/sgl-project/sglang/issues/19447)
- Issue 内容已截断。
- [Feature] ModelOpt to CompressedTensors bridge (https://github.com/sgl-project/sglang/issues/19444)
- Issue 内容已截断。
- [Question] Offload or Release? (https://github.com/sgl-project/sglang/issues/19442)
- Issue 内容已截断。
- [Question] Still have 5GB can not offload in SGLang diffusion (https://github.com/sgl-project/sglang/issues/19441)
- Issue 内容已截断。
- [Bug] KeyError: 'model.language_model.layers.8.mlp.down_proj.weight' on finetuned qwen2-vl-7b (https://github.com/sgl-project/sglang/issues/19438)
- Issue 内容已截断。
- [Feature] Helix (TP + CP) Parallelism: A2A Communication + Decoupled Attention/FFN Parallelism (https://github.com/sgl-project/sglang/issues/19436)
- Issue 内容已截断。
- [Bug] ValueError: input_ids should be a list of lists for batch processing. (https://github.com/sgl-project/sglang/issues/19435)
- Issue 内容已截断。
### vllm-project/vllm
- **e369198** – 修复 ROCm 上 aiter rope 的 functionalization（[提交链接](https://github.com/vllm-project/vllm/commit/e3691988d0bf7c915d544d9204b53506815464ca)）  
  - 关键文件：`vllm/compilation/passes/utility/fix_functionalization.py`（+9 行，‑1 行）  
  - 主要改动：在 `fix_functionalization.py` 中加入对 `aiter` rope 的特殊处理，确保在 ROCm 环境下的算子能够正确 functionalize。

- **9fa6c68** – 在 ROCm 与 AITER 统一后端上启用 encoder 与 encoder‑decoder（[提交链接](https://github.com/vllm-project/vllm/commit/9fa6c68fa627c7ab041c48ac9987fb093719597f)）  
  - 关键文件：  
    - `docs/design/attention_backends.md`（微调文档）  
    - `vllm/v1/attention/backends/rocm_aiter_unified_attn.py`（+31 行）  
    - `vllm/v1/attention/backends/rocm_attn.py`（+73 行，‑5 行）  
  - 主要改动：实现统一的 ROCm/AITER 注意力后端，支持 encoder 与 encoder‑decoder 模型，提升跨平台兼容性。

- **2ce6f3c** – 引入原生权重同步 API：IPC（[提交链接](https://github.com/vllm-project/vllm/commit/2ce6f3cf67934ebe199188c9a1f83ff1c2d8ba96)）  
  - 关键文件：  
    - `examples/offline_inference/new_weight_syncing/rlhf_ipc.py`（新增 149 行）  
    - `examples/online_serving/new_weight_syncing/rlhf_http_ipc.py`（新增 181 行）  
    - `vllm/distributed/weight_transfer/ipc_engine.py`（新增 291 行）  
    - 其他：`tests/distributed/test_weight_transfer.py`（+453 行）等  
  - 主要改动：实现基于进程间通信（IPC）的权重同步机制，提供 `WeightTransferEngine` 接口，支持 RLHF 场景下的高效权重转移。

- **1f3dbd9** – 修复 GPT‑OSS 模型的 batch 不变性问题（[提交链接](https://github.com/vllm-project/vllm/commit/1f3dbd95fd13849e974f1f31ff36b3e91d7768bc)）  
  - 关键文件：`vllm/model_executor/models/gpt_oss.py`（+13 行，‑2 行）  
  - 主要改动：在 `GPTOSS` 实现中调整 batch 处理逻辑，确保在不同 batch 大小下的推理结果保持一致。

- **1d532f9** – 仅在实际使用 cudagraph 时才启用 DP padding（[提交链接](https://github.com/vllm-project/vllm/commit/1d532f9d8fb205942035313293af701ee580a7e2)）  
  - 关键文件：`vllm/v1/cudagraph_dispatcher.py`（+40 行，‑22 行）等  
  - 主要改动：在 `cudagraph_dispatcher` 中加入条件判断，避免在未使用 cudagraph 时额外的 DP padding 开销。

- **234a65b** – 添加 monkey‑patch 防止编译器写入竞争（[提交链接](https://github.com/vllm-project/vllm/commit/234a65b781d9dc51d28aebb208096baa8fe0458e)）  
  - 关键文件：`vllm/compilation/compiler_interface.py`（+43 行）  
  - 主要改动：在编译器接口层加入锁机制，防止多线程环境下的文件写入冲突。

- **2decec9** – 当 `num_nextn_predict_layers=0` 时忽略 MTP 权重（[提交链接](https://github.com/vllm-project/vllm/commit/2decec9856033347f3129f1d1b2ec015e1ad88ea)）  
  - 关键文件：`vllm/model_executor/models/transformers/base.py`（+13 行）  
  - 主要改动：在 Transformers 后端的模型构造中加入判断，避免加载无效的 MTP 权重。

- **29b3547** – 修复 pytree slice 节点的缓存错误（[提交链接](https://github.com/vllm-project/vllm/commit/29b35477b0661f527d2b951ff5125f5c58fce3fe)）  
  - 关键文件：`vllm/compilation/caching.py`（+19 行，‑2 行）  
  - 主要改动：在编译缓存层面处理 `pytree` 切片节点，防止因缓存键冲突导致的错误。

- **b1d9f53** – 为 Model Runner V2 添加 kernel 预热（[提交链接](https://github.com/vllm-project/vllm/commit/b1d9f5372d802513e9e009a5d572dd594a09e1dc)）  
  - 关键文件：`vllm/v1/worker/gpu/warmup.py`（新增 105 行）  
  - 主要改动：在 GPU worker 启动时预热常用 kernel，降低首次推理的延迟。

- **fd6de37** – 修复 Transformers 后端的 3D rope 实现（[提交链接](https://github.com/vllm-project/vllm/commit/fd6de37fcafe9540ed821256877127df75d74db8)）  
  - 关键文件：`vllm/model_executor/models/transformers/multimodal.py`（+12 行，‑2 行）  
  - 主要改动：纠正 3D rope 参数的计算方式，提升多模态模型的数值稳定性。

- **c8aca0c** – 支持 Parakeet 作为 Nemotron‑Nano‑VL 的音频编码器（[提交链接](https://github.com/vllm-project/vllm/commit/c8aca0c9e1b35ee4a1683a01467e638b23076a37)）  
  - 关键文件：`vllm/model_executor/models/nano_nemotron_vl.py`（+254 行，‑20 行）  
  - 新增：`vllm/model_executor/models/parakeet.py`、`vllm/transformers_utils/configs/parakeet.py`  
  - 主要改动：集成 Parakeet 编码器，实现音频特征的端到端处理。

- **b602e4f** – 修正文档中 Llama chat template 的链接（[提交链接](https://github.com/vllm-project/vllm/commit/b602e4f299a596a14402e6a4ead5e51abb180c49)）  
  - 关键文件：`docs/serving/openai_compatible_server.md`（微调）  
  - 主要改动：更新指向 Llama chat template 的 URL，提升文档可用性。

- **157722d** – 在 `do_mamba_copy_block` 中使用 pinned memory 加速异步 H2D 传输（[提交链接](https://github.com/vllm-project/vllm/commit/157722da756daa6f967433903680745abc0c4861)）  
  - 关键文件：`vllm/v1/worker/mamba_utils.py`（+60 行，‑34 行）  
  - 主要改动：为 Mamba 复制块引入 pinned memory，降低主机‑GPU 数据拷贝延迟。

- **1d897ff** – 补全 v1 代码所有者（CODEOWNERS）条目（[提交链接](https://github.com/vllm-project/vllm/commit/1d897ff04f90c46041ed3966dea671a6ae532184)）  
  - 关键文件：`.github/CODEOWNERS`（+5 行，‑2 行）  
  - 主要改动：为 v1 目录下的关键文件添加维护者，提升审查效率。

- **905d76b** – 新增 HuggingFace `skt/A.X‑K1` 模型支持（[提交链接](https://github.com/vllm-project/vllm/commit/905d76b51dc3b98c4d0ee35317493f21f9e6b5d0)）  
  - 关键文件：`vllm/model_executor/models/AXK1.py`（新增 1168 行）  
  - 新增配置：`vllm/transformers_utils/configs/AXK1.py`（+215 行）  
  - 主要改动：实现 `AXK1` 模型的加载、推理路径，并在模型注册表中加入对应条目。

- **9098ce6** – Helion kernel 使用 HOP 表示以支持 fx tracing 与模式匹配（[提交链接](https://github.com/vllm-project/vllm/commit/9098ce690c802887fb36a8f3ee95bb18aacd2f76)）  
  - 关键文件：`vllm/kernels/helion/register.py`（+111 行，‑16 行）  
  - 新增测试：`tests/kernels/helion/test_pattern_matching.py`（+203 行）  
  - 主要改动：在 Helion kernel 注册表中加入 HOP 表示层，便于 TorchFX 追踪和自动化模式匹配。

- **876312f** – 修复 `gpu_worker.py` 的 pre‑commit 检查错误（[提交链接](https://github.com/vllm-project/vllm/commit/876312f0b59b24f95704a37c93675e36a018a140)）  
  - 关键文件：`vllm/v1/worker/gpu_worker.py`（+6 行，‑2 行）  
  - 主要改动：调整代码格式，消除 lint 报错。

- **5de98ab** – 将 @BoyuanFeng 加入 CODEOWNERS（[提交链接](https://github.com/vllm-project/vllm/commit/5de98abc122dd4049adf1234c9fc20b5cc83d6cb)）  
  - 关键文件：`.github/CODEOWNERS`（+1 行，‑1 行）  
  - 主要改动：更新所有者列表，确保新贡献者得到审查权。

- **9251ed5** – 处理 Kimi 模型在推理结束时出现工具调用的情况（[提交链接](https://github.com/vllm-project/vllm/commit/9251ed5c4fc6c954a5cdc5399d9d4f25ea5a8dd3)）  
  - 关键文件：`vllm/reasoning/kimi_k2_reasoning_parser.py`（新增 228 行）  
  - 主要改动：在 Kimi 推理解析器中加入对工具调用的检测与回退逻辑，防止异常终止。

- **e824937** – 修正多模态 Qwen2.5 Omni 的 `check_interleaved_audio_video` 假阳性（[提交链接](https://github.com/vllm-project/vllm/commit/e8249378e414d76387a027a6ffb5bde8b9aff765)）  
  - 关键文件：`vllm/model_executor/models/qwen2_5_omni_thinker.py`（+25 行，‑4 行）  
  - 主要改动：在音视频交叉检查函数中加入批处理判断，避免误报。

- **6d4f9d3** – 修复 DCP + FA3 在缺失 `num_splits` 时的崩溃（[提交链接](https://github.com/vllm-project/vllm/commit/6d4f9d3ad5aa3750697edcf013ad080619ae25e9)）  
  - 关键文件：`vllm/v1/attention/backends/flash_attn.py`（+2 行）  
  - 主要改动：在 FlashAttention 后端加入 `num_splits` 参数的默认值处理。

- **fbe3f01** – 回滚 GLM‑OCR 配置的添加（[提交链接](https://github.com/vllm-project/vllm/commit/fbe3f0120a5e786a1459c982c72185311c78a276)）  
  - 关键文件：`vllm/transformers_utils/configs/glm_ocr.py`（删除 91 行）  
  - 主要改动：撤销对 GLM‑OCR 配置的引入，恢复原始代码基线。

- **66c1751** – 移除序列并行时不必要的 `+rms_norm` 强制（[提交链接](https://github.com/vllm-project/vllm/commit/66c1751d13b7b3c294418a88fbfbfe2ec49d5d3f)）  
  - 关键文件：`vllm/config/vllm.py`（‑9 行）  
  - 主要改动：在配置文件中删除对 RMSNorm 的强制开启，简化序列并行设置。

- **6467b63** – 为 `RMSNormGated` 添加缺失的 `activation` 属性（[提交链接](https://github.com/vllm-project/vllm/commit/6467b635b6acfd5b30dd31804c79ef70ad4bf834)）  
  - 关键文件：`vllm/model_executor/layers/layernorm.py`（+3 行）  
  - 主要改动：在 `RMSNormGated` 类中补全 `activation` 参数，防止运行时属性错误。

- **9c3fe99** – 为 Qwen3 VL ViT 注意力实现 FlashInfer + cuDNN 后端（[提交链接](https://github.com/vllm-project/vllm/commit/9c3fe9936b929b5503d780bd4e8e3cd524de1c4e)）  
  - 关键文件：`vllm/model_executor/layers/attention/mm_encoder_attention.py`（+170 行，‑2 行）  
  - 主要改动：在多模态编码器注意力中集成 FlashInfer 与 cuDNN 实现，提升 ViT 注意力的吞吐。

- **b66a746** – 将 completions 接口的 `response_format` 校验从 `assert` 改为抛 `ValueError`（[提交链接](https://github.com/vllm-project/vllm/commit/b66a74649e40022c6b1180b98fbb1b6e4b4af74a)）  
  - 关键文件：`vllm/entrypoints/openai/completion/protocol.py`（+29 行，‑1 行）  
  - 主要改动：改进错误处理，使非法 `response_format` 返回明确的 HTTP 400 错误。

- **07bdabe** – 在异步 TP reduce‑scatter 中使用 `sum` 而非 `avg`（[提交链接](https://github.com/vllm-project/vllm/commit/07bdabef03c78c9dc6beb549185930888ec6f2ce)）  
  - 关键文件：`vllm/compilation/passes/fusion/collective_fusion.py`（±3 行）  
  - 主要改动：修正聚合操作的归约方式，避免数值偏差。

- **a572baf** – 为 H200 GPU 添加 Qwen3MoE 的调优 MoE 配置（[提交链接](https://github.com/vllm-project/vllm/commit/a572baff5e5fde4aa3fb92961de04eb043dc5cf4)）  
  - 关键文件：`vllm/model_executor/layers/fused_moe/configs/E=128,N=96,device_name=NVIDIA_H200.json`（新增 147 行）等  
  - 主要改动：提供针对 H200 的 MoE 超参数配置，提升大模型在该硬件上的性能。

- **516cf26** – 修正 `rms_norm_gated` 原生路径的输出 dtype（[提交链接](https://github.com/vllm-project/vllm/commit/516cf26698f07d2c92dd61bd5541ceb1376401bc)）  
  - 关键文件：`vllm/model_executor/layers/layernorm.py`（±1 行）  
  - 主要改动：确保 `RMSNormGated` 在原生实现中返回正确的 dtype，避免类型不匹
### NVIDIA/cutile-python
昨日无更新。

## 总结
### 关注要点 & 未来展望
- **TMA 2D Tile 存储**：CUTLASS 社区已登记需求，建议关注后续实现进度，尤其对 SM90A 及更高代数的显存搬运优化至关重要。
- **SM120 XQA 边界**：flashinfer 报告的 `head_dim=256 & page_size<64` 崩溃仍未解决，需在页面分配层面加入更细粒度的检查或自动调节。
- **AMD/ROCm 统一后端**：vllm 与 sglang 已在 ROCm 上实现统一注意力后端，后续可期待更多模型（如 Qwen‑3‑MoE）在 AMD Instinct 上的原生加速。
- **安全与可维护**：vllm 的编译器锁与 CODEOWNERS 扩展为多线程环境提供了防护，建议在自研推理框架中同步这些最佳实践。
- **性能回归监控**：Flash‑Attention 在 B200 上的 KVCACHE 测试仍挂起，建议在 CI 中加入更广泛的硬件覆盖，以防止类似的隐藏瓶颈。

保持对上述热点的跟进，将有助于在多硬件、多模态的大模型部署时代保持竞争优势。
