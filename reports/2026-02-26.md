今日的 AI Infra 的新闻如下。

## 摘要
```json
{
  "summary": "NVIDIA的CUTlass移除了冗余DSL示例，但TMA操作文档仍存在空白。FlashInfer扩展了GLM-5支持并优化了GDN解码性能，尽管XQA测试在特定配置下显示局限性。Flash-attention解决了mask mod相关bug并增强了SM100性能，新增了LSE返回功能。SGLang在AMD优化、CI改进和调试工具方面进行了广泛更新。VLLM加强了ROCm支持，添加了Nemotron模型支持，并实现了性能优化，但仍存在多个兼容性问题。",
  "conclusion": "生态系统持续发展，特别是在Blackwell和AMD平台上的硬件特定优化。然而，不同GPU架构和配置之间的兼容性挑战依然存在。团队在部署新功能前应仔细评估测试结果，特别是针对小页面大小或特定头维度等边缘情况。性能改进的焦点显而易见，但在多样化环境中进行全面测试仍然至关重要。"
}
```

## 具体内容分析
### deepseek-ai/DeepGEMM
昨日无更新。
### deepseek-ai/FlashMLA
昨日无更新。
### NVIDIA/cutlass
**提交**  
- `057635d` – [Remove redundant dsl example. (#3074)](https://github.com/NVIDIA/cutlass/commit/057635de5c493a34e9913e2c5e836bdd98c58861)  
  - 作者：Junkai‑Wu，2026‑02‑26 13:10:59 UTC  
  - 变更文件：`examples/python/CuTeDSL/blackwell/mla.py`（删除，删除 5 197 行）  

- `c213bfd` – [Remove redundant dsl examples. (#3071)](https://github.com/NVIDIA/cutlass/commit/c213bfdfc1f4ffb156c69d51d07efcf7f367f2fb)  
  - 作者：Junkai‑Wu，2026‑02‑26 03:42:01 UTC  
  - 变更文件：  
    - `examples/python/CuTeDSL/blackwell/grouped_mixed_input_gemm.py`（删除，删除 3 198 行）  
    - `examples/python/CuTeDSL/blackwell/mixed_input_gemm.py`（删除，删除 2 674 行）  

**Issues**  
- **标题**：`[DOC] Storing 2D Tile From Accumulator Registers To Global Memory Using TMA.`  
  - 链接：<https://github.com/NVIDIA/cutlass/issues/3076>  
  - 作者：tugrul512bit，创建于 2026‑02‑26 20:43:49 UTC  
  - 备注：Issue 正文被截断（`body_truncated: true`），因此无法完整呈现提问内容。已知提问涉及在 SM90A 架构上使用 TMA 将累加器寄存器中的 2D Tile 写回全局内存的文档缺失，以及动态传递 TMA 描述符数组的实现困难。  

---  
*以上信息基于 2026‑02‑26 的仓库快照整理。*
### flashinfer-ai/flashinfer
## 2026-02-26 FlashInfer 更新摘要

### 提交

1. **支持 GLM-5 的 qk_nope_head_dim=192 配置** (#2607)
   - 修改 `flashinfer/mla.py`：扩展验证逻辑，允许 qk_nope_head_dim 为 128 或 192（之前仅支持 128）
   - 修改 `tests/attention/test_trtllm_gen_mla.py`：添加对不同 qk_nope_head_dim 和 num_attn_heads 的测试支持
   - 提交：[ad94692](https://github.com/flashinfer-ai/flashinfer/commit/ad94692d6b911af9498415d2faa946fbb3bba882)

2. **优化 GDN 解码预转置内核性能** (#2588)
   - 修改 `flashinfer/gdn_decode.py`：通过提前读取门控值隐藏延迟，优化小批量场景下的 GDN 解码预转置内核
   - 提交：[1589ebb](https://github.com/flashinfer-ai/flashinfer/commit/1589ebb405b437dab5a3b0ca3b77097da0959071)

3. **修复 codeowners_analyzer.py 中的重复用户名 bug** (#2637)
   - 修改 `scripts/codeowner_analyzer.py`：添加不区分大小写的用户名去重逻辑，防止同一用户因不同邮箱地址而重复出现
   - 提交：[f852eb6](https://github.com/flashinfer-ai/flashinfer/commit/f852eb6cd24cd9e17dc254a13a80701c7d25ee58)

### 问题

1. **XQA 测试在 head_dim=256 且 pagesize<64 时失败** (#2638)
   - 在 SM120 架构（RTX PRO 6000 Blackwell/5090）上，当 head_dim=256 且 page_size<64 时，XQA 测试失败
   - 测试在 page_size=64 时通过，但在 page_size=16 和 32 时失败
   - 问题报告：[#2638](https://github.com/flashinfer-ai/flashinfer/issues/2638)
### Dao-AILab/flash-attention
## 提交

**修复 mask mod 相关 bug**

- 修复了 `block_sparse_utils.py` 中的变量名错误，将 `o_corr_consumer_phase` 改为 `corr_epi_producer_phase`。[944e457](https://github.com/Dao-AILab/flash-attention/commit/944e4574c6d3d972fe7ec3b77951bd51ae10f2b5)
- 在 `mask.py` 中修正了 `tScS` 的索引方式，添加 `tScS = tScS[(None, None), 0, 0]` 以处理边界情况。[944e457](https://github.com/Dao-AILab/flash-attention/commit/944e4574c6d3d972fe7ec3b77951bd51ae10f2b5)
- 更新测试文件 `test_mask_mod.py`，将 `_compute_capability` 参数改为 `_arch`。[944e457](https://github.com/Dao-AILab/flash-attention/commit/944e4574c6d3d972fe7ec3b77951bd51ae10f2b5)

**优化 SM100 架构下的 score_mod 实现**

- 在 `flash_fwd_sm100.py` 中调整 `tScS` 分区逻辑，增加 `tScS = tScS[(None, None), 0, 0]` 以提升兼容性。[aa5f7db](https://github.com/Dao-AILab/flash-attention/commit/aa5f7db2862650ae5fcca3938072e8d4920efd65)

**重构 SM100 前向计算流水线**

- 使用流水线抽象优化 `s0_s1_sequence` 处理流程，调整共享内存屏障（mbar）相关偏移量设置。[bf4d8ee](https://github.com/Dao-AILab/flash-attention/commit/bf4d8eec227aebb22531e1d166349c64a08741b7)
- 引入可调参数 `split_P_arrive` 控制 P 矩阵写入时机，增强性能调优灵活性。[0293155](https://github.com/Dao-AILab/flash-attention/commit/02931551ece7eb7f36e94302ad79daee6beda2e6)
- 集成 `TmemAllocator` 管理共享内存分配，并更新命名屏障枚举值。[ed85ed7](https://github.com/Dao-AILab/flash-attention/commit/ed85ed7ab2e3545c600574e33577648fc481d11b)
- 进一步简化流水线中 `mbar_P_full_2` 和 `O_full` 的管理逻辑。[cf027a4](https://github.com/Dao-AILab/flash-attention/commit/cf027a4961cafd9ca1175e1c5e2aa65f0975295d) [ffbc678](https://github.com/Dao-AILab/flash-attention/commit/ffbc678bf3985778aa60d9dd29ce43cad279303b)

**新增功能：支持返回 LogSumExp (LSE)**

- 在接口层和前向实现中加入 `return_lse` 参数选项，允许用户获取注意力分数的对数归一化项。注意当前版本暂不支持梯度回传。[5963594](https://github.com/Dao-AILab/flash-attention/commit/59635947a020c3a99ce4bd360d4e221b8f3af572)

## Issues

**[Cute][Test] 单元测试 `test_flash_attn_kvcache` 在 B200 上挂起**

- 测试在特定配置下运行时出现 GPU 利用率满载但功耗处于空闲水平、CPU 端卡在 `cudaStreamSynchronize` 的现象。
- 示例复现命令：
  ```
  pytest -x 'tests/cute/test_flash_attn.py::test_flash_attn_kvcache[1-128-64-False-False-False-1-0.0-True-False-False-False-False-False-False-mha-dtype0]'
  ```  
  Issue 链接：[#2278](https://github.com/Dao-AILab/flash-attention/issues/2278)
### sgl-project/sglang
**提交**

| 短 SHA | 提交信息 | PR 链接 | 关键文件 / 代码片段 |
|--------|----------|--------|-------------------|
| 5172c37 | **[AMD] Use fused GEMM with FP8 cast for FP8 prefill** | <https://github.com/sgl-project/sglang/commit/5172c378456fa9735ba6429a2ca7688d1152d256> | `python/sglang/srt/layers/attention/aiter_backend.py`：新增 `fp8_prefill_kv_indices` 字段；在 `init_forward_metadata` 中生成 `torch.arange(total_s)` 用于 FP8 预填充。 |
| b79fa9c | **[CI] Add Claude skill for writing SGLang tests** | <https://github.com/sgl-project/sglang/commit/b79fa9ccd71fbaf7bcbb12f888c3ca3378379386> | ` .claude/skills/write-sglang-test/SKILL.md`（新增 248 行，定义 Claude 编写测试的指引）。 |
| a1ef8e2 | **[AMD] optimize Kimi K2.5 fused_moe_triton performance by tuning** | <https://github.com/sgl-project/sglang/commit/a1ef8e2cc0976eeb0b2bacf265bed80c645a7ded> | `benchmark/kernels/fused_moe_triton/tuning_fused_moe_triton.py`（调参脚本，新增 63 行）；`python/sglang/srt/layers/moe/fused_moe_triton/configs/.../int4_w4a16.json`（新增配置文件 164 行）。 |
| 288300a | **[PD] Tiny code cleanup for prefill info registering** | <https://github.com/sgl-project/sglang/commit/288300aafd15c5a454404d44b312de4a019f59b3> | `python/sglang/srt/disaggregation/base/conn.py`（仅一行改动，删除空行）；`python/sglang/srt/disaggregation/common/conn.py`（17 行新增，19 行删除）。 |
| e4b708d | **[Spec V2] Support specV2 for mamba hybrid attention** | <https://github.com/sgl-project/sglang/commit/e4b708d3e9dd90613a9707c5c3cd4e98cd8c4cd8> | `python/sglang/srt/layers/attention/hybrid_linear_attn_backend.py`（新增 7 行）；`python/sglang/srt/speculative/eagle_worker_v2.py`（新增 74 行实现 V2 版 Eagle worker）。 |
| 78d6674 | **[diffusion] feat: support hybrid parallelism for diffusers backend** | <https://github.com/sgl-project/sglang/commit/78d6674c45137ff3a625f5fac2f228ae95f2c8f4> | `docs/diffusion/performance/cache/cache_dit.md`（新增 70 行性能说明）；`python/pyproject.toml`（微调依赖版本）。 |
| 5939b89 | **[MUSA][11/N] ci: add MUSA 4.3 kernel build and release pipeline** | <https://github.com/sgl-project/sglang/commit/5939b8912a823a55ed6a9437d7262d99dff716b4> | `.github/workflows/release-whl-kernel.yml`（新增 99 行 CI 步骤）；`scripts/ci/musa/rename_wheels_musa.sh`（新增 46 行脚本）。 |
| e55e655 | **[Bugfix] Add rids to the batch filtering for two batch overlap** | <https://github.com/sgl-project/sglang/commit/e55e65535e59e4784817550eac400d6c9f830efe> | `python/sglang/srt/batch_overlap/two_batch_overlap.py`（新增 1 行 `rids` 过滤逻辑）。 |
| 97f1fa5 | **[NPU] Fix disaggregation metadata buffer bootstrap_room_dtype for npu backend** | <https://github.com/sgl-project/sglang/commit/97f1fa5e6bfd558217214fca9918e950c3f1bbd7> | `python/sglang/srt/disaggregation/utils.py`（修改 4 行 dtype 处理）。 |
| 86eb800 | **[NPU] support Kimi-K2.5 on NPU** | <https://github.com/sgl-project/sglang/commit/86eb80007e78b61bc8bf91ebe48aa8ee2eb19f01> | `python/sglang/srt/models/kimi_k25.py`（新增 14 行模型适配代码）。 |
| bdc1e46 | **[Qwen3.5] Qwen3.5-27B inference repeat bug fix** | <https://github.com/sgl-project/sglang/commit/bdc1e46e5ac93dd7ee2d53e5d8c2d3de7dc03f69> | `python/sglang/srt/models/qwen3_5.py`（仅 2 行改动，修复重复推理）。 |
| 74c8e7b | **refactor(jit_kernel): reduce duplication and separate test code** | <https://github.com/sgl-project/sglang/commit/74c8e7b2152471048c2275d3cb58d07e9281f715> | `python/sglang/jit_kernel/hadamard.py`（删除 84 行旧实现，新增 21 行简化实现）。 |
| a7152df | **[diffusion] CLI: Fix typo in CLI usage doc string** | <https://github.com/sgl-project/sglang/commit/a7152df2e34be67b6ca83273b8603f9243858f38> | `python/sglang/multimodal_gen/runtime/entrypoints/cli/generate.py`（修正 1 行文档字符串）。 |
| 27fd014 | **[PD] Add kv_cache_dtype consistency check for PD Disaggregation** | <https://github.com/sgl-project/sglang/commit/27fd014726a9f5b308cf940c9553751b0e384a60> | `python/sglang/srt/disaggregation/common/conn.py`（新增 21 行 dtype 检查）。 |
| e14fd4a | **Fix nightly Mistral-Large-3 NVFP4 accuracy threshold** | <https://github.com/sgl-project/sglang/commit/e14fd4accb43cc551dc7cb08a69ef2c2e74c4922> | `test/registered/8-gpu-models/test_mistral_large3.py`（微调阈值 1 行）。 |
| d2b3c7f | **[Tracing] update script for converting otel tracing data to perfetto format** | <https://github.com/sgl-project/sglang/commit/d2b3c7fb14d97d744b8df198ef45c767237720f6> | `scripts/convert_otel_2_perfetto.py`（新增 195 行，删除 107 行，完整转换脚本）。 |
| de3d1e7 | **[misc] use ORJSONResponse in http-server generate** | <https://github.com/sgl-project/sglang/commit/de3d1e766949f7dae20728095ba3ec4623ebf086> | `python/sglang/srt/entrypoints/http_server.py`（将返回改为 `ORJSONResponse`，新增 8 行）。 |
| 0fd44ff | **Fix NSA CP positions mismatch in eagle NextN model** | <https://github.com/sgl-project/sglang/commit/0fd44ff342bb1e1ed7a110b5164034a288cc67a0> | `python/sglang/srt/models/deepseek_nextn.py`（修复 2 行位置偏移）。 |
| 119c91c | **Skip signal handler registration when not on main thread** | <https://github.com/sgl-project/sglang/commit/119c91cb8ba7db60f473ea0d773e5500ce07ba83> | `python/sglang/srt/entrypoints/engine.py`（新增 23 行，避免子线程注册信号处理）。 |
| 88ad3b8 | **[sgl-kernel][Feat][B200][2/N] Support MXFP8 Grouped GEMM in Blackwell** | <https://github.com/sgl-project/sglang/commit/88ad3b894a1b82454a1f6f15019b7846dff809ef> | `sgl-kernel/csrc/expert_specialization/es_sm100_mxfp8_blockscaled_group_quant.cuh`（新增 6 行实现）。 |
| b3202fe | **[PCG] fix piecewise cuda graph for Qwen3.5** | <https://github.com/sgl-project/sglang/commit/b3202fe6d072c6afdfd8a567977d84de17c1d50d> | `python/sglang/srt/layers/quantization/fp8_utils.py`（新增 7 行工具函数）。 |
| a0a8f14 | **[Benchmark] Fix generated_shared_prefix attribute naming and remove args dependency** | <https://github.com/sgl-project/sglang/commit/a0a8f1473c07921fca0469d922a8c745fb79419a> | `python/sglang/benchmark/datasets/generated_shared_prefix.py`（重命名属性，新增 41 行，删除 26 行）。 |
| 6e82183 | **[Disagg] Route disagg prefill results through `process_batch_result`** | <https://github.com/sgl-project/sglang/commit/6e82183f5a28ca6a7f4dc0db4075c63d2dd6e23e> | `python/sglang/srt/disaggregation/prefill.py`（新增 3 行调用 `process_batch_result`）。 |
| 914ed34 | **update jit_kernel codeowners** | <https://github.com/sgl-project/sglang/commit/914ed3475786f8d3d52c2d4be566781060682a75> | `.github/CODEOWNERS`（微调 1 行）。 |
| 265eb56 | **Support multi-step alignment and pipeline integration in dump comparator** | <https://github.com/sgl-project/sglang/commit/265eb56d44b2a50dd201f38f08d6223701f63893> | `python/sglang/srt/debug_utils/comparator/aligner/token_aligner/executor.py`（新增 25 行执行逻辑）。 |
| 4e843f1 | **[DeepSeek-V3.2][JIT-kernel] Support nsa fuse store indexer k cache** | <https://github.com/sgl-project/sglang/commit/4e843f12165705b036718ecce8cdfe9d4dce1691> | `python/sglang/jit_kernel/fused_store_index_cache.py`（新增 103 行 JIT 接口）。 |
| f230967 | **[AMD] Fix ROCm Docker builds, update apache-tvm-ffi** | <https://github.com/sgl-project/sglang/commit/f230967e651b38550342d6214dd60370047ea987> | `docker/rocm.Dockerfile`（微调 1 行）。 |
| f9a2f03 | **Support token aligner planning and execution in dump comparator** | <https://github.com/sgl-project/sglang/commit/f9a2f0398f8d2fa07f29d4cf17b12e1e6e87e7a9> | `python/sglang/srt/debug_utils/comparator/aligner/token_aligner/planner.py`（新增 130 行规划逻辑）。 |
| d34d5ac | **Support loading token aligner data in dump comparator** | <https://github.com/sgl-project/sglang/commit/d34d5aca07502223b00683e10e2323e878133930> | `python/sglang/srt/debug_utils/comparator/aligner/token_aligner/aux_loader.py`（新增 224 行数据加载实现）。 |
| e8dd145 | **Add aligner entrypoint and bundle handler in dump comparator** | <https://github.com/sgl-project/sglang/commit/e8dd14519dcd0ced95f4b42ee5c7265ad157abcc> | `python/sglang/srt/debug_utils/comparator/bundle_comparator.py`（新增 105 行整体比较入口）。 |
| 2ad475b | **use flashinfer.sampling** | <https://github.com/sgl-project/sglang/commit/2ad475b4edfed009701737d199b9410540af63d7> | `python/sglang/srt/layers/sampler.py`（将采样实现切换到 `flashinfer.sampling`，改动 4 行）。 |
| 2739d7d | **Reorganize modules and pipeline in dump comparator** | <https://github.com/sgl-project/sglang/commit/2739d7df622c929d587239acbf11a6db2238cde6> | `python/sglang/srt/debug_utils/comparator/pipeline.py`（重构 58 行，删除 34 行旧实现）。 |

> **说明**：部分提交的 diff 被截断（`patch_truncated: true`），因此仅提供了关键文件和概览。若需完整改动，请访问对应 PR 链接。

**Issues**

| 标题 | 链接 | 简要说明 |
|------|------|----------|
| **[Bug] [Diffusion] FLUX.2-klein-9B image editing produces corrupted output with multi-GPU (`--num-gpus 8`)** | <https://github.com/sgl-project/sglang/issues/19449> | 多 GPU 环境下图像编辑输出异常，单卡正常。 |
| **[Feature] Sglang FA4 Refactor** | <https://github.com/sgl-project/sglang/issues/19447> | 讨论 FlashAttention 4 重构方案及兼容性。 |
| **[Feature] ModelOpt to CompressedTensors bridge** | <https://github.com/sgl-project/sglang/issues/19444> | 设计将 ModelOpt 量化模型桥接到 CompressedTensors 的路线图。 |
| **[Question] Offload or Release?** | <https://github.com/sgl-project/sglang/issues/19442> | 询问 `release_memory_occupation` 与 `offload` 的区别及实现细节。 |
| **[Question] Still have 5GB can not offload in SGLang diffusion** | <https://github.com/sgl-project/sglang/issues/19441> | 在 Diffusion 模型中仍有约 5GB GPU 内存无法释放的疑问。 |
| **[Bug] KeyError: 'model.language_model.layers.8.mlp.down_proj.weight' on finetuned qwen2-vl-7b** | <https://github.com/sgl-project/sglang/issues/19438> | 加载微调模型时报错，涉及权重键缺失。 |
| **[Feature] Helix (TP + CP) Parallelism: A2A Communication + Decoupled Attention/FFN Parallelism** | <https://github.com/sgl-project/sglang/issues/19436> | 提议在 Helix 框架中加入全互联通信与解耦注意力/前馈并行。 |
| **[Bug] ValueError: input_ids should be a list of lists for batch processing.** | <https://github.com/sgl-project/sglang/issues/19435> | 批处理输入格式错误导致异常。 |

> **说明**：所有 Issue 均提供了标题、链接和简要描述，若 Issue 内容缺失（如 body 被截断），已在表中注明。
### vllm-project/vllm
## 提交

**仓库**: [vllm-project/vllm](https://github.com/vllm-project/vllm)

### 核心功能与优化

- **ROCm 平台支持增强**
  - 新增对 GPT OSS 上游 MoE 模型的 wmxfp4_afp8 静态缩放量化支持，主要修改了 `vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py` 等文件。([01923ee](https://github.com/vllm-project/vllm/commit/01923eec7092fd5b718cb9b45eb6df152abe9296))
  - 为 DeepSeek V2 投影层添加动态 MXFP4 量化支持。([ec8ab9d](https://github.com/vllm-project/vllm/commit/ec8ab9d254d3b2e6b919a55277da599a7b9ab146))
  - 更新 ROCm 构建所需的 PyTorch 版本至官方 2.10 发布版。([9e2cabd](https://github.com/vllm-project/vllm/commit/9e2cabdf9c86e9fceca8842c8ea2a260281c31e8))

- **性能优化**
  - 优化 NCCL 对称内存与自定义 AllReduce (custom_AR) 的选择阈值，提升分布式训练效率。([31fb6f4](https://github.com/vllm-project/vllm/commit/31fb6f43dac735369851c1d908d3d6ed5d6dc1c2))
  - 将 KV Cache 更新操作从 FlashInfer 前向计算中分离出来以提高性能。([98217b0](https://github.com/vllm-project/vllm/commit/98217b09f9ce22429ce35badfa1d50e1f4fe4137))
  - 优化 Pooling 模型中的 MaxSim 分数计算逻辑，端到端吞吐量提升了 13.9%。([99c7892](https://github.com/vllm-project/vllm/commit/99c7892c5bf20afc90e2ef0e1ad0a89637ae67a9))

- **模型支持扩展**
  - 添加对 NVIDIA Llama-Nemotron 多模态嵌入模型的支持，并提供相应测试和文档说明。([111d869](https://github.com/vllm-project/vllm/commit/111d8690699927af686fa6750cfbbc692a1f8740))
  - 在 NemotronHMLPDecoderLayer 中启用每层配置，以支持异构模型结构。([832a780](https://github.com/vllm-project/vllm/commit/832a780f3aed332287203217d0d946b8b03299b4))
  - 支持在推测解码（speculative decoding）场景下使用 `min_tokens` 参数控制最小生成长度。([d940607](https://github.com/vllm-project/vllm/commit/d940607629b03602f34ba4dd75c747162b01aedd))

- **推理与解析器改进**
  - 修复 Qwen3 Reasoning Parser 返回截断输出的问题。([967572d](https://github.com/vllm-project/vllm/commit/967572dd5f8da947aa4344f0e75516b6ee0ede9b))
  - 修正 MiniMax 2.1 工具调用后缺少 `</function>` 标签的 Bug。([7fea725](https://github.com/vllm-project/vllm/commit/7fea7250a46c88c1ba9684d7774d2c4ac17c4b90))

### Bug 修复

- 修复 MLA 模型加载 KV Scale 时出现的错误。([6283021](https://github.com/vllm-project/vllm/commit/6283021142bbf5ee324395dad5e80b8661400329))
- 解决跨节点数据并行时 MessageQueue 连接 IP 地址不正确的问题。([0f2f24c](https://github.com/vllm-project/vllm/commit/0f2f24c8b205b5bf2dadacf1f95f1ad9f7de73e0))
- 移除 WideEP 中不再使用的 pplx all2all 后端实现及相关代码。([eb19955](https://github.com/vllm-project/vllm/commit/eb19955c37089056883831838dc155340ae67edd))
- 修复 LoRA 词汇表大小约束检查中的边界条件问题。([5e58bdc](https://github.com/vllm-project/vllm/commit/5e58bdc7113a2c62a9bfb71304d0d1563b0da7f3))
- 修正 fused MoE-LoRA 内核配置未对齐实际权重形状的问题。([a1f53ad](https://github.com/vllm-project/vllm/commit/a1f53addb132f75704710184f4c1cc4780343329))
- 修复 KV Offload 内核块大小检测逻辑。([f2ad952](https://github.com/vllm-project/vllm/commit/f2ad952f40a98e0bb7f89763c51a73124ccc20a6))
- 解决 Mamba Selective Scan 状态指针运算中的 uint32 溢出问题。([ec13e54](https://github.com/vllm-project/vllm/commit/ec13e549d3e1de13d05af759cc8bef3f7cf5e318))
- 修复 Qwen2.5-Omni 和 Qwen3-Omni 混合模态嵌入回归问题。([c0615a2](https://github.com/vllm-project/vllm/commit/c0615a296d44ce1963d795ea65dcff6172b4ae8d))
- 修正 Qwen3.5 FP8 量化过程中元组 shard_id 权重加载失败的情况。([32693db](https://github.com/vllm-project/vllm/commit/32693db8cea5cb9099c4e9d9876def97fdbc5387))

### 架构重构与清理

- 移除了已废弃或重复的功能工具函数及变量，精简代码库。([05972ea](https://github.com/vllm-project/vllm/commit/05972ea7e5f81250cc4ceaae8a174cfffe7755ac))
- 清理了注意力基准测试脚本中的无用代码。([05970c7](https://github.com/vllm-project/vllm/commit/05970c772c1ca32be058d4cccfbb12aaf2032d70))
- 删除了 `bc-lint` 相关组件。([0191444](https://github.com/vllm-project/vllm/commit/01914445b0513ab355b1275acec2f2e5da4d91d6))

### 其他更新

- 引入混合精度支持至 ModelOpt 量化框架。([d0105b8](https://github.com/vllm-project/vllm/commit/d0105b84f00fadc18d9e7859d3e76887b7f5772c))
- 统一处理多模态处理器参数中的 `size` 字段。([845ee34](https://github.com/vllm-project/vllm/commit/845ee348ef82d12b5d106384070f7578c843d3cd))
- XPU Dockerfile 使用固定版本的 UMD。([5281713](https://github.com/vllm-project/vllm/commit/5281713e1119d6312dba2e4d0a95a517dbc24b06))
- 在 PowerPC 架构上启用前缀缓存和分块预填充功能。([e03ddcf](https://github.com/vllm-project/vllm/commit/e03ddcfbd4d686c91f6509c5451546437bbbf3e5))

---

## Issues

### 新功能请求

- 请求为 Blackwell SM100+ 架构增加 W4A8 (compressed-tensors) 内核支持。当前发布的二进制包无法在 NVIDIA B200 GPU 上运行该量化方案。([#35439](https://github.com/vllm-project/vllm/issues/35439))

### Bug 报告

- 当使用无效的 `response_format`（如 json_schema 类型但缺少完整 schema 定义）时，API 应返回 HTTP 400 错误而非 500 内部服务器错误。([#35438](https://github.com/vllm-project/vllm/issues/35438))
- 在 CUDA 13.0 和 PyTorch 2.10 环境下的 CI 测试中发现 LoRA 词汇表大小限制导致 Mixtral LoRA 测试失败。([#35437](https://github.com/vllm-project/vllm/issues/35437))
- 预编译的 vLLM wheel 包和官方镜像在 RTX 50 系列（Blackwell, SM120/SM121）显卡上因缺少对应架构支持而崩溃。([#35432](https://github.com/vllm-project/vllm/issues/35432))
- 使用流水线并行（PP）时，AllReduceRMSFusionPass 导致 NCCL 错误。([#35426](https://github.com/vllm-project/vllm/issues/35426))
- Qwen3-VL-Reranker 模型产生的相关性评分与原生 Transformers 实现存在显著差异。([#35412](https://github.com/vllm-project/vllm/issues/35412))

### 其他反馈

- 社区成员建议发布 macOS 平台的 wheel 包以便于本地开发和部署。([#35419](https://github.com/vllm-project/vllm/issues/35419))
- 用户报告在四张 2080Ti 显卡上部署 Qwen3.5-35B-A3B 模型失败，原因是旧硬件不支持 BF16 数据类型。([#35414](https://github.com/vllm-project/vllm/issues/35414))
### NVIDIA/cutile-python
昨日无更新。

## 总结
- Diff 内容已截断以满足 prompt 预算。
- Issue 内容已截断以满足 prompt 预算。
