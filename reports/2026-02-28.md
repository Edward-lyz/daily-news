今日的 AI Infra 的新闻如下。

## 摘要
模型摘要不可用，以下为原始数据整理。

## 具体内容分析
### deepseek-ai/DeepGEMM
昨日无更新。
### deepseek-ai/FlashMLA
昨日无更新。
### NVIDIA/cutlass
# NVIDIA/cutlass 更新摘要 (2026-02-28)

## 提交
- **printf 函数增强** ([b984769](https://github.com/NVIDIA/cutlass/commit/b9847690c5838ac3d909ebc163ed16c388802485))
  - 作者: drazi
  - 修改文件: `python/CuTeDSL/cutlass/cute/core.py`
  - 更新了 `printf` 函数，添加了 `end` 参数，允许自定义输出结束符：
    ```python
    def printf(*args, loc=None, ip=None, end="\n") -> None:
    ```
  - 默认行为仍为输出换行符，但用户现在可以指定其他结束符

## 问题
无新问题报告。
### flashinfer-ai/flashinfer
提交：
- chore: Update CODEOWNERS (#2286) [`f521fe1`](https://github.com/flashinfer-ai/flashinfer/commit/f521fe19ac387e8baffd7b5c925ef59d9f2ecc0c)
- 受影响文件: .github/CODEOWNERS
Issues：
- [Feature Request] Support for Ascend NPU (https://github.com/flashinfer-ai/flashinfer/issues/2658)
- Issue 内容已截断。
- BF16 hidden_states for trtllm_fp4_block_scale_moe (https://github.com/flashinfer-ai/flashinfer/issues/2657)
- Issue 内容已截断。
- [Feature]: BF16 MLA decode support on SM120/SM121 (consumer Blackwell) (https://github.com/flashinfer-ai/flashinfer/issues/2655)
- Issue 内容已截断。
### Dao-AILab/flash-attention
## 提交

**flash-attention 仓库在 2026 年 2 月 28 日至 3 月 1 日的提交主要围绕 SM100 架构下的功能增强和 Bug 修复，特别是对 2CTA（双 CTA）指令的支持优化。**

### 核心变更

- **2CTA 支持增强**
  - 在 SM100 前向传播中为 `hdim128` 非因果场景启用 2CTA 指令 ([`9aadb8b`](https://github.com/Dao-AILab/flash-attention/commit/9aadb8bdc609775e6a8991c79ef47ac9f7944c3e))。
  - 在 SM100 反向传播中修复并启用了 `hdim128` 的 2CTA 路径 ([`a00ddeb`](https://github.com/Dao-AILab/flash-attention/commit/a00ddeb2b8f8a68b2e03b7bee5980e819069cdd1))。

- **调度器与流水线改进**
  - 修改了 `SingleTileScheduler` 以使用 `block_idx()`，暂时绕过集群索引逻辑 ([`d146eff`](https://github.com/Dao-AILab/flash-attention/commit/d146efff6f3226f465f1b4f089eaefe52c475e9c))。
  - 为 SM100 前向传播添加了 `pipeline.producer_tail` 调用，确保生产者阶段正确结束 ([`b936061`](https://github.com/Dao-AILab/flash-attention/commit/b936061f497beb2ccdd7869b429ed93476975f3f))。
  - 在 SM100 前向传播中，通过 `is_leader_cta` 标志门控 MMA 计算，避免非领导者 CTA 执行冗余计算 ([`58d0c57`](https://github.com/Dao-AILab/flash-attention/commit/58d0c57c689b0aa6015ad4d227054670881c9f19))。

- **Bug 修复**
  - 修复了 SM100 前向传播中因缺少 `tSrQs` 初始化导致的回归问题 ([`6d36c1c`](https://github.com/Dao-AILab/flash-attention/commit/6d36c1c6d7140c1d263aec36523993accd9a4a0a))。
  - 更新依赖项 `nvidia-cutlass-dsl` 至版本 `4.4.1` 以避免段错误 ([`7ed0898`](https://github.com/Dao-AILab/flash-attention/commit/7ed0898caa50a8f3fbb4f7212a437479bd59dfc1))。

- **代码结构调整**
  - 调整了 SM100 前向传播中的张量布局 (`cta_tiler`, `mma_tiler`) 以更好地适配 2CTA ([`9aadb8b`](https://github.com/Dao-AILab/flash-attention/commit/9aadb8bdc609775e6a8991c79ef47ac9f7944c3e), [`01bc8ef`](https://github.com/Dao-AILab/flash-attention/commit/01bc8ef60d3237395743bb6e807200f27df9af6d))。
  - 将 `q_stage` 参数传递给 SM100 前向传播的流水线和调度器 ([`d1d3e8d`](https://github.com/Dao-AILab/flash-attention/commit/d1d3e8dc3554df8112f2c14e8f3e2fa3e3cb25d6), [`9aadb8b`](https://github.com/Dao-AILab/flash-attention/commit/9aadb8bdc609775e6a8991c79ef47ac9f7944c3e))。

### 涉及文件

- `flash_attn/cute/flash_fwd_sm100.py`: 实现 SM100 前向传播的核心逻辑，包括 2CTA 启用、流水线和调度器调用。
- `flash_attn/cute/flash_bwd_sm100.py`: 实现 SM100 反向传播的核心逻辑，包含 2CTA 修复。
- `flash_attn/cute/tile_scheduler.py`: 定义块调度策略。
- `flash_attn/cute/interface.py`: 用户接口层，决定是否启用 2CTA 等特性。
- `flash_attn/cute/pyproject.toml`: 项目依赖配置文件。

---

## Issues

1. **安装失败**: 用户报告在 Windows 11 上使用 Python 3.12 和 CUDA 12.1 安装 `flash-attn` 失败，提示构建 Wheel 错误 ([#2288](https://github.com/Dao-AILab/flash-attention/issues/2288))。该 Issue 缺少完整的错误日志 (`body_truncated` 为 true)，难以定位具体原因。

2. **功能请求**: 请求在 B300 GPU 上为滑动窗口且头维度为 192 的场景支持 2CTA ([#2286](https://github.com/Dao-AILab/flash-attention/issues/2286))。当前实现似乎未覆盖此特定组合。
### sgl-project/sglang
提交：
- [Test] add unit test for skipping already preempted request (#18912) [`8a0b757`](https://github.com/sgl-project/sglang/commit/8a0b7575b0a2e2cf9e5bd53990fc2546e3fe7440)
- 受影响文件: test/registered/scheduler/test_prefill_adder.py
- [Bugfix] Add missing auto_create_handle_loop to communicator methods (#19610) [`98224de`](https://github.com/sgl-project/sglang/commit/98224de29b78ed92374e1e3db110667226b5487e)
- 受影响文件: python/sglang/srt/managers/tokenizer_communicator_mixin.py
- [fix typo] seperated_timestep -> separated_timestep (#19622) [`0b3ddbc`](https://github.com/sgl-project/sglang/commit/0b3ddbcf105b01b205534ce30de0c99da6884b2d)
- 受影响文件: python/sglang/multimodal_gen/configs/models/dits/mova_audio.py, python/sglang/multimodal_gen/configs/models/dits/mova_video.py, python/sglang/multimodal_gen/runtime/models/dits/mova_audio_dit.py, python/sglang/multimodal_gen/runtime/models/dits/mova_video_dit.py
- [HiCache] Re-land spec v2 + decode KV cache offloading compatibility (#19615) [`dc02e5b`](https://github.com/sgl-project/sglang/commit/dc02e5bea76a9c0e91229e333e9895a54657dc62)
- 受影响文件: python/sglang/srt/disaggregation/decode_kvcache_offload_manager.py, python/sglang/srt/server_args.py, test/registered/disaggregation/test_specv2_kvcache_offloading.py
- [RL] Support per-layer mixed FP8/BF16 serving for FP8 checkpoints (#18742) [`0e86977`](https://github.com/sgl-project/sglang/commit/0e869778111529721ee68e2be64f1c224c1799d3)
- 受影响文件: python/sglang/srt/layers/quantization/fp8.py, python/sglang/srt/layers/quantization/utils.py
- [diffusion] CI: create and refactor UT (#19619) [`a75840b`](https://github.com/sgl-project/sglang/commit/a75840b37339869c8ec7edafdf952a4f616bf573)
- 受影响文件: python/sglang/multimodal_gen/test/run_suite.py, python/sglang/multimodal_gen/test/unit/test_lora_format_adapter.py, python/sglang/multimodal_gen/test/unit/test_sampling_params_validate.py, python/sglang/multimodal_gen/test/unit/test_storage.py
- [Perf] Optimize NSA backend metadata under MTP (#19536) [`80a6b32`](https://github.com/sgl-project/sglang/commit/80a6b32703db7f0fe1ef69fa9b5e2154f3e51258)
- 受影响文件: python/sglang/srt/layers/attention/nsa_backend.py, python/sglang/srt/layers/attention/utils.py
- [diffusion] add .claude and update contributing with attitude towards vibe-pr (#19511) [`d098c8d`](https://github.com/sgl-project/sglang/commit/d098c8dab0941b1b5d350c74c192df62f6dfe1ae)
- 受影响文件: docs/diffusion/contributing.md, python/sglang/multimodal_gen/.claude/.skills/diffusion-perf/SKILL.md, python/sglang/multimodal_gen/.claude/CLAUDE.md
- [AMD] Fix MoRI EP warmup hang by restoring deepep_mode=normal default (#19498) [`5fa6633`](https://github.com/sgl-project/sglang/commit/5fa66334858a5e8f4596ea9618e140e010008d95)
- 受影响文件: python/sglang/srt/server_args.py
- Revert "[HiCache] Enable spec v2 + decode KV cache offloading compatibility" (#19613) [`dcf462c`](https://github.com/sgl-project/sglang/commit/dcf462cfbaae7557f213ce2baceaebc5c61c2ccd)
- 受影响文件: python/sglang/srt/disaggregation/decode_kvcache_offload_manager.py, python/sglang/srt/server_args.py, test/registered/disaggregation/test_specv2_kvcache_offloading.py
- [HiCache] Enable spec v2 + decode KV cache offloading compatibility (#19518) [`8167346`](https://github.com/sgl-project/sglang/commit/8167346609b6a30d60db90cfea3bfa9a89c2213d)
- 受影响文件: python/sglang/srt/disaggregation/decode_kvcache_offload_manager.py, python/sglang/srt/server_args.py, test/registered/disaggregation/test_specv2_kvcache_offloading.py
- [Blackwell] Make mxint4 flashinfer_trtllm moe gemm set by default on blackwell (#18136) [`894e887`](https://github.com/sgl-project/sglang/commit/894e887e4a0dab6f74509abcbb0db0c9ab0f8a4f)
- 受影响文件: python/sglang/srt/server_args.py
- [Bugfix] Fix KeyError: 'prompt_tokens' when streaming requests are aborted (#19514) [`38dc372`](https://github.com/sgl-project/sglang/commit/38dc372dae00fceb372e0cca75312c136f156713)
- 受影响文件: python/sglang/srt/entrypoints/openai/serving_chat.py, python/sglang/srt/entrypoints/openai/serving_completions.py, python/sglang/srt/entrypoints/openai/usage_processor.py
- [diffusion][MOVA] fix: fix task type in MOVA pipeline and shared model placement (#19489) [`4ec450e`](https://github.com/sgl-project/sglang/commit/4ec450e97b4a592b279e38434ff0c93043cc3e34)
- 受影响文件: python/sglang/multimodal_gen/configs/pipeline_configs/mova.py, python/sglang/multimodal_gen/configs/sample/mova.py, python/sglang/multimodal_gen/runtime/models/bridges/mova_dual_tower.py, python/sglang/multimodal_gen/runtime/pipelines_core/stages/model_specific_stages/mova.py
- [Session] Extract `SessionController` and clean up session logic in `Scheduler` (#19547) [`5acb45c`](https://github.com/sgl-project/sglang/commit/5acb45cf32a6344b008d7850bbc27be303cb577b)
- 受影响文件: python/sglang/srt/managers/scheduler.py, python/sglang/srt/managers/session_controller.py
- Revert "[SGL] sync patch: Remove sync points, prefill cudagraph for DP, disable cache reset in mem check (#19190)" (#19581) [`a45613f`](https://github.com/sgl-project/sglang/commit/a45613f2a650f4545f12ceebfdca7b1452386a27)
- 受影响文件: python/sglang/srt/batch_overlap/two_batch_overlap.py, python/sglang/srt/connector/__init__.py, python/sglang/srt/layers/logits_processor.py, python/sglang/srt/managers/schedule_batch.py, python/sglang/srt/mem_cache/memory_pool.py, python/sglang/srt/model_executor/forward_batch_info.py, python/sglang/srt/model_executor/model_runner.py
- Support data parallel attention in dump comparator (#19602) [`e64095c`](https://github.com/sgl-project/sglang/commit/e64095c3c7e4bb299283bc1bade68dcbf54d2df1)
- 受影响文件: python/sglang/srt/debug_utils/comparator/aligner/axis_aligner.py, python/sglang/srt/debug_utils/comparator/aligner/axis_swapper.py, python/sglang/srt/debug_utils/comparator/aligner/entrypoint/planner.py, python/sglang/srt/debug_utils/comparator/bundle_comparator.py, python/sglang/srt/debug_utils/comparator/dims.py, python/sglang/srt/debug_utils/comparator/dp_utils.py, python/sglang/srt/debug_utils/comparator/entrypoint.py, python/sglang/srt/debug_utils/dumper.py, test/registered/debug_utils/comparator/aligner/reorderer/test_planner.py, test/registered/debug_utils/comparator/aligner/unsharder/test_executor.py, test/registered/debug_utils/comparator/aligner/unsharder/test_planner.py, test/registered/debug_utils/comparator/test_dims.py, test/registered/debug_utils/comparator/test_dp_utils.py, test/registered/debug_utils/comparator/test_entrypoint.py, test/registered/debug_utils/comparator/test_model_validation.py, test/registered/debug_utils/source_patcher/test_code_patcher.py
- 文件列表已截断。
- Support multi sharding group on the same dimension in dump comparator (#19601) [`ea6ff7b`](https://github.com/sgl-project/sglang/commit/ea6ff7b01fa34071562dfe7bbd527708b3472371)
- 受影响文件: python/sglang/srt/debug_utils/comparator/aligner/reorderer/planner.py, python/sglang/srt/debug_utils/comparator/aligner/token_aligner/smart/aux_plugins.py, python/sglang/srt/debug_utils/comparator/aligner/unsharder/planner.py, python/sglang/srt/debug_utils/comparator/dims.py, python/sglang/srt/debug_utils/comparator/entrypoint.py, test/registered/debug_utils/comparator/aligner/entrypoint/test_planner.py, test/registered/debug_utils/comparator/aligner/reorderer/test_planner.py, test/registered/debug_utils/comparator/aligner/token_aligner/test_aux_loader.py, test/registered/debug_utils/comparator/aligner/token_aligner/test_aux_plugins.py, test/registered/debug_utils/comparator/aligner/unsharder/test_executor.py, test/registered/debug_utils/comparator/aligner/unsharder/test_planner.py, test/registered/debug_utils/comparator/test_dims.py, test/registered/debug_utils/comparator/test_entrypoint.py, test/registered/debug_utils/test_dumper.py
- Add skip patterns, tee to file, tensor load warning in dump comparator (#19600) [`46960e6`](https://github.com/sgl-project/sglang/commit/46960e65cfbafe7bb6ae10745e9397ef1e2164cf)
- 受影响文件: python/sglang/srt/debug_utils/comparator/bundle_comparator.py, python/sglang/srt/debug_utils/comparator/display.py, python/sglang/srt/debug_utils/comparator/entrypoint.py, python/sglang/srt/debug_utils/comparator/output_types.py, python/sglang/srt/debug_utils/comparator/warning_sink.py, test/registered/debug_utils/comparator/conftest.py, test/registered/debug_utils/comparator/test_bundle_comparator.py, test/registered/debug_utils/comparator/test_entrypoint.py, test/registered/debug_utils/comparator/test_warning_sink.py, test/registered/debug_utils/test_engine_dumper_comparator_e2e.py
- Support concat mode in token aligner in dump comparator (#19599) [`b0b26a7`](https://github.com/sgl-project/sglang/commit/b0b26a7ef10bf262b05fc1ab16ac41f72927f888)
- 受影响文件: python/sglang/srt/debug_utils/comparator/aligner/entrypoint/executor.py, python/sglang/srt/debug_utils/comparator/aligner/entrypoint/planner.py, python/sglang/srt/debug_utils/comparator/aligner/entrypoint/types.py, python/sglang/srt/debug_utils/comparator/aligner/token_aligner/concat_steps/__init__.py, python/sglang/srt/debug_utils/comparator/aligner/token_aligner/concat_steps/executor.py, python/sglang/srt/debug_utils/comparator/aligner/token_aligner/concat_steps/thd_seq_lens_loader.py, python/sglang/srt/debug_utils/comparator/aligner/token_aligner/entrypoint.py, python/sglang/srt/debug_utils/comparator/aligner/token_aligner/smart/__init__.py, python/sglang/srt/debug_utils/comparator/aligner/token_aligner/smart/aux_loader.py, python/sglang/srt/debug_utils/comparator/aligner/token_aligner/smart/aux_plugins.py, python/sglang/srt/debug_utils/comparator/aligner/token_aligner/smart/executor.py, python/sglang/srt/debug_utils/comparator/aligner/token_aligner/smart/planner.py, python/sglang/srt/debug_utils/comparator/aligner/token_aligner/smart/seq_info_builder.py, python/sglang/srt/debug_utils/comparator/aligner/token_aligner/smart/types.py, python/sglang/srt/debug_utils/comparator/bundle_comparator.py, python/sglang/srt/debug_utils/comparator/entrypoint.py
- 文件列表已截断。
- Support overriding and post-hoc providing metadata in dump comparator (#19598) [`e78f128`](https://github.com/sgl-project/sglang/commit/e78f1283f77804820053702b58aa43b0306d7afe)
- 受影响文件: python/sglang/srt/debug_utils/comparator/bundle_comparator.py, python/sglang/srt/debug_utils/comparator/entrypoint.py, python/sglang/srt/debug_utils/comparator/meta_overrider.py, test/registered/debug_utils/comparator/test_entrypoint.py, test/registered/debug_utils/comparator/test_meta_overrider.py
- Enhance replicated tensor checker in dump comparator (#19597) [`e41164a`](https://github.com/sgl-project/sglang/commit/e41164af1c84a72c5e6e904db5efa945636f3d29)
- 受影响文件: python/sglang/srt/debug_utils/comparator/aligner/entrypoint/executor.py, python/sglang/srt/debug_utils/comparator/aligner/token_aligner/aux_loader.py, python/sglang/srt/debug_utils/comparator/aligner/unsharder/executor.py, python/sglang/srt/debug_utils/comparator/bundle_comparator.py, python/sglang/srt/debug_utils/comparator/output_types.py, python/sglang/srt/debug_utils/comparator/tensor_comparator/comparator.py, python/sglang/srt/debug_utils/comparator/tensor_comparator/formatter.py, test/registered/debug_utils/comparator/aligner/entrypoint/test_executor.py, test/registered/debug_utils/comparator/aligner/reorderer/test_executor.py, test/registered/debug_utils/comparator/aligner/reorderer/test_planner.py, test/registered/debug_utils/comparator/aligner/unsharder/test_executor.py, test/registered/debug_utils/comparator/tensor_comparator/test_comparator.py, test/registered/debug_utils/comparator/tensor_comparator/test_types.py, test/registered/debug_utils/comparator/test_entrypoint.py, test/registered/debug_utils/comparator/test_warning_sink.py
- Support data parallel in dump comparator (#19596) [`ec08240`](https://github.com/sgl-project/sglang/commit/ec08240a6ace9a6cedbe510581ccd00eb09d78a3)
- 受影响文件: python/sglang/srt/debug_utils/comparator/aligner/token_aligner/aux_loader.py, python/sglang/srt/debug_utils/comparator/bundle_comparator.py, python/sglang/srt/debug_utils/comparator/dp_utils.py, test/registered/debug_utils/comparator/aligner/token_aligner/test_aux_loader.py, test/registered/debug_utils/comparator/test_dp_utils.py, test/registered/debug_utils/comparator/test_entrypoint.py
- Support partial tensors waiting for reduction and pipeline parallel in dump comparator (#19595) [`003ad6d`](https://github.com/sgl-project/sglang/commit/003ad6daaad7c808bcc53dbf40a82d6f8652a54e)
- 受影响文件: python/sglang/srt/debug_utils/comparator/aligner/unsharder/executor.py, python/sglang/srt/debug_utils/comparator/aligner/unsharder/planner.py, python/sglang/srt/debug_utils/comparator/aligner/unsharder/types.py, python/sglang/srt/debug_utils/source_patcher/code_patcher.py, test/registered/debug_utils/comparator/aligner/unsharder/test_executor.py, test/registered/debug_utils/comparator/aligner/unsharder/test_planner.py, test/registered/debug_utils/comparator/test_bundle_matcher.py, test/registered/debug_utils/comparator/test_entrypoint.py
- Visualize per-token information in dump comparator (#19594) [`6781082`](https://github.com/sgl-project/sglang/commit/67810828cf6014ce0586272633931aeba53eb980)
- 受影响文件: python/sglang/srt/debug_utils/comparator/bundle_comparator.py, python/sglang/srt/debug_utils/comparator/entrypoint.py, python/sglang/srt/debug_utils/comparator/per_token_visualizer.py, python/sglang/srt/debug_utils/comparator/tensor_comparator/comparator.py, python/sglang/srt/debug_utils/comparator/tensor_comparator/types.py, python/sglang/srt/debug_utils/comparator/utils.py, test/registered/debug_utils/comparator/tensor_comparator/test_comparator.py, test/registered/debug_utils/comparator/test_entrypoint.py, test/registered/debug_utils/comparator/test_manually_verify.py, test/registered/debug_utils/comparator/test_per_token_visualizer.py, test/registered/debug_utils/comparator/test_utils.py
- Support arbitrary filtering in dumper (#19593) [`f5a10e0`](https://github.com/sgl-project/sglang/commit/f5a10e04cd38a49d7c8b9c8309ad368de1be2cf2)
- 受影响文件: python/sglang/srt/debug_utils/dumper.py, test/registered/debug_utils/test_dumper.py
- [AMD] MORI-EP support for EP4. (#19578) [`8240a87`](https://github.com/sgl-project/sglang/commit/8240a8730624c627e7908534c00666039bf50624)
- 受影响文件: docker/rocm.Dockerfile, python/sglang/srt/layers/moe/token_dispatcher/moriep.py
- [AMD] Remove Redundant tvm-ffi Installation in amd_ci_install_dependency.sh (#19554) [`560b867`](https://github.com/sgl-project/sglang/commit/560b867ccea9f03bdf754591e17f8c1d214a22eb)
- 受影响文件: scripts/ci/amd/amd_ci_install_dependency.sh
- [Fix] Add --disable-draft-model-update to control draft model updates(especially in RL) (#15726) [`f451664`](https://github.com/sgl-project/sglang/commit/f4516645041137d4ee826fdda7efc05b8fba4819)
- 受影响文件: python/sglang/srt/managers/io_struct.py, python/sglang/srt/managers/scheduler_update_weights_mixin.py
- [Anthropic API] Preserve image content in `tool_result` conversion (#19233) [`9c81ce4`](https://github.com/sgl-project/sglang/commit/9c81ce4707927c5a18cbac82dd70c3769ad269be)
- 受影响文件: python/sglang/srt/entrypoints/anthropic/serving.py, test/registered/openai_server/basic/test_anthropic_server.py
- [RadixTree][6/N Refactor]: Refactor SWARadixTree to simplify the computation and alignment of bigram keys. (#19427) [`a0d8a7a`](https://github.com/sgl-project/sglang/commit/a0d8a7ae6dd23001a05da073424703c579444cf0)
- 受影响文件: python/sglang/srt/mem_cache/radix_cache.py, python/sglang/srt/mem_cache/swa_radix_cache.py, test/registered/radix_cache/test_swa_unittest.py
- Support singleton dimension squeezing in dump comparator (#19566) [`5705e02`](https://github.com/sgl-project/sglang/commit/5705e02d286e59863b54935a415735f173e29361)
- 受影响文件: python/sglang/srt/debug_utils/comparator/aligner/axis_aligner.py, python/sglang/srt/debug_utils/comparator/aligner/entrypoint/executor.py, python/sglang/srt/debug_utils/comparator/aligner/entrypoint/planner.py, python/sglang/srt/debug_utils/comparator/aligner/entrypoint/types.py, python/sglang/srt/debug_utils/comparator/aligner/token_aligner/aux_loader.py, python/sglang/srt/debug_utils/comparator/bundle_comparator.py, python/sglang/srt/debug_utils/comparator/dims.py, python/sglang/srt/debug_utils/comparator/output_types.py, test/registered/debug_utils/comparator/aligner/test_axis_aligner.py, test/registered/debug_utils/comparator/aligner/unsharder/test_executor.py, test/registered/debug_utils/comparator/test_dims.py, test/registered/debug_utils/comparator/test_entrypoint.py, test/registered/debug_utils/comparator/test_model_validation.py, test/registered/debug_utils/source_patcher/test_code_patcher.py, test/registered/debug_utils/source_patcher/test_dumper_integration.py, test/registered/debug_utils/source_patcher/test_source_editor.py
Issues：
- [Bug] Qwen3.5 FP8 Downcasting not allowed: target.dtype=torch.float8_e4m3fn, loaded_weight.dtype=torch.bfloat16 (https://github.com/sgl-project/sglang/issues/19589)
- Issue 内容已截断。
- [Bug] [v32/glm5]Triton JIT on alloc_extend during prealloc causes ~15s TTFT delay on one EP rank (https://github.com/sgl-project/sglang/issues/19588)
- Issue 内容已截断。
- [Bug] txn545/Qwen3.5-27B-NVFP4 fails to load on latest main: missing input_scale param + shape assertion in qwen3_5.py (https://github.com/sgl-project/sglang/issues/19587)
- Issue 内容已截断。
- [Bug] FlashInfer sampling.lock contention with PP + CP: multiple ranks block on shared cache lock on first request (https://github.com/sgl-project/sglang/issues/19583)
- Issue 内容已截断。
- [Bug] many port have CLOSE_WAIT state with enable attention dp and PD disaggregation (https://github.com/sgl-project/sglang/issues/19576)
- Issue 内容已截断。
- [Bug] [CPU][AMX] error: ‘class at::vec::CPU_CAPABILITY::Vectorized<float>’ has no member named ‘fexp_u20’; did you mean ‘exp_u20’? (https://github.com/sgl-project/sglang/issues/19574)
- Issue 内容已截断。
- [Bug] DeepSeek v3.2 TBO RuntimeError: seqlens_k must have shape (batch_size) (https://github.com/sgl-project/sglang/issues/19553)
- Issue 内容已截断。
- Fix CPU CI issues (https://github.com/sgl-project/sglang/issues/19538)
### vllm-project/vllm
提交：
- [Attention] FA4 integration (#32974) [`8b5014d`](https://github.com/vllm-project/vllm/commit/8b5014d3dd343736ccf3e26cd44a0bb7700d205c)
- 受影响文件: .buildkite/test_areas/misc.yaml, .gitignore, cmake/external_projects/vllm_flash_attn.cmake, docs/design/attention_backends.md, requirements/cuda.txt, setup.py, tools/pre_commit/generate_attention_backend_docs.py, vllm/config/attention.py, vllm/model_executor/layers/attention/mla_attention.py, vllm/model_executor/layers/attention/mm_encoder_attention.py, vllm/v1/attention/backends/fa_utils.py, vllm/v1/attention/backends/flash_attn.py, vllm/v1/cudagraph_dispatcher.py, vllm/vllm_flash_attn/__init__.py, vllm/vllm_flash_attn/flash_attn_interface.py
- Revert "[Bugfix] Disable TRTLLM attention with KV transfer enabled (#33192)" (#34832) [`57a96e2`](https://github.com/vllm-project/vllm/commit/57a96e26c913cb9fae96c9e600fa4ff10dc40a1a)
- 受影响文件: vllm/v1/attention/backends/flashinfer.py
- [torch.compile] Undo the fast_moe_cold_start hack in torch>=2.11 (#35475) [`e82fbee`](https://github.com/vllm-project/vllm/commit/e82fbeec7b360af4fb908bf67a659b22f93266d3)
- 受影响文件: vllm/config/vllm.py, vllm/env_override.py, vllm/model_executor/layers/fused_moe/runner/default_moe_runner.py, vllm/utils/torch_utils.py
- [Bugfix] Fix dtype mismatch in RMSNormGated.forward_native() during torch.compile (#35256) [`6290470`](https://github.com/vllm-project/vllm/commit/6290470843c131681e3e1318ae71070a34f33225)
- 受影响文件: tests/kernels/test_fla_layernorm_guard.py, vllm/model_executor/layers/layernorm.py
- [Model Runner V2] Use block table apis for capture inputs (#35671) [`72f4d16`](https://github.com/vllm-project/vllm/commit/72f4d162623854786d29e1d9c6e232cfdf81d3cc)
- 受影响文件: vllm/v1/worker/gpu/block_table.py, vllm/v1/worker/gpu/cudagraph_utils.py
- fix(mxfp4): return is_monolithic=False when LoRA is enabled for Triton backend (#35382) [`5a43550`](https://github.com/vllm-project/vllm/commit/5a435507d877f4eb16802095037d5c56e767c589)
- 受影响文件: vllm/model_executor/layers/quantization/mxfp4.py
- [MISC] Fixing a null reference by removing parallel_utils from mypy EXCLUDE (#35630) [`59d7af9`](https://github.com/vllm-project/vllm/commit/59d7af9c6ced8958a2ca9d257c59dc7c22fa32c6)
- 受影响文件: tools/pre_commit/mypy.py
- [Mamba1] - Kernel Level Chunk Alignment for Prefix Caching (#34798) [`bbf81f9`](https://github.com/vllm-project/vllm/commit/bbf81f9a9284d572b69db2c4fb002c2a8a80d507)
- 受影响文件: csrc/mamba/mamba_ssm/selective_scan.h, csrc/mamba/mamba_ssm/selective_scan_fwd.cu, csrc/ops.h, csrc/torch_bindings.cpp, tests/kernels/mamba/test_mamba_ssm.py, vllm/_custom_ops.py, vllm/model_executor/layers/mamba/mamba_mixer.py, vllm/model_executor/layers/mamba/ops/mamba_ssm.py, vllm/v1/attention/backends/mamba1_attn.py, vllm/v1/attention/backends/mamba2_attn.py, vllm/v1/attention/backends/mamba_attn.py
- [Model Runner V2] Minor refactoring for EncoderRunner (#35628) [`da543d1`](https://github.com/vllm-project/vllm/commit/da543d1abe2468a1b79f230e91e8bbdc2bf6ee71)
- 受影响文件: vllm/v1/worker/gpu/mm/encoder_runner.py, vllm/v1/worker/gpu/model_states/default.py
- [AMD][CI] Support Triton attention with ExampleConnector (#34931) [`87d319c`](https://github.com/vllm-project/vllm/commit/87d319c52f22d3d08ef8ee49163aad9aad08f472)
- 受影响文件: tests/v1/kv_connector/unit/test_example_connector.py, tests/v1/kv_connector/unit/test_multi_connector.py, vllm/distributed/kv_transfer/kv_connector/v1/example_connector.py
- Fix typo: implictly -> implicitly in isaac.py docstring (#35646) [`a9ec392`](https://github.com/vllm-project/vllm/commit/a9ec392c86446996087e6919eaf59023c984b8fe)
- 受影响文件: vllm/model_executor/models/isaac.py
- [Bugfix][Model] Fix Qwen3.5/Qwen3Next ignoring --dtype flag on older GPUs (#35617) [`afd089f`](https://github.com/vllm-project/vllm/commit/afd089f231d714e7fd06b51e3bc7df7fe004c7f9)
- 受影响文件: vllm/model_executor/models/qwen3_5.py, vllm/model_executor/models/qwen3_next.py
- Add TMA support to fused_moe_lora kernel (#32195) [`3ecd0bf`](https://github.com/vllm-project/vllm/commit/3ecd0bf9fccc425c015f7723b6a7730c0dda2970)
- 受影响文件: tests/lora/test_fused_moe_lora_kernel.py, tests/lora/test_olmoe_tp.py, vllm/lora/ops/triton_ops/fused_moe_lora_op.py, vllm/lora/ops/triton_ops/utils.py, vllm/triton_utils/allocation.py
- [Model Runner V2] Add ModelStateInterface [4/N] (#35621) [`e3eb146`](https://github.com/vllm-project/vllm/commit/e3eb146f7ad4bc920e11e98cf88cee3839cf5f89)
- 受影响文件: vllm/v1/worker/gpu/cudagraph_utils.py, vllm/v1/worker/gpu/model_runner.py, vllm/v1/worker/gpu/model_states/__init__.py, vllm/v1/worker/gpu/model_states/default.py, vllm/v1/worker/gpu/model_states/interface.py
- [Bugfix] Fix Anthropic API base64 image handling in Messages endpoint (#35557) [`95a395d`](https://github.com/vllm-project/vllm/commit/95a395dbec08e795ea4eb30494b7a86c8e906c08)
- 受影响文件: tests/entrypoints/openai/test_anthropic_messages_conversion.py, vllm/entrypoints/anthropic/serving.py
- [Chore] Cleanup BNB utilization dead code (#35620) [`e94b263`](https://github.com/vllm-project/vllm/commit/e94b263bd6557dc54582bfc5ba74f0a631bd642d)
- 受影响文件: vllm/model_executor/layers/linear.py
- [Deprecation] Deprecate code in 0.17 as scheduled (#35441) [`e113a30`](https://github.com/vllm-project/vllm/commit/e113a301136402301381a86fb89d58da488ab55b)
- 受影响文件: tests/entrypoints/pooling/embed/test_online.py, vllm/entrypoints/grpc_server.py, vllm/entrypoints/llm.py, vllm/entrypoints/openai/chat_completion/protocol.py, vllm/entrypoints/openai/completion/protocol.py, vllm/entrypoints/openai/translations/__init__.py, vllm/entrypoints/openai/translations/api_router.py, vllm/entrypoints/openai/translations/protocol.py, vllm/entrypoints/openai/translations/serving.py, vllm/entrypoints/openai/translations/speech_to_text.py, vllm/entrypoints/pooling/base/protocol.py, vllm/entrypoints/pooling/classify/protocol.py, vllm/entrypoints/pooling/embed/protocol.py, vllm/entrypoints/pooling/pooling/protocol.py, vllm/entrypoints/pooling/score/protocol.py, vllm/model_executor/layers/mamba/mamba_utils.py
- 文件列表已截断。
- [Benchmark] Avoid unnecessary video download in MMVU (#35618) [`1dafb29`](https://github.com/vllm-project/vllm/commit/1dafb29f91661778d3bcb6a83c7ff03f02c049d4)
- 受影响文件: vllm/benchmarks/datasets.py
- [Fix] Avoid sending image input to other PP ranks (#35405) [`49b9ae3`](https://github.com/vllm-project/vllm/commit/49b9ae32e94b902b87e3d2894f5ac4a5f8dd4abb)
- 受影响文件: vllm/v1/executor/ray_utils.py
- Fix Qwen3_5MTP packed_modules_mapping for gate_up_proj (#35581) [`63d7972`](https://github.com/vllm-project/vllm/commit/63d7972f13d1c5a9d9bd55b664017067a9abd451)
- 受影响文件: vllm/model_executor/models/qwen3_5_mtp.py
- custom dataset img support base64 (#35280) [`c68e69f`](https://github.com/vllm-project/vllm/commit/c68e69f1449cc6d84f43137fcc36c142de1c8fd3)
- 受影响文件: vllm/benchmarks/datasets.py
- [Feat] Add CUDA torch fallbacks for fp8_mqa_logits/fp8_paged_mqa_logits_torch function (#35271) [`7e08c22`](https://github.com/vllm-project/vllm/commit/7e08c22b8cb65a1bea6b4bf9c52ed6e71d4acc47)
- 受影响文件: vllm/model_executor/layers/sparse_attn_indexer.py, vllm/utils/deep_gemm.py, vllm/v1/attention/backends/mla/indexer.py
- add io_process_plugin for sparse embedding (#34214) [`8e75d88`](https://github.com/vllm-project/vllm/commit/8e75d885544c9d7602344e9db2c7e3cff9b73c11)
- 受影响文件: .buildkite/test-amd.yaml, .buildkite/test_areas/plugins.yaml, docs/design/io_processor_plugins.md, tests/plugins/bge_m3_sparse_plugin/bge_m3_sparse_processor/__init__.py, tests/plugins/bge_m3_sparse_plugin/bge_m3_sparse_processor/sparse_embeddings_processor.py, tests/plugins/bge_m3_sparse_plugin/bge_m3_sparse_processor/types.py, tests/plugins/bge_m3_sparse_plugin/setup.py, tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/prithvi_processor.py, tests/plugins_tests/test_bge_m3_sparse_io_processor_plugins.py, tests/plugins_tests/test_io_processor_plugins.py, vllm/plugins/io_processors/__init__.py, vllm/plugins/io_processors/interface.py, vllm/v1/engine/async_llm.py, vllm/v1/engine/llm_engine.py
- [Feature]Supports Anthropic Thinking Block (#33671) [`0892d1a`](https://github.com/vllm-project/vllm/commit/0892d1ab1f9b3476f31811e851d7b3705dfeaefe)
- 受影响文件: vllm/entrypoints/anthropic/protocol.py, vllm/entrypoints/anthropic/serving.py
- Add padding support to wvSplitK solution for skinny GEMMs (#33762) [`7600642`](https://github.com/vllm-project/vllm/commit/7600642eaead7454fd977dde3513682244109e7c)
- 受影响文件: csrc/rocm/skinny_gemms.cu, tests/kernels/quantization/test_rocm_skinny_gemms.py, vllm/model_executor/layers/utils.py
- [ROCm][CI] Parametrize vision score tests across attention backends with per-backend tolerances (#35571) [`1e69c04`](https://github.com/vllm-project/vllm/commit/1e69c048877335e92720772cac704650ad99b219)
- 受影响文件: tests/entrypoints/pooling/score/test_online_score_vision.py
- [Benchmark] Improve UX of sweep scripts (#35600) [`4292e3b`](https://github.com/vllm-project/vllm/commit/4292e3b807a51507f60f43b3829b5e5e918f5b87)
- 受影响文件: docs/benchmarking/sweeps.md, vllm/benchmarks/sweep/plot.py, vllm/benchmarks/sweep/plot_pareto.py, vllm/benchmarks/sweep/serve.py, vllm/benchmarks/sweep/serve_workload.py, vllm/benchmarks/sweep/startup.py
- [Benchmark] Rename SLA Finder to Workload Explorer (#35586) [`24d6ea8`](https://github.com/vllm-project/vllm/commit/24d6ea8afdb13ceee95b36645ba61a641f9a2f7f)
- 受影响文件: docs/benchmarking/sweeps.md, docs/cli/bench/sweep/serve_sla.md, docs/cli/bench/sweep/serve_workload.md, docs/mkdocs/hooks/generate_argparse.py, vllm/benchmarks/sweep/cli.py, vllm/benchmarks/sweep/serve_workload.py
- [Misc] Change logging level from info to debug for tool parser import (#35575) [`57c86c0`](https://github.com/vllm-project/vllm/commit/57c86c07411606eb2ec523e19ec287833b2e7f66)
- 受影响文件: vllm/tool_parsers/qwen3coder_tool_parser.py
- [CI] add trainer_send_weights for MockWeightTransferEngine (#35589) [`06254d4`](https://github.com/vllm-project/vllm/commit/06254d4cbb79e0a406fbf4e18d739293d5470114)
- 受影响文件: tests/entrypoints/weight_transfer/test_weight_transfer_llm.py
- [ROCm][CI] Expose tests to AMD production CI and fix amdsmi heap corruption (#35071) [`f5d1281`](https://github.com/vllm-project/vllm/commit/f5d1281c9d1b96cb4f046f1ec2c53a525f319098)
- 受影响文件: .buildkite/test-amd.yaml, tests/utils.py
- [ROCm] Derive device capability from GCN arch string without CUDA init (#35069) [`94029ff`](https://github.com/vllm-project/vllm/commit/94029ffaf02f0b73e296e11cab721c23fd5a5f97)
- 受影响文件: vllm/platforms/rocm.py, vllm/utils/system_utils.py
Issues：
- [Feature]: Pooling Model Performance Optimizations (https://github.com/vllm-project/vllm/issues/35631)
- [Bug]: TTFT latency issue with Qwen3.5-35B-A3B model using vllm (https://github.com/vllm-project/vllm/issues/35625)
- Issue 内容已截断。
- [Bug]: Qwen3-Omni Model Fails when try to l (https://github.com/vllm-project/vllm/issues/35624)
- Issue 内容已截断。
- [Bug]: invalid argument at cumem_allocator.cpp:119 (https://github.com/vllm-project/vllm/issues/35612)
- Issue 内容已截断。
- [Bug]: vllm 0.16.0+image encountered CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasGemmEx (https://github.com/vllm-project/vllm/issues/35608)
- Issue 内容已截断。
- [Bug]: vllm: error: unrecognized arguments: --task embedding (https://github.com/vllm-project/vllm/issues/35603)
- Issue 内容已截断。
- [Bug]:  vllm: error: unrecognized arguments: --language-model-only (https://github.com/vllm-project/vllm/issues/35602)
- Issue 内容已截断。
- [Bug]: cpu version compile failed in 0.16.0 (https://github.com/vllm-project/vllm/issues/35599)
- Issue 内容已截断。
### NVIDIA/cutile-python
昨日无更新。

## 总结
- Diff 内容已截断以满足 prompt 预算。
- Issue 内容已截断以满足 prompt 预算。
- OpenRouter repo summarize failed for flashinfer-ai/flashinfer: OpenRouter 429 Too Many Requests (z-ai/glm-4.5-air:free): {"error":{"message":"Provider returned error","code":429,"metadata":{"raw":"z-ai/glm-4.5-air:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations","provider_name":"Z.AI","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
- OpenRouter repo summarize failed for sgl-project/sglang: OpenRouter 429 Too Many Requests (z-ai/glm-4.5-air:free): {"error":{"message":"Provider returned error","code":429,"metadata":{"raw":"z-ai/glm-4.5-air:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations","provider_name":"Z.AI","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
- OpenRouter repo summarize failed for vllm-project/vllm: OpenRouter 429 Too Many Requests (z-ai/glm-4.5-air:free): {"error":{"message":"Provider returned error","code":429,"metadata":{"raw":"z-ai/glm-4.5-air:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations","provider_name":"Z.AI","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
- OpenRouter global summarize failed: OpenRouter 429 Too Many Requests (z-ai/glm-4.5-air:free): {"error":{"message":"Provider returned error","code":429,"metadata":{"raw":"z-ai/glm-4.5-air:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations","provider_name":"Z.AI","is_byok":false}},"user_id":"user_2wqU29q2Bhpw2S2iw7Pwn8RHaXB"}
